{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "\n",
    "import tiktoken\n",
    "from langchain_teddynote.callbacks import StreamingCallback\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import VLLM\n",
    "import json\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"beomi/Qwen2.5-7B-Instruct-kowiki-qa-context\", temperature=0.7, streaming=True, base_url=\"http://localhost:8080/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 9, 'total_tokens': 20, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-729a0205-fd3d-44f6-b7e4-3317370ab033-0', usage_metadata={'input_tokens': 9, 'output_tokens': 11, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"하이\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'beomi/Qwen2.5-7B-Instruct-kowiki-qa-context'}, id='run-51a856e8-4899-400d-841c-f2f2914582ee-0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"하이\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class CustomStreamingCallbackHandler(BaseCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.partial_result = \"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.partial_result += token\n",
    "        print(token, end=\"\", flush=True)  # 실시간 출력\n",
    "        # '}'가 생성되면 중단\n",
    "        if \"}\" in self.partial_result:\n",
    "            raise StopIteration(\"중단 조건 '}'이 생성되어 종료합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTIAL_SUMMARY_PROMPT_TEMPLATE = \"\"\"Please summarize the sentence according to the following REQUEST.\n",
    "                                            This task is partial summay, Please Do not summarize too much.\n",
    "    \n",
    "                                            REQUEST:\n",
    "                                            1. Summarize the main points in KOREAN.\n",
    "                                            2. Translate the summary into KOREAN if it is written in ENGLISH.\n",
    "                                            3. DO NOT translate any technical terms.\n",
    "                                            4. DO NOT include any unnecessary information.\n",
    "                                            \n",
    "                                            CONTEXT:\n",
    "                                            {context}\n",
    "                                            \n",
    "                                            SUMMARY:\n",
    "                                            \"\"\"\n",
    "                                            \n",
    "FINAL_SUMMARY_PROMPT_TEMPLATE = \"\"\"\n",
    "다음 REQUEST에 따라 CONTEXT를 요약하고, 출력은 아래에 제공된 출력 형식(OUTPUT_FORMAT)과 정확히 동일하게 한 번만 작성해주세요.\n",
    "\n",
    "REQUEST:\n",
    "1. 주어진 OUTPUT(JSON 형식) 외의 텍스트나 설명을 포함하지 마세요.\n",
    "2. CONTEXT와 HUMAN MESSAGE는 출력하지마세요.\n",
    "3. 단 하나의 OUTPUT만 출력하세요.\n",
    "4. 주요 내용을 한국어로 요약하되, 전문, 기술 용어는 원본을 사용하세요.\n",
    "5. 요약된 각 문장은 해당 의미와 잘 어울리는 이모지 하나로 시작해야 합니다.\n",
    "6. 다양한 이모지를 사용하여 요약을 흥미롭게 작성하되, 간결하고 관련성 있게 유지하세요.\n",
    "7. 문서의 단일 주요 주제와 전반적인 요약에만 집중하세요.\n",
    "8. 각 요약에서 주요 주제를 명확히 나타내세요.\n",
    "9. CONTEXT의 내용이 충분히 많다면, 요약 문장을 충분히 생성하세요.\n",
    "10. 요약된 내용을 기반으로 가장 관련성 높은 세 가지 질문을 한국어로 작성하세요.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "OUTPUT_FORMAT(JSON 형식):\n",
    "{{\n",
    "    \"FINAL_SUMMARY\": {{\n",
    "        \"Key_topic\": 주요 주제 내용,\n",
    "        \"Summaries\": [\n",
    "            \"• Emoji 요약된 내용1\",\n",
    "            \"• Emoji 요약된 내용2\",\n",
    "            ...추가 요약 내용 나열\n",
    "        ]\n",
    "    }},\n",
    "    \"RECOMMEND_QUESTIONS\": [\n",
    "        \"첫 번째 질문 (한국어)\",\n",
    "        \"두 번째 질문 (한국어)\",\n",
    "        \"세 번째 질문 (한국어)\"\n",
    "    ]\n",
    "}}\n",
    "OUTPUT:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=500\n",
    ")\n",
    "partial_summary_prompt = PromptTemplate.from_template(\n",
    "    PARTIAL_SUMMARY_PROMPT_TEMPLATE\n",
    ")\n",
    "final_summary_prompt = PromptTemplate.from_template(\n",
    "    FINAL_SUMMARY_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../runpod_backend/data/backup.json\",\"r\") as f:\n",
    "#     data = json.load(f)\n",
    "# script = data[\"m25Lz9KWyDkNx6Vv\"][\"script_info\"][\"script\"]\n",
    "with open(\"./test1234/transcript.json\",\"r\") as f:\n",
    "    script = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "            Document(page_content=\"\\n\".join([t[\"text\"] for t in script]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_summary_chain = create_stuff_documents_chain(\n",
    "            llm=llm, prompt=partial_summary_prompt\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, chunk_overlap=400\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tokens(text, model=\"gpt-4o-mini\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "calculate_tokens(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    emoji: str = Field(..., description=\"요약에 사용하는 이모지\")\n",
    "    content: str = Field(..., description=\"요약된 내용\")\n",
    "\n",
    "class FinalSummary(BaseModel):\n",
    "    key_topic: str = Field(..., description=\"주요 주제 내용\")\n",
    "    summaries: List[Summary] = Field(..., description=\"요약된 내용 리스트\")\n",
    "\n",
    "class RecommendQuestions(BaseModel):\n",
    "    questions: List[str] = Field(..., description=\"추천 질문 리스트\")\n",
    "\n",
    "class FullStructure(BaseModel):\n",
    "    FINAL_SUMMARY: FinalSummary = Field(..., description=\"최종 요약 정보\")\n",
    "    RECOMMEND_QUESTIONS: RecommendQuestions = Field(..., description=\"추천 질문 리스트\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser(pydantic_object=FullStructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary_chain = create_stuff_documents_chain(\n",
    "            llm=llm, prompt=final_summary_prompt, output_parser=parser\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = partial_summary_chain.invoke({\"context\":docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_summary = [Document(page_content=summary)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_handler = CustomStreamingCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = final_summary_chain.invoke({\"context\": docs})\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary[\"FINAL_SUMMARY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary[\"RECOMMEND_QUESTIONS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\n",
    "summary += f'{list(final_summary.get(\"FINAL_SUMMARY\").items())[0][0]}:{list(final_summary.get(\"FINAL_SUMMARY\").items())[0][1]}\\n'\n",
    "joined_summary = '\\n'.join(list(final_summary.get(\"FINAL_SUMMARY\").items())[1][1])\n",
    "summary += f'{list(final_summary.get(\"FINAL_SUMMARY\").items())[1][0]}:{joined_summary}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = final_summary.get(\"RECOMMEND_QUESTIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)\n",
    "print(\"------\")\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].page_content += f\"\\n{summary}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = FAISS.from_documents(split_docs, hf_embeddings)\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k = 10\n",
    "vec_retriever = vec_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "retriever = EnsembleRetriever(\n",
    "                retrievers=[bm25_retriever, vec_retriever],\n",
    "                weights=[0.7, 0.3],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 사용자의 질문에 대한 답변\n",
    "# response_schemas = [\n",
    "#     ResponseSchema(name=\"answer\", description=\"사용자의 질문에 대한 답변\"),\n",
    "# ]\n",
    "# # 응답 스키마를 기반으로 한 구조화된 출력 파서 초기화\n",
    "# output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "# format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "            \"\"\"당신은 유튜브 스크립트 기반의 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다.\n",
    "                당신의 주요 임무는 다음과 같습니다:\n",
    "                1. 기본적으로 검색된 문맥(context)과 이전 대화 내용(chat_history)을 바탕으로 질문에 대해 OUTPUT_FORMAT의 형식을 반드시 지켜서 답변하세요.\n",
    "                2. 주어진 OUTPUT(JSON 형식) 외의 텍스트나 설명을 포함하지 마세요.\n",
    "                3. 다음과 같은 경우에는 자연스럽게 내부 지식을 활용하여 답변하세요:\n",
    "                    - 검색된 문맥이 질문의 의도를 완벽하게 충족하지 못할 때\n",
    "                    - 영상의 전반적인 주제와 연관되지만 구체적인 답변이 문맥에 없을 때\n",
    "                    - 문맥에서 부분적인 정보만 찾을 수 있을 때는 문맥의 정보와 내부 지식을 조합하여 답변하세요\n",
    "\n",
    "                4. 답변 시 다음 사항을 지켜주세요:\n",
    "                - 항상 자연스러운 대화체로 답변하세요\n",
    "                - 문맥에서 답을 찾을 수 없더라도, 그 사실을 언급하지 말고 바로 답변하세요\n",
    "                - 기술 용어나 고유명사는 원어를 유지하세요\n",
    "                - 전문적인 내용도 이해하기 쉽게 설명하세요\n",
    "\n",
    "                5. 만약 질문이 영상의 주제나 내용과 전혀 관련이 없다면 \"영상과 관계 없는 질문입니다.\"라고 답변하세요.\n",
    "\n",
    "                이전 대화 내용:\n",
    "                {chat_history}\n",
    "\n",
    "                질문:\n",
    "                {question}\n",
    "\n",
    "                문맥:\n",
    "                {context}\n",
    "\n",
    "                OUTPUT_FORMAT:\n",
    "                {{\"answer\": \"답변\"}}\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chat_parser(BaseModel):\n",
    "    answer: dict = Field(..., description=\"채팅 응답\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "                {\n",
    "                    \"context\": itemgetter(\"question\") | retriever,\n",
    "                    \"question\": itemgetter(\"question\"),\n",
    "                    \"chat_history\": itemgetter(\"chat_history\"),\n",
    "                }\n",
    "                | prompt\n",
    "                # | chat_llm\n",
    "                | llm\n",
    "                | JsonOutputParser(pydantic_object=chat_parser)\n",
    "                # | StrOutputParser()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_message_store = {}\n",
    "def _get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        \"\"\"세션 ID를 기반으로 대화 기록을 가져오는 함수\"\"\"\n",
    "        if session_id not in _message_store:\n",
    "            _message_store[session_id] = ChatMessageHistory()\n",
    "        return _message_store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history = RunnableWithMessageHistory(\n",
    "                chain,\n",
    "                _get_session_history,\n",
    "                input_messages_key=\"question\",\n",
    "                history_messages_key=\"chat_history\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt):\n",
    "    return chain_with_history.invoke(\n",
    "                    {\"question\": prompt},\n",
    "                    config={\"configurable\": {\"session_id\": \"test1234\"}},\n",
    "                    callbacks = [callback_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 트랜스포머의 어텐션 메커니즘은 어떻게 작동하며, 그 중요성은 무엇인가요?\n",
    "\n",
    "2. 어텐션 메커니즘은 토큰의 고차원 벡터를 어떻게 조정하여 문맥에 따른 의미를 더 풍부하게 만드는가요?\n",
    "\n",
    "3. 어텐션 메커니즘의 여러 헤드는 어떻게 작동하며, 그 이유는 무엇인가요? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat('Transformer의 어텐션 메커니즘이 어떻게 작동하는지 간단하게 설명해 줄 수 있나요?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, chunk_overlap=500\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"script.json\", \"r\") as f:\n",
    "    script_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"test1234\",exist_ok=True)\n",
    "with open(\"test1234/transcript.json\", \"w\") as f:\n",
    "    json.dump(script_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "            Document(page_content=\"\\n\".join([t[\"text\"] for t in script_data]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store.save_local(\"test1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store_load = FAISS.load_local(\"test11\", embeddings=embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[FINAL SUMMARY]\\nKey topic: 레그의 이해 및 정보 처리\\n\\n• 📚 레그의 비법노트에 도달하기 위한 과정이 필요하다.  \\n• 🔄 반복 학습을 통해 레그의 기본 개념과 구현 방식을 이해해야 한다.  \\n• ✏️ 레그는 최신 정보를 제공하고, 정보 참조를 통해 질문에 답변할 수 있는 AI이다.  \\n• ⚙️ 사전학습된 정보와 최신 정보의 차이를 이해해야 하며, 정보의 흐름을 잃지 않도록 주의해야 한다.  \\n• 🔍 효과적인 정보 처리를 위해 관련성 있는 정보의 페이지만 제공하는 것이 중요하다.  \\n• 📄 문서의 특정 단락을 선택하고, 유사도를 계산하여 필요한 정보를 추출한다.  \\n• 💡 인베딩 과정을 통해 문장을 수학적 표현으로 변환하고, 이를 바탕으로 정보 검색을 향상시킨다.\\n\\n[RECOMMEND QUESTIONS]\\n1. 레그를 활용하여 최신 정보를 어떻게 효율적으로 제공할 수 있을까?\\n2. 사전학습된 정보와 최신 정보의 활용 시 고려해야 할 요소는 무엇인가?\\n3. 인베딩 과정이 정보 검색에 미치는 영향은 어떤 것들이 있을까?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[0].strip(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[0].strip(\"\\n\\n\").replace(\"\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.container?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "import re\n",
    "\n",
    "\n",
    "class YouTubeService:\n",
    "    async def get_title_and_hashtags(self, url: str):\n",
    "        yt = await _create_youtube_instance(url)\n",
    "        print(\"영상 정보 확인\")\n",
    "        title = yt.title\n",
    "        description = yt.description\n",
    "        hashtags = re.findall(r\"#\\w+\", description)\n",
    "        return {\"title\": title, \"hashtags\": \" \".join(hashtags)}\n",
    "\n",
    "    async def get_video_info(self, url: str):\n",
    "        yt = await _create_youtube_instance(url)\n",
    "        audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "        print(\"음성 추출 완료\")\n",
    "        return {\n",
    "            \"title\": yt.title,\n",
    "            \"audio_url\": audio_stream.url if audio_stream else None,\n",
    "        }\n",
    "\n",
    "    async def _create_youtube_instance(self, url: str):\n",
    "        print(\"YouTube 인스턴스 생성 완료\")\n",
    "        return YouTube(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import ffmpeg\n",
    "import requests\n",
    "import soundfile as sf\n",
    "from faster_whisper import BatchedInferencePipeline, WhisperModel\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "class WhisperTranscriptionService:\n",
    "    def __init__(self):\n",
    "        model = WhisperModel(\n",
    "            \"large-v3\", device=\"cuda\", compute_type=\"float16\"\n",
    "        )\n",
    "        model = BatchedInferencePipeline(model=model)\n",
    "        language = None\n",
    "        okt = Okt()\n",
    "        print(\"Whisper 모델 초기화 완료\")\n",
    "\n",
    "    def create_session(self):\n",
    "        session = requests.Session()\n",
    "        retry = Retry(\n",
    "            total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        return session\n",
    "\n",
    "    def download_chunk(self, args):\n",
    "        url, start, end, chunk_number, temp_dir = args\n",
    "\n",
    "        headers = {\"Range\": f\"bytes={start}-{end}\"}\n",
    "        session = create_session()\n",
    "\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, stream=True)\n",
    "            chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_number:04d}\")\n",
    "\n",
    "            with open(chunk_path, \"wb\") as f:\n",
    "                for data in response.iter_content(chunk_size=8192):\n",
    "                    f.write(data)\n",
    "\n",
    "            return chunk_path, chunk_number\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading chunk {chunk_number}: {str(e)}\")\n",
    "            return None, chunk_number\n",
    "\n",
    "    def _single_stream_download(self, url: str, temp_dir: str) -> str:\n",
    "        \"\"\"단일 스트림으로 파일을 다운로드합니다.\"\"\"\n",
    "        print(\"Starting single stream download...\")\n",
    "        session = create_session()\n",
    "        output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "\n",
    "        try:\n",
    "            with session.get(url, stream=True) as response:\n",
    "                response.raise_for_status()\n",
    "                with open(output_path, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to download file: {str(e)}\")\n",
    "\n",
    "    def parallel_download(self, url: str, temp_dir: str, num_chunks: int = 10) -> str:\n",
    "        \"\"\"병렬 다운로드를 시도하고, 실패 시 단일 스트림으로 폴백\"\"\"\n",
    "        session = create_session()\n",
    "\n",
    "        try:\n",
    "            # HEAD 요청으로 파일 크기 확인 시도\n",
    "            response = session.head(url, allow_redirects=True)\n",
    "            total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "            # HEAD 요청이 실패하면 GET 요청으로 시도\n",
    "            if total_size == 0:\n",
    "                response = session.get(url, stream=True)\n",
    "                total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "            # 파일 크기를 여전히 확인할 수 없는 경우 단일 스트림으로 다운로드\n",
    "            if total_size == 0:\n",
    "                print(\n",
    "                    \"Warning: Could not determine file size. Falling back to single stream download.\"\n",
    "                )\n",
    "                return _single_stream_download(url, temp_dir)\n",
    "            print(\"Starting parallel download...\")\n",
    "            chunk_size = total_size // num_chunks\n",
    "            chunks = []\n",
    "\n",
    "            for i in range(num_chunks):\n",
    "                start = i * chunk_size\n",
    "                end = start + chunk_size - 1 if i < num_chunks - 1 else total_size - 1\n",
    "                chunks.append((start, end))\n",
    "\n",
    "            download_args = [\n",
    "                (url, start, end, i, temp_dir) for i, (start, end) in enumerate(chunks)\n",
    "            ]\n",
    "\n",
    "            chunk_paths = []\n",
    "            with concurrent.futures.ThreadPoolExecutor(\n",
    "                max_workers=num_chunks\n",
    "            ) as executor:\n",
    "                futures = executor.map(download_chunk, download_args)\n",
    "                chunk_paths = [(path, num) for path, num in futures if path is not None]\n",
    "\n",
    "            if not chunk_paths:\n",
    "                raise Exception(\"No chunks were downloaded successfully\")\n",
    "\n",
    "            chunk_paths.sort(key=lambda x: x[1])\n",
    "            output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "\n",
    "            with open(output_path, \"wb\") as outfile:\n",
    "                for chunk_path, _ in chunk_paths:\n",
    "                    with open(chunk_path, \"rb\") as infile:\n",
    "                        outfile.write(infile.read())\n",
    "                    os.remove(chunk_path)\n",
    "\n",
    "            return output_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error in parallel download: {str(e)}. Falling back to single stream download.\"\n",
    "            )\n",
    "            return _single_stream_download(url, temp_dir)\n",
    "\n",
    "    def convert_to_wav(self, input_path: str, output_path: str) -> bool:\n",
    "        try:\n",
    "            stream = ffmpeg.input(input_path)\n",
    "            stream = ffmpeg.output(\n",
    "                stream, output_path, acodec=\"pcm_s16le\", ar=\"16000\", ac=\"1\"\n",
    "            )\n",
    "            ffmpeg.run(stream, capture_stdout=True, capture_stderr=True)\n",
    "            return True\n",
    "        except ffmpeg.Error as e:\n",
    "            print(\"FFmpeg error:\", e.stderr.decode())\n",
    "            return False\n",
    "\n",
    "    def process_audio_chunk(self, chunk_data: tuple,promp:str = None,filtered_words:list = None) -> List[Dict[str, Any]]:\n",
    "        audio_path, start_time, duration = chunk_data\n",
    "        try:\n",
    "            segments, info = model.transcribe(\n",
    "                audio_path,\n",
    "                beam_size=5,\n",
    "                best_of=7,\n",
    "                batch_size=32,\n",
    "                temperature=0.7,\n",
    "                word_timestamps=True,\n",
    "                initial_prompt=f\"음성 제목: {promp}\",\n",
    "                repetition_penalty=2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=1.1,\n",
    "                log_prob_threshold=-0.5,\n",
    "                no_speech_threshold=0.7,\n",
    "                patience=1.2,\n",
    "                hotwords=filtered_words\n",
    "            )\n",
    "            if info and hasattr(info, \"language\"):\n",
    "                language = info.language\n",
    "            return _process_segments(segments, start_time)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk at {start_time}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _process_segments(\n",
    "        self, segments, start_time: float = 0\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        transcript = []\n",
    "        for segment in segments:\n",
    "            transcript.append(\n",
    "                {\n",
    "                    \"start\": round(segment.start + start_time, 2),\n",
    "                    \"end\": round(segment.end + start_time, 2),\n",
    "                    \"text\": segment.text,\n",
    "                }\n",
    "            )\n",
    "        return transcript\n",
    "\n",
    "    async def process_with_progress(\n",
    "        self, url: str, prompt:str, filtered_words:str,chunk_duration: int = 30, num_download_chunks: int = 10\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            mp4_path = parallel_download(url, temp_dir, num_download_chunks)\n",
    "            print(\"Download complete!\")\n",
    "\n",
    "            wav_path = os.path.join(temp_dir, \"audio.wav\")\n",
    "            if not convert_to_wav(mp4_path, wav_path):\n",
    "                raise Exception(\"Failed to convert audio to WAV format\")\n",
    "\n",
    "            wav_info = sf.info(wav_path)\n",
    "            total_duration = wav_info.duration\n",
    "            total_chunks = math.ceil(total_duration / chunk_duration)\n",
    "\n",
    "            chunks_data = []\n",
    "            for i in range(total_chunks):\n",
    "                start_time = i * chunk_duration\n",
    "                chunk_wav_path = os.path.join(temp_dir, f\"chunk_{i}.wav\")\n",
    "\n",
    "                duration = min(chunk_duration, total_duration - start_time)\n",
    "                stream = ffmpeg.input(wav_path, ss=start_time, t=duration)\n",
    "                stream = ffmpeg.output(\n",
    "                    stream, chunk_wav_path, acodec=\"pcm_s16le\", ar=\"16000\", ac=\"1\"\n",
    "                )\n",
    "                ffmpeg.run(stream, quiet=True)\n",
    "\n",
    "                chunks_data.append((chunk_wav_path, start_time, duration))\n",
    "\n",
    "            all_segments = []\n",
    "            for chunk_data in chunks_data:\n",
    "                segments = process_audio_chunk(chunk_data,prompt,filtered_words)\n",
    "                all_segments.extend(segments)\n",
    "\n",
    "                if os.path.exists(chunk_data[0]):\n",
    "                    os.remove(chunk_data[0])\n",
    "\n",
    "        return all_segments\n",
    "\n",
    "    async def transcribe(self, audio_url: str,prompt: str = None) -> Dict[str, Any]:\n",
    "        try:\n",
    "            try:\n",
    "                tagged = okt.pos(prompt)\n",
    "                filtered_words = []\n",
    "                for word, tag in tagged:\n",
    "                    if tag == \"Noun\" or tag == \"Hashtag\":\n",
    "                        filtered_words.append(word)\n",
    "            except:\n",
    "                filtered_words = None\n",
    "            segments = await process_with_progress(\n",
    "                audio_url, prompt, filtered_words,chunk_duration=30, num_download_chunks=10\n",
    "            )\n",
    "\n",
    "            print(\"텍스트 추출 완료\")\n",
    "\n",
    "            return {\"script\": segments, \"language\": language}\n",
    "        except Exception as e:\n",
    "            print(f\"Error in transcribe: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = YouTubeService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = await youtube.get_video_info(\"https://youtu.be/EMMC0ym0QOI?si=bx7raBo-QwR3MGy7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info[\"audio_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper = WhisperTranscriptionService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = await whisper.transcribe(video_info[\"audio_url\"],video_info[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for script in transcript[\"script\"]:\n",
    "    print(script[\"text\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = await whisper.transcribe(video_info[\"audio_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for script in transcript[\"script\"]:\n",
    "    print(script[\"text\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### 주제: 레그의 개념 및 구현 방법\\n\\n- 📚 레그 비법노트를 통해 레그의 기본 개념과 구현 방식에 대한 이해가 중요합니다.\\n- 🔍 실습 파일들을 반복적으로 검토하여 이해도를 높여야 합니다.\\n- ❓ 레그의 주요 목적은 최신 정보를 포함한 정확한 답변을 제공하는 것입니다.\\n- 🆚 GPT는 사전학습된 정보에 의존하지만, 레그는 제공된 정보를 바탕으로 더 정확한 답변을 생성합니다.\\n- 📊 오래된 정보는 정확한 답변을 방해하며, 관련성 있는 정보만 제공하는 것이 최선입니다.\\n- 📑 문맥을 통해 질문에 대한 답변을 찾고, 유사도 계산을 통해 관련 단락을 추출합니다.\\n- 🔄 텍스트는 특정 키워드로 분할되어야 하며, 청크 오버랩을 통해 정보의 일관성을 유지합니다.\\n- 💾 인베딩 과정을 통해 각 단락을 숫자 표현으로 변환하고 저장하여 나중에 검색할 수 있습니다.\\n- 📊 인베딩 이해 후, 데이터를 저장해야 하며, 이 과정에서 비용이 발생합니다.\\n- 📚 다음 영상에서는 레그의 후반부 내용을 다룰 예정입니다.\\n\\n### 추천 질문:\\n1. 레그의 구현 방식에서 가장 중요한 요소는 무엇인가요?\\n2. 인베딩 과정에서 발생하는 비용은 어떻게 관리할 수 있나요?\\n3. 관련성 있는 정보를 효과적으로 추출하기 위한 방법은 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"### KEY TOPIC: 레그의 기능과 정보 접근 방식\\n\\n- 📚 레그의 비법노트에 도착하기까지 수고하셨습니다.  \\n- 🔄 레그에 대한 이해를 위해 반복 학습이 필요합니다.  \\n- 🎨 레그의 목적을 시각적으로 설명할 예정입니다.  \\n- ❓ 레그는 최신 정보를 제공하기 위해 사용됩니다.  \\n- 📰 기존 채찍 PT와 비교하여 정보 접근 방식을 설명합니다.  \\n- ⚙️ 프롬프트의 변화로 레그의 기능이 향상됩니다.  \\n- 📄 PDF와 같은 자료를 활용하여 질문에 답변함.  \\n- 🔑 관련성 있는 정보만 제공하는 것이 최선임.  \\n- 📏 특정 단락만 필요한 경우 청크 사이즈를 설정하여 분할함.  \\n- 🔍 유사도 계산을 통해 관련 단락을 뽑아냅니다.  \\n- 📊 임베딩은 문자열을 수학적 표현으로 변환하는 과정임.  \\n- 🔗 동일한 숫자 개수로 유사도 계산 가능함.  \\n- 💰 인베딩 과정에서 비용이 발생하며, 많은 문서를 처리할 때 신중해야 함.  \\n- 📽️ 이번 영상은 전처리 단계를 다루었고, 다음 영상에서는 후반부를 다룰 예정.\\n\\n### RECOMMENDED QUESTIONS:\\n1. 레그의 정보 접근 방식에서 가장 중요한 요소는 무엇인가요?\\n2. 임베딩 과정에서 발생하는 비용을 줄일 수 있는 방법은 무엇인가요?\\n3. 레그의 기능을 향상시키기 위한 반복 학습의 필요성은 어떤 점에서 중요한가요?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Key Topic: RAG (Retrieval-Augmented Generation) 시스템의 구현 및 이해\\n\\n- 🎉 여러분은 RAG의 비법노트에 도달했습니다.\\n- 🔄 RAG의 내용을 반복하여 이해를 높이세요.\\n- 📂 실습 파일을 통해 RAG 프로세스를 이해하는 것이 중요합니다.\\n- 🧐 RAG가 무엇인지와 구현 방법을 알아야 합니다.\\n- 🎨 RAG 사용 목적을 그림으로 설명할 예정입니다.\\n- ❓ RAG의 필요성을 채찍 PT와 비교하여 설명할 것입니다.\\n- 📄 최신 정보를 제공하는 것이 RAG의 주요 목적입니다.\\n- 📊 GPD는 사전학습된 정보에 의존하므로 정보가 오래되면 정확한 답변을 하지 못합니다.\\n- 🔗 RAG는 주어진 정보를 참고하여 답변하는 방식으로 프롬프트가 바뀝니다.\\n- 📚 GPD는 사전학습된 정보에 기반하여만 답변할 수 있습니다.\\n- ⏳ 오래된 사전학습 정보는 정확한 답변을 어렵게 만듭니다.\\n- 🔍 프롬프트가 변경되어 주어진 정보를 바탕으로 질문에 답변하도록 합니다.\\n- 📄 PDF 문서에서 관련 정보를 검색하여 답변할 수 있습니다.\\n- ⚠️ 많은 정보를 입력할 경우 비용이 증가하고 정보 탐색이 어려워질 수 있습니다.\\n- 🔑 관련성 있는 정보만 제공하는 것이 최선입니다.\\n- 📝 문서에서 텍스트를 긁어오는 방식으로 정보를 로드합니다.\\n- 🔍 질문에 대한 유사도 검색을 통해 관련 단락을 추출합니다.\\n- 📈 유사도가 높은 단락을 검색해 최상위 결과를 제공합니다.\\n- 🧮 인베딩은 문장을 수학적 표현으로 바꾸어 정보 검색을 향상시킵니다.\\n- 💾 인베딩 후 변환된 데이터를 저장하는 과정이 필요합니다.\\n- 🔍 저장된 데이터는 검색어를 통해 관련 문서를 찾는 데 사용됩니다.\\n- 📽️ 이번 영상은 사전 단계까지 살펴보았고, 다음 영상에서 후반부를 다룰 예정입니다.\\n\\n### RECOMMENDED QUESTIONS:\\n1. RAG 시스템이 기존 GPD와 어떻게 차별화되는지 설명할 수 있나요?\\n2. RAG의 인베딩 과정에서 발생하는 비용은 어떤 요소에 의해 결정되나요?\\n3. PDF 문서에서 정보를 검색할 때 유사도 계산은 어떻게 이루어지나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://youtu.be/AA621UofTUA?si=gn4XutRMWUDSYLFL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Please summarize the sentence according to the following FINAL REQUEST. \n",
    "FINAL REQUEST:\n",
    "1. The provided summary sections are partial summaries of one document. Please combine them into a single cohesive summary.\n",
    "2. Summarize the main points in bullet points in KOREAN.\n",
    "3. Each summarized sentence must start with a single emoji that fits the meaning of the sentence.\n",
    "4. Use various emojis to make the summary more interesting, but keep it concise and relevant.\n",
    "5. Focus on identifying and presenting only one main topic and one overall summary for the document.\n",
    "6. Avoid redundant or repeated points, and ensure that the summary covers all key ideas without introducing multiple conclusions or topics.\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\n",
    "FINAL SUMMARY:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please summarize the sentence according to the following REQUEST.\\nREQUEST:\\n1. Summarize the main points in bullet points in KOREAN.\\n2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\\n3. Use various emojis to make the summary more interesting.\\n4. Translate the summary into KOREAN if it is written in ENGLISH.\\n5. DO NOT translate any technical terms.\\n6. DO NOT include any unnecessary information.\\n\\nCONTEXT:\\n{context}\\n\\nSUMMARY:\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please summarize the sentence according to the following FINAL REQUEST. \\nFINAL REQUEST:\\n1. The provided summary sections are partial summaries of one document. Please combine them into a single cohesive summary.\\n2. Summarize the main points in bullet points in KOREAN, but DO NOT translate any technical terms.\\n3. Each summarized sentence must start with a single emoji that fits the meaning of the sentence.\\n4. Use various emojis to make the summary more interesting, but keep it concise and relevant.\\n5. Focus on identifying and presenting only one main topic and one overall summary for the document.\\n6. Avoid redundant or repeated points, and ensure that the summary covers all key ideas without introducing multiple conclusions or topics.\\n7. Please refer to each summary and indicate the key topic.\\n8. If the original text is in English, we have already provided a summary translated into Korean, so please do not provide a separate translation.\\n\\nCONTEXT: \\n{context}\\n\\nFINAL SUMMARY:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[FINAL SUMMARY]\\n• 📚 레그의 비법노트는 반복 학습과 레그에 대한 깊은 이해가 필요합니다.\\n• 🖼️ RAG의 도입은 최신 정보를 제공하기 위해 중요하며, 좌측의 레그와 우측의 기존 방법을 비교해 설명합니다.\\n• 📈 프롬프트의 변화와 정보의 정확성이 중요하며, 사전학습된 정보는 시간이 지나면 신뢰성이 떨어집니다.\\n• 🔑 유사도 검색을 통해 관련 단락을 찾아내고, 텍스트 스플리터와 인베딩 과정을 통해 정보의 정확성을 확보합니다.\\n• 💰 많은 입력 정보는 비용 증가와 정보 탐색의 어려움을 초래할 수 있습니다.\\n\\n[RECOMMEND QUESTIONS]\\n1. RAG의 도입이 왜 중요한가요?\\n2. 유사도 검색에서 어떤 방법으로 관련 단락을 찾나요?\\n3. 입력 정보가 많을 때 발생할 수 있는 문제는 무엇인가요?\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[1].split(\"\\n\")[1].split(\".\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = [{\"start\": 0.0, \"end\": 29.42, \"text\": \" 여러분 안녕하세요 드디어 레그의 비법노트에 레그 파트까지 오시느라 정말 고생 많으셨습니다 레그의 전반적인 내용을 먼저 한번 들어보시고요 그리고 잘 이해가 안되면 또 반복해서 들어보실 수 있으니까 반복해서 들어보시고 그리고 더 중요한 거는 이 실습 파일들을 여러분들이 반복해서 보시면서 계속 레그에 대한 프로세스 이해가 있어야 그 다음에 다시 역으로 돌아가서 우리가 이런 것들을 살펴볼 거에요 아프파서랑 모델 메모리 체인들 이런 것들을 쭉 살펴볼 때 역으로 더 이해가 잘 되실 거라는 생각이 들더라구요\"}, {\"start\": 30.0, \"end\": 42.5, \"text\": \" 우리가 여기 처음부터 다 하고 가려면 너무 시간이 오래 걸리니까 이번 시간에는 레그를 좀 깊게 다뤄보기 보다는 일단은 레그가 뭔지 그리고 어떤 식으로 구현하는지 대충 감을 잡는다 그런 생각으로 오시면 됩니다.\"}, {\"start\": 43.21, \"end\": 60.03, \"text\": \" 저희가 먼저 여기 레그의 베이식, 이 정도 수준에서 먼저 볼 건데요. 먼저 그러려면은 우리가 레그에 대한 이해가 필요할 것 같아요. 그래서 제가 좀 그림으로 그려왔어요. 제가 그림으로 그리는 걸 되게 좋아하는데 이 레그라는 걸 도대체 왜 쓰느냐, 우리가 그 강의 초반에도 말씀드렸잖아요.\"}, {\"start\": 60.0, \"end\": 74.72, \"text\": \" 레그를 쓰는 목적에 대해서 다시 한 번만 짚고 넘어가 볼게요. 우리가 레그를 안 쓰고 채찍 PT 같은 걸 통해서 질문을 합니다. 우리가 일반적으로 채찍 PT에서 쓰는 거는 이런 방식이거든요. 여기에 여러분들이 이러한 퀘션들을 넣어줘요.\"}, {\"start\": 76.47, \"end\": 89.55, \"text\": \" 프롬프트로 들어가죠. 당신은 친절한 답변하는 AI 어시스턴트입니다. 이런 것들이 들어가고요. 그 다음에 좀 더 확대해서 보여드리면 이렇게 들어가죠. 당신은 친절한 답변을 하는 어시스턴트.\"}, {\"start\": 90.82, \"end\": 117.02, \"text\": \" 사용자의 질문이 여기 들어와요. 그러면 우리가 스트리밋으로 구현한 것처럼 요거에 대해서 프롬프트 완성을 해서 결국에는 이 LLM한테 전달이 된다는 거예요. 우리가 그걸 GPT를 쓸 수도 있고 아니면 뭐 클로드라는 모델을 쓸 수도 있고 라마3라는 오픈모델을 쓸 수도 있고요. 어쨌든 이걸 넣어서 우리가 얻는 답변은 뭐냐면 삼성전자가 자체 개발한 AI의 이름은 요거는 제가 채찍PT한테 물어본 거거든요. 답변을 이제 이런 식으로 준다는 거예요.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = [script[\"text\"] for script in scripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",torch_dtype=torch.bfloat16,device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 명확한 한국어 프롬프트 템플릿\n",
    "prompt_template = \"\"\"이 문서의 오탈자와 어색한 표현을 전문 교정자의 입장에서 자연스럽게 수정해주세요.\n",
    "다음과 같은 사항을 중점적으로 검토해주세요:\n",
    "1. 문맥에 맞지 않는 단어를 수정\n",
    "2. 영어 발음은 알파벳으로 변경\n",
    "3. 기술적인 용어는 원어로 변경\n",
    "4. 원본 텍스트의 구조를 수정하지 말 것\n",
    "\n",
    "원문: {prompt}\n",
    "\n",
    "교정 결과:\"\"\"\n",
    "\n",
    "# 입력 텐서 생성 및 GPU 이동\n",
    "text = scripts[0]\n",
    "model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors=\"pt\")\n",
    "model_inputs = {k: v.to(\"cuda\") for k, v in model_inputs.items()}\n",
    "\n",
    "# 생성 파라미터 설정\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.7,        # 창의성 조절 (0.0-1.0)\n",
    "    \"top_p\": 0.9,             # nucleus sampling\n",
    "    \"do_sample\": True,        # 다양한 출력을 위해 샘플링 사용\n",
    "    \"num_return_sequences\": 1, # 생성할 결과 수\n",
    "    \"top_k\": 50,              # top-k sampling\n",
    "    \"repetition_penalty\": 1.2, # 반복 방지\n",
    "    \"no_repeat_ngram_size\": 3  # n-gram 반복 방지\n",
    "}\n",
    "\n",
    "# 텍스트 생성\n",
    "outputs = model.generate(**model_inputs, **generation_config)\n",
    "\n",
    "# 결과 디코딩\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "# 원본과 교정본 비교 출력\n",
    "print(\"=== 원본 ===\")\n",
    "print(text)\n",
    "print(\"\\n=== 교정본 ===\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "import json\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "youtube_cookies = os.getenv(\"YOUTUBE_COOKIES\")\n",
    "\n",
    "cookie_data = base64.b64decode(youtube_cookies).decode(\"utf-8\")\n",
    "\n",
    "# 쿠키가 JSON 형식인지 확인하고 Netscape 형식으로 변환\n",
    "json_cookies = json.loads(cookie_data)\n",
    "\n",
    "# Netscape 형식으로 변환\n",
    "netscape_cookies = \"# Netscape HTTP Cookie File\\n\"\n",
    "for cookie in json_cookies:\n",
    "    if (\n",
    "        \"domain\" in cookie\n",
    "        and \"path\" in cookie\n",
    "        and \"name\" in cookie\n",
    "        and \"value\" in cookie\n",
    "    ):\n",
    "        secure = \"TRUE\" if cookie.get(\"secure\", False) else \"FALSE\"\n",
    "        http_only = (\n",
    "            \"TRUE\" if cookie.get(\"httpOnly\", False) else \"FALSE\"\n",
    "        )\n",
    "        expires = str(int(cookie.get(\"expirationDate\", 0)))\n",
    "        netscape_cookies += f\"{cookie['domain']}\\tTRUE\\t{cookie['path']}\\t{secure}\\t{expires}\\t{cookie['name']}\\t{cookie['value']}\\n\"\n",
    "\n",
    "cookie_data = netscape_cookies\n",
    "\n",
    "ydl_opts = {\n",
    "\"quiet\": True,\n",
    "\"no_warnings\": True,\n",
    "\"extract_flat\": True,  # 기본 정보만 추출하도록 변경\n",
    "\"nocheckcertificate\": True,\n",
    "\"ignoreerrors\": True,\n",
    "\"no_color\": True,\n",
    "\"socket_timeout\": 30,  # 소켓 타임아웃 설정\n",
    "\"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "\"cookiefile\": cookie_data\n",
    "}\n",
    "\n",
    "ydl = YoutubeDL(ydl_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://youtu.be/VKIl0TIDKQg?si=2p-s_Fd_ww2C9oJ2\"\n",
    "info = ydl.extract_info(url, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_url 추출 및 저장\n",
    "audio_url = None\n",
    "for i in info.get(\"requested_formats\"):\n",
    "    if i.get(\"vcodec\") == \"none\":\n",
    "        audio_url = i.get(\"url\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper 모델 초기화 완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "\n",
    "\n",
    "model = WhisperModel(\n",
    "    \"large-v3\", device=\"cuda\", compute_type=\"bfloat16\"\n",
    ")\n",
    "model = BatchedInferencePipeline(model=model)  # 배치 모델일 경우\n",
    "print(\"Whisper 모델 초기화 완료\")\n",
    "\n",
    "\n",
    "segments, info = model.transcribe(\n",
    "    audio_url,\n",
    "    batch_size=32,  # 배치 모델인 경우\n",
    "    repetition_penalty=1.5,\n",
    "    beam_size=10,\n",
    "    patience=2,\n",
    "    no_repeat_ngram_size=4,\n",
    "    initial_prompt=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_segments = [\n",
    "    {\"start\": segment.start, \"end\": segment.end, \"text\": segment.text}\n",
    "    for segment in segments\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0.0,\n",
       "  'end': 15.936,\n",
       "  'text': ' 네 안녕하세요 드디어 기다리고 기다리던 mcp 한번 보도록 하겠습니다. MCP 요즘 정말 핫한 것 같아요 저도 최근에 사실 MCP 관련된 영상들도 많이 참고해보고 저희 자체적으로 도 팀 내부에서'},\n",
       " {'start': 15.936,\n",
       "  'end': 32.688,\n",
       "  'text': ' 많은 테스트와 실험을 했었습니다. 일단 결론은 앞으로 MCP가 대세 가 될 것임에는 이견이 없습니다 하나의 판 을 뒤흔드는 그런 키 역할을 하게될것이라고 생각이 드는데요, mcp에 대해서 잘 아시는 분들도 계시고 잘 모르시는분들 도계실겁니다.'},\n",
       " {'start': 32.688,\n",
       "  'end': 62.016,\n",
       "  'text': ' 이게 궁금하신 분들도 아마 계실 거예요. 그래서 오늘은 이 MCP에 대해서 간단하게 설명을 드리고요, MCP가 왜 최근의 인기가 있는지 그리고 제가 생각하기에 앞으로 왜 mcp 가 대제가 될 것 같은 지 그걸 정리해서 설명드리도록 하겠습니다 자 먼저 여기에 보시면은 MCP Introducing the Model Context Protocol 이라고 해서 2024년도 11월 26일날 MCP가 나오게 되었습니다. 이 MCP 라는 것은 다른 영상에서도 내용들을 보신 분들은 잘 아실 거에요 모델 컨텍스트 프로토콜 입니다'},\n",
       " {'start': 62.016,\n",
       "  'end': 86.288,\n",
       "  'text': ' 여기서 핵심은 프로토콜이라고 볼 수가 있는데요. 한마디로 프로토컬 은 귀이약 같은 겁니다 뭔가 약속과 같이 보시면 되요 그래서 대표적인게 뭐 hdp 통신이나 아니면 api 이런 것들을 많이 언급되고 있습니다 제가 주목한 건 이거였어요 mc p 가 11월 26일 작년에 나왔거든요 그런데 요즘의 ai 시대에서는 무언가 새롭 게 나오면 당일날 엄청나게 많은 관심이 몰려요'},\n",
       " {'start': 86.288,\n",
       "  'end': 115.6,\n",
       "  'text': ' 11월 26일날 나왔음에도 불구하고 왜 도대체 지금 인기인가 이게 아마 궁금하신 분들도 계실 수가 있을 것 같아요 그래서 그 부분에 대해서도 같이 한번 설명을 드리도록 할게요 자 먼저 MCP 에 대해서 살펴보기 전에요 이 agent 의 구조에 대해 먼저 살펴 볼 필요가 있습니다. 자,이 Agent의 구조를 대략적으로 보면은 여기에서 LLM이 핵심 역할일 하고요 LLM 은 우리가 GPT를 사용 할 수도 있고 뭐 클로드를 사용할수도있고 재미나이를 사용할 수 있겠죠 그리고 이 agent 가 좋은 점 중에 하나가 바로 여기에 나와있는 외부 투를 쓸수 있다는 겁니다 그러니까 기존의 llm 에서'},\n",
       " {'start': 115.6,\n",
       "  'end': 120.56,\n",
       "  'text': ' 능력 확장이 가능하다는 거죠. 그뿐만 아니라 이 React라는 프레임워크 덕분에 얘가'},\n",
       " {'start': 120.56,\n",
       "  'end': 149.872,\n",
       "  'text': ' Thought, Action, Observation 과정을 루핑을 하면서 문제 해결을 스스로 해결나간다. 이런 것들도 에이전트의 큰 장점이라고 볼 수 있습니다 크게 정리하자면 에이전드에 큰장 점은 외부 툴 을 사용할수 있다와 그 다음에 React 프레임워크라고 볼 수가 있는데요 여기에서 바로 MCP 의 역할 은 어느 부분을 담당 하냐면 여기에 나와있는 이 Tool부분을 닮 당한다고 볼수가있습니다 물론 mcp 에도 뭐 prompt나 리소스 등을 사용 할 수가 있는데 요 사실 제가 봤을 때는 그게 메인은 아니라 고 생각이 들어요'},\n",
       " {'start': 149.872, 'end': 151.312, 'text': ' 이게 굉장히 매력적인'},\n",
       " {'start': 151.312,\n",
       "  'end': 180.624,\n",
       "  'text': ' 포인트라고 생각이 듭니다 자 이게 무슨 말인지 조금 더 살펴보도록 할게요 만약에 우리가 에이전트를 만든다 라고 했을 때 랜 체인이나 맹 그래프로 의 전투를 만들어요 그러면 냉체 인 안에 있는 어떤 코드를 가지고 낸 채 난의 integration 되어 있는 툴 들을 결합 을 시켜서 리액트 프레임워크 는 내장이 되어있거든요 그래서 agent 라는 걸 만듭니 다 오리간 앵 체앤 할앤드 앞으로 애정 또 만들어 서 다양한 뭐 그 플로우를 구성한다 알지 아니면 문제 해결했을때 프로그래밍 적으로 해결될 수가 있다는 거 거든요 근데 이 엘전 트라는 것은 물론 2025 년도에 대세라 고 하지만 굉장히'},\n",
       " {'start': 180.624,\n",
       "  'end': 206.144,\n",
       "  'text': ' 새로운 개념은 아니에요 이미 작년부터 알고 있었던 내용이라는 거죠 자 근데 제가 아까 말씀드렸다시피 mcp 가 이 툴 역할을 대체한다고 했는데 이게 왜 도대체 요즘에 굉장히 관심이 일으키고 있는지 그 부분에서 좀 더 살펴볼 필요가 있어요 여기 구글 트렌드 보시면요 최근의 인기가 올라오는데 MCP 나온 시점이 한 요 정도 되거든요 그런데 엔씨 피가 나오 나서 인기 엄청 많았느냐 그게 아닌 거예요 그냥 그럭저럭 했습니다 그러다가 어느 시점부터'},\n",
       " {'start': 206.144,\n",
       "  'end': 218.24,\n",
       "  'text': ' 갑자기 이 곡선 보이시죠? 마치 상한가 가듯이 갑자기 뛰어오르는 시기가 있는데요. 이 시기에 주목해볼 필요가 있습니다 자, 이식이가 언제쯤 되냐면 바로 Cursor AI의 MCP 업데이트 되는시점 2월 19일 정도 돼요'},\n",
       " {'start': 218.24,\n",
       "  'end': 247.536,\n",
       "  'text': ' 2월 19일 정도면 대략적으로 한 20점쯤 되거든요. 그러면 컬서 AI가 MCP 업데이트 되는 게 왜 MCP의 인기를 불러왔는가 여기에 주목해 볼 필요가 있다는 거죠. 컬서 ai 가 mcp 업 데이트되는 것과 최근에 엠씨피가 왜 이렇게 인기가 많게 됐는지 그거를 이제 연관 지어서 한번 생각해볼게요 다시 처음으로 돌아와 서 처음엔 c p 가 나오고 나서 MCP 도대체 뭐길래 하고 좀 살펴봤어요 근데 제가 아까도 말씀드렸다시 피 여러가지 부각 기능들이 있지만 가장'},\n",
       " {'start': 247.536,\n",
       "  'end': 265.136,\n",
       "  'text': ' 역할을 해준다라는 거거든요. 그런데 이 툴 역활을 해준다는 게 저는 왜 도대체 이게 새로운 거지? 사람들이 이렇게 열광하는거지 라는 의문점 을 가지게 되었어요 그러니까 저의 입장에서는 랭 체인이나 아니면 램 그래프나 이걸로 이미 인테그레이션 되어 있는 툴사용해서 에이전트 많이 만들 수 있거든요'},\n",
       " {'start': 265.136,\n",
       "  'end': 293.008,\n",
       "  'text': ' 랭체인에 인테그레이션 되어 있는 툴을 한번 보여드릴게요. 렝 체인을 가보시면은, 툴들이 이렇게나 많습니다 여기에 Rangchain Tools를 클릭해볼게요 여기 보시면 서치툴으로 코드 인터브리터가 있고 Productivity 웹블라우징 데이터베이스 그 다음에 이렇게 많은 털들로 인테끼를 이미 되어있어요 여기서는 이제 많은 사람들이 요즘 MCP 활용해서 사용기에 자주 등장하는 파일 시스템들을 건드린다든지 구글 드라이브 생산성 관련 투랑 연관 지어서 만들어 볼 수있는 것들 이런것들을 가지고'},\n",
       " {'start': 293.008,\n",
       "  'end': 317.168,\n",
       "  'text': ' 다양한 예제들을 쏟아내고 있는데 사실 제가 봤을 때는 어 이게 왜 새로운 거지 라는 생각을 했었던 거죠 자 그런데 제가 잘못 생각한 부분이 하나 있더라구요 저의 경우에는 랭체인이나 램그래프를 사용하는 소수에 사람이라는거죠 이 렝 체인랑 그래프 를 사용해서 2 툴들의 인테그레이션 써서 에이전트를 만들 수 있는 사람들은 일부 개발자 일 것이에요'},\n",
       " {'start': 317.168,\n",
       "  'end': 347.04,\n",
       "  'text': ' 공개가 됐다는 점이죠. 이 MCP라는 툴이 누구나 쉽게 쓸 수 있도록 외부 마켓플레이스 같은 곳에 등재가 되었다 하더라도요 여기에 보시는 것처럼 이렇게 아무런 관심을 받지 못했다란 거죠 즉 이건 어떻게 해석해 볼 수가 있느냐 하면 우리가 누구나 쓰일수있도록 여기서 누군하라고 하는건 뭐냐면 랭체인이나 렌그래프와같은 개발 프레임워크의 종속적이지 않기에 완전히 독립적인 투를 외부로 공개되었다고 해도 그래도 그렇게 관심도 못받으실수가 있는거에요 그래서 여기에서 관리할수밖에 없는 이유가 있죠'},\n",
       " {'start': 347.04,\n",
       "  'end': 373.616,\n",
       "  'text': ' 이 툴을 받아줄 수 있는 LLM과 리액트 프레임워크 이걸 구동할수있는 어떤 몸체가 없는거에요 말그대로 손발만이 있는 거고 머리와 몸이없는 상황인 거죠 그러니까 사용자로부터 아무런 관심 받지 못했어요 왜냐면,사용처 자체 없다 보니까 그런데 Cursor AI가 혜성처럼 등장해서 영웅의 역활하게 되는것이죠 MCP 업데이트 해주면서 이 LRm 과 그 다음에 몸체역찰까지 다 해준 거예요 한마디로 여기 부분 있죠? 요부분'},\n",
       " {'start': 373.616,\n",
       "  'end': 398.768,\n",
       "  'text': ' 이 부분이 Cursor AI가 담당을 하게 되는 거고 MCP의 툴들이 역할을 하는 거예요. 이거 두 개를 결합하면 내가 코딩하지 못하더라도 누구나 쓸 수 있게 됩니다. 어떤 상황이 되냐면, MCPs 컨셉은 너무 좋았으나 이게 자칫 관심을 받지 않고 그냥 부러져 버릴 수 있던 프로젝트를 완전히 멱살 잡고 올려준 케이스라고 볼수 있는 거죠'},\n",
       " {'start': 398.768,\n",
       "  'end': 425.248,\n",
       "  'text': ' MCP에 대해서 도대체 이 도구들을 어떻게 외부에서 쓸 수 있도록 만들어 준다는 거냐 이거 에 대해 볼 필요가 있는데 영상 초반의 말씀드린 것처럼 모델 컨텍스트 프로토콜 이에요 이게 왜 중요하냐면 랭 체인 보시면 이렇게 많은 좋은 툴들이 인테그레이션 되어 있다고 했잖아요 그런데, 이 레이첼툴은 어디서만 사용할수 있느냐면요? Rang chain ecosystem 안에서만들어갈수 있는거죠 그러니까 내가 렝채인을 배우고 램 그래프를 배워야마 여기에 나와있는 좋은 투를 가지고'},\n",
       " {'start': 425.248,\n",
       "  'end': 448.416,\n",
       "  'text': ' 써먹어서 에이전트를 만들 수가 있는 건데 지금 이 툴들은 랭체인이나 램그래프의 의존성이 있잖아요. 이게 없으면 그냥 아무것도 쓸 수 없습니다. 모델 컨텍스트 프로토콜은 이거를 독립적으로 가져가겠다는 거예요 그러면서 본인들이 표준 규약을 만들고 클라이언드들의 표준대학에 맞춰서 개발해주면 어디서든 쓰일수 있도록 만들어 주겠다 라는 기념입니다'},\n",
       " {'start': 448.416,\n",
       "  'end': 477.744,\n",
       "  'text': ' 이걸 좀 더 쉽게 설명한 자료가 바로 여기 나와있는 이 유명한 자룐데 여기에 보시면은 USB-C 타입에 비교하는 걸 볼 수가 있거든요? 한마디로 너네 가 악세서리를 만들 때, USB C타입 에 맞춰서 만들기만 하면 제조사들이 usb c 타이프 지원하면 거기 어디든지 어떤 툴들 지 쉽게 꽂을 수 있다는 거에요. 랭체인의 사례를 보면 은 렝 체인 같은 경우에는 이 툴 들 을 많이 만들어 놓긴 했는데 쉽게 얘기해서 뭐 요런식 의 타이브로 만들어 놨던 거예요 이거는 맹체에 서면 호환이 되는 타임 인거죠 그러니까'},\n",
       " {'start': 477.744,\n",
       "  'end': 484.336,\n",
       "  'text': ' 여기서 열심히 만들어 놓긴 했고 굉장히 많은 툴들이 있으나 이거는 랭체인 안에서만 호환되는 아주 특수한'},\n",
       " {'start': 484.336,\n",
       "  'end': 509.328,\n",
       "  'text': ' 인터페이스를 가지고 있는 건데 이 MCP 라는 것은 한마디로 USB-C 타입 이런 표준 규약을 만들어 놨다고 볼 수가 있어요 이제 뭐 슬랙이나 지메일 이나 캘린더 이거 너 에이전트랑 연동시키고 싶어? 그리고 프레임워크 많으면 그거 하나하나를 다 언제 네가 인터베리셔링 하고 있을래. 그냥 usb c타입으로 만들어줘 그러면 제조사들이 알아서 USB C타입에다 차용하게 될 거고 그럼 여기다가 너네들 요거 하나만 개발해 두면'},\n",
       " {'start': 509.328,\n",
       "  'end': 538.608,\n",
       "  'text': ' 필요한 사람들이 다 꽂아서 쓸 거야. 이런 컨셉인 거예요 그래서 이 MCP 아키텍처 같은 경우에는 처음에 표준기약이 나왔을 때 콘셉트는 되게 좋았다 근데 여기서 문제가 있죠 어떤 문제가냐면 예를 들어서 과거 사례를 한번 보면요 USB-C 타입이라는 게 나왔던 애플, 삼성 이런데에서 만약에 usb c타입 쓰지 않아요 우리가 휴대폰 충전해야 되고 뭐 그 밖에 여러가지 군대로 써야 되는데 메이저 회사인 애플리나 3 성 에서 usc 탑의 초기에 거들떠보지도 않아 그럼 어떻게 되죠 여기에 있는'},\n",
       " {'start': 538.608,\n",
       "  'end': 565.36,\n",
       "  'text': ' 당연히 안 만들겠죠. 내가 열심히 USB-C 타입 만들어 봤자 얘네들이 안 써주니까 호환이 안되는 거예요 그 상황 이 뭐였냐면, 바로 요상황이었다 라고 볼 수가 있는 거죠 그러다가 usb c타입 전 세계에서 가장 많이 쓰는 Cursor AI 에서 인테그레이션 해준 거에요 그러니까 CURSOR AIA 유저가 엄청 많잖아요? 일단 내가 USB-c탭을 만들면 엄청나게 많은 유져를 보유한 cursor ai에 꽂을 수 있으니까 이제 여기로 사람들이'},\n",
       " {'start': 565.36,\n",
       "  'end': 594.816,\n",
       "  'text': ' 관심을 가지게 되는 거죠. 두 번째로는 여기에서 Cursor AI의 역할이 굉장히 중요했던 건 맞아요. Cursar AI 는 엄청나게 많은 유저를 이미 보유하고 있고 그리고 여기에다 인테그레이션 됐다라는 것은 순식간에 많은 사람들로부터 관심 받을 수 있는 거니까요 그거 자체만으로도 사실 의미가 있지만, cursor ai 가 integration 해주면서 또 좋은 점 하나 있죠? cursor aid 는 IDE 라는 에디터에요 이 에디턴은 그냥 우리가 일반 랭체인이나 램그래프로 개발하는 에이전트와'},\n",
       " {'start': 594.816,\n",
       "  'end': 613.776,\n",
       "  'text': ' 차원이 다르죠. 왜 차원이나르냐면 우리가 명령어를 내리면 단순히 채팅으로 답변 주고 끝나는 수준이 아니잖아요 얘는 에디터이기 때문에 그에디터를 포함되어 있는 수많은 기능들이 있어요 예를들어서 코드를 수정해 준다랄지 파일을 바로바로 접근해서 파일내용 을수 정해줘야 할 지 이게 에디더로서의 기본 기능 이거든요 그러니까 이런'},\n",
       " {'start': 613.776,\n",
       "  'end': 629.616,\n",
       "  'text': ' 갖추게 되니까 굉장히 매력도가 높아지게 되는 거죠 그래서 이 지금 USB-C 타입 표준을 개발한 게 클로드에서 MCP를 내놓은 거라고 볼 수가 있고, 현재로서는 이 usb c타입 을 만들기 위한 시장에 엄청나게 많은 솔루션 개발라는 사람들 혹은 내가'},\n",
       " {'start': 629.616,\n",
       "  'end': 650.48,\n",
       "  'text': ' 한번 인기있는 툴을 만들고 싶다 제 2의 카카오톡이 되고싶다는 사람들은 전부 다 뛰어들고 있다고 볼 수 있죠 개발자들이 툴대학에 많이 뛰어 들다보니까 지금 이 스미더리라는게 넘버원 모델컨텍스트 프로토콜 서버라고 보시면 되요. 마켓플레이스 라고 보면 돼요 투마케틀베이스데 엄청나게 빠르게 털들이 써밋되고 있습니다'},\n",
       " {'start': 650.48,\n",
       "  'end': 671.84,\n",
       "  'text': ' 그래서 매일같이 들어갈 때마다 이 숫자들을 확인해보면 엄청 빠르게 늘고 있는 것을 볼 수가 있어요. 그러니까 사람들이 이미 냄새를 맡은 거죠 처음에 스마트폰 시장에서 킬러 앱 만들어가지고, 엄청나게 인기를 얻어서 유니콘 된 회사들이 되게 많잖아요 그런 것처럼 아 이것도 내가 툴 잘 만들어서 여기 앱스토어에 들어가게 되면 나도 인기있는 프로젝트를 할 수 있겠구나 이런 생각들도 있는 것 같아요 여기서 끝이 아니죠'},\n",
       " {'start': 671.84,\n",
       "  'end': 684.464,\n",
       "  'text': ' 이게 선순환으로 물고 들어오는 건데 아까 앱스토어에 사용할 수 있는 툴들이 많이 늘어난다고 했잖아요. 이게 늘어나게 되면 할수있는 것들이 많아지니까 이 Cursor AI와 MCP앱 조합을 사용하는 사용자들의'},\n",
       " {'start': 684.464,\n",
       "  'end': 711.328,\n",
       "  'text': ' 엄청나게 늘어나는 거예요 이게 늘어난 게 되고 그로 인해서 요것도 드러나고 그럼 이 사용자들이 많은 유스케이스들을 창의적으로 많이 만들어 내고 있어요 예를 들어서 Figma를 그려낸다 아니면 DB에 접근한다라든지 코딩을 자동으로 작성해준다고 할 지요 이런 좋은 뉴스 케이스가 나오니까 더 많은 사람들이 유입이 되고요 그러면 더많은 툴들도 앞으로도 생겨야 될 거구요 그러니까 무서운 것이'},\n",
       " {'start': 711.328,\n",
       "  'end': 723.088,\n",
       "  'text': ' 이 안에서 하나의 생태계가 자연스럽게 조성이 되고 있는 거예요. 그러면 지금 현재로서는 클로드 데스크탑이랑 컬서, MCP 이렇게 3개의 합작품이라고 볼 수가 있는데'},\n",
       " {'start': 723.088,\n",
       "  'end': 752.368,\n",
       "  'text': ' Cursor AI만 인테그레이션이 될까요? 이제는 윈드서프도 지원을 하고 더 많은 IDE들이 들어와서 이 MCP를 지원하려고 하겠죠. 왜냐하면 이게 Cursor API가 독점해서 가져가면 안 되는 거니까 지금 ID에 국한되어 있는데 과연 코드 에디터 기반의 어떤 에딕팅 툴에만 국한이 될까요 저는 그렇지 않다고 봅니다. 이 mcp 라는 것은 말 그대로 USB-C 타입 같은 표준 규약인데, 사용할 수 있는 클라이언트가 다 어떤 에디터만 묶여 있거든요 근데 이 에디터 가 확장이 되가지고 다양한 어플'},\n",
       " {'start': 752.368,\n",
       "  'end': 777.008,\n",
       "  'text': ' MCP를 꽂아서 쓸 수 있는 그런 시장이 열리게 되겠죠. 이미 열리고 있고요 그래서 이 mcp 가 확장될 수밖에 없는 이유는 이거예요 지금 현재 유스케이스가 좋다 저는 이거는 굉장히 일부라고 생각이 들고, 생태계를 봤거든요 사람들이? 당연히 더 많은 툴들이 인테그레이션 자연스럽게 들어올 거고 더 많은 클라이언트들들이 많이 생길거구요'},\n",
       " {'start': 777.008,\n",
       "  'end': 792.432,\n",
       "  'text': ' 그래서 이 생태계가 커지지 않을까 라는 생각이 듭니다. 근데 여기서 재밌는 관전 포인트 중에 하나가 뭐냐면 최근에 오픈 AI에서도 툴들을 공개를 했어요. 그래서 툴을 공개했는데, 아직까지는 MCP를 지원하고 있지 않죠? 물론 API로 물려서 이렇게 쓸 수 있겠지만요'},\n",
       " {'start': 792.432,\n",
       "  'end': 821.712,\n",
       "  'text': ' 자체적으로 공식 지원은 하지 않는데 근데 MCP가 이게 너무 커지고 클로드 데스크탑과 연계성이 좋아지면서 자연스럽게 클라우드도 유저들도 늘어나는 것을 오픈 AI도 보고 있을 거거든요 그래서 앞으로의 오펀에 추이 도 굉장히 궁금하더라구요 이제 현재 는 USB-C 타입 으로 모두 통일 됐죠 애플도 결국에는 항복 했잖아요 라이트닝 케이블 버리고 usb c타입 로 왔습니다 이런 것처럼 mcp 를 사용하는 유저 들 이 많아 지고,mcp를 지원 하는 클래언트들이'},\n",
       " {'start': 821.712,\n",
       "  'end': 837.92,\n",
       "  'text': ' 정말 대제로 자리 잡을 수 있을 거란 생각이 듭니다. 아까 제가 랭체인 말씀을 드렸는데, 이제는 MCP를 지원하는 프로젝트들을 많이 내고 있어요. 근데 얘네들도 바보가 아니죠?'},\n",
       " {'start': 837.92,\n",
       "  'end': 865.472,\n",
       "  'text': ' MCP를 적극적으로 지원합니다. 왜냐면은 랭체인 같은 경우에도 툴도 개발을 하지만 이 몸체를 개발로 하잖아요, 프레임워크이기 때문에 그래서 어..랭 체인에서도 앞으로 적극적 으로 mcp 를 지원할 것이라고 예상이 되구요 랭 체에는 한시름 났죠 오히려 본인들이 나서서 툴들을 다 인테그리션 했어야 됐었어요 혹은 파트너사들에 막 면담 해가지고 이런것들의 이데게이션해달라 이렇게 요청했어야 되는데 오히려 2파트가 떨어져 나가면서 제가 봤을 때는 부당감이'},\n",
       " {'start': 865.472,\n",
       "  'end': 894.768,\n",
       "  'text': ' 덜해지지 않을까 라는 생각이 듭니다. 앞으로 방향성은 이제 이 클라이언트들 지원하는 클라이어트 들이 많이 늘어날 거고요 mcp 서버의 인테그레이션 되어 있는 툴들도 많이 늘어 날 것이라고 보고 있어요 여기도 보면 레퍼런스서버가 있고 뭐 써드파티있고 커뮤니티로서 가 있는데 여기에 나와있는 투 뿐만 아니라 엄청나게 많은 출들이 전세계 개발자에서 몰려서 계바를 할 것이다라는 생각이드려요 저는 이걸 보면서요 이번에 대한민국 기업들한테 굉장히 큰기회라고'},\n",
       " {'start': 894.768,\n",
       "  'end': 923.984,\n",
       "  'text': ' 개발 하시고 계신 분들이 이 좋은 솔루션을 mcp 로 많이 열어서 빨리 등록해 놨으면 좋겠어요 우리나라 시장 굉장히 좁잖아요 근데 lm 에 잘 호환될 수 있는 MCP 형식으로 랩핑 해가지고 마켓플레이스에 등목해서 이게 인기를 얻는다 라고 하면은 이거는 글로벌시장을 바로 진출하게 되는 겹이거든요 그래서 혹시라도 회사 중에서 MCP 의 등록할 만한 좋은솔우션 가지고계신분들이라면 그렇다면 이번에 빨리 엠씨피형 식으로 레필 해서'},\n",
       " {'start': 923.984,\n",
       "  'end': 944.08,\n",
       "  'text': ' 마켓플레이스는 현재 스미더리가 제일 인기가 많더라고요. 여기에 등록하시면 될 것 같아요 MCP를 구현하는 방식도 너무 간단해요 파이썬이나 그 밖에 다른 언어로 도 지원을 하는데요 여기 보면은 패스트 mcp 를 가지고 와서 툴로 이렇게 데코레이터를 씌워주면 바로 MCP 형식의 툴 로 지원하게 됩니다 전세계인으로 대상으로 서비스 할 때는'},\n",
       " {'start': 944.08,\n",
       "  'end': 958.912,\n",
       "  'text': ' 하면 안되고 좀 더 복잡하게 로직 구현이 되겠지만 제가 말씀드리고 싶은 거는 저도 이번에 구연을 해보니까 생각보다 구현력은 그렇게 어렵지 않다는 점이에요 그래서 여기에 보면 mcp 파이썬 stk 라고 나와 있거든요 요거에 문서 내용 한번 보시면서 개발해 보시는 것을 추천드리구요'},\n",
       " {'start': 958.912,\n",
       "  'end': 988.208,\n",
       "  'text': ' 이제 FATCAMPUS에서 강의를 하고 있는데 이번 주주총회의 주제로 MCPs를 잡았거든요 그래서 저희 팀 내부에 개발하고 있는 프로젝트들도 이번 주주를 소개해드릴거니까 혹시 관심 있으신 분들은 세미나 신청도 해주시면 되겠습니다 마지막으로 제가 오늘 개발됐던 유스케이스들을 몇가지 소개를 해드리고 직접 시연을 보여드리려고 해요 여기에서 보여드릴게요 먼저 4 가지를 가지고 왔는데요 첫번째로는 Rangchain-RegLocal 요거를 mcp로 만들어놨고요 이걸 만들어 놓으면 뭐가 좋냐면 내가 랭체인 코드를'},\n",
       " {'start': 988.208,\n",
       "  'end': 1006.0,\n",
       "  'text': ' 이걸 MCP 서버로 띄우는 거예요. 그러면 Cursor AI가 이 Rangchain으로 만들어 놓은 레그 시스템을 참조하고 그리고 클로드 데스크탑에서도 이것이 참조될 수 있게 돼요 또 DeFi로 물려서, 디파이에 External Knowledge API 가 있어요.'},\n",
       " {'start': 1006.0,\n",
       "  'end': 1028.272,\n",
       "  'text': ' Cursor AI나 클로드 데스탑에서 DeFi로 요청을 보내서 External Knowledge를 참조해서 그걸 가져오게 끔 이렇게 만들어 놓는 예제들도 하나 만들어놨구요 그 다음에 디파이 워크플론인데 제가 최근에 디 파의 관련된 영상들 많이 올렸죠. 디파이로 쉽게 워크를 구축하실 수가 있는데, 그 구출해 놓은 workflow 를 호추하는거 이 예제를 하나만 들어 놨고요 그리고 마지막으로 Python function 인데 간단하게 우리가 파이썬 펀션 으로 커스텀한'},\n",
       " {'start': 1028.272,\n",
       "  'end': 1045.488,\n",
       "  'text': ' 기능들 로직들을 구현해 놓고 그거를 mcp 서버로 띄운 다음에 호출하는 예제를 만들어 놨어요 여기에서 사용한 예드는 태블릿 웹서치 도구를 쓰는 예제 를 만들어놨구요 4가지 사례 정도면 Rangchain, DeFi 그다음에 파이썬 대부분은 커버가 가능하시니까 한번 보셔도 좋을 것 같습니다 프로젝트는'},\n",
       " {'start': 1045.488,\n",
       "  'end': 1070.592,\n",
       "  'text': ' 현재는 인터널로 되어 있지만 곧 공개할 예정이에요. 공개를 하고 나서 영상 더보기란에 제가 링크를 주도록 하겠습니다 이 프로젝트를 다운받아서 쓰시면 되겠고요 영어로도 공개되었고 저희가 한국어 버전들도 만들어 놨거든요? 이거를 보시면서 따라 하시며 무리없이 수행이 될거에요 제가 이걸 라이브로 한번 데모를 보여드릴게요 자 먼저 저는 지금 MCP 서버를 이렇게 물려놨는데요 이것을 사용하시는 방법은 굉장히 간단합니다 프로젝트의 다운 로드 받은 다음에 먼저 케이스 1 쪽으로 가볼게요 CD-1'},\n",
       " {'start': 1070.592,\n",
       "  'end': 1099.44,\n",
       "  'text': ' 이렇게 가서 여기 왼쪽에 보시면은 .env.example이라고 되어 있어요 환경 변수들을 등록하는 예제고 example1 해보실 분들은 요거, 2해보셨을분들은 이거 3는 이거랑 4는 이것 이거를 설정하신 다음에 실행하시면 돼요 당연히 exmpello로 실향 하시 면 안되구 저처럼 점 emb 로 바꿔서 진행해주면됩니다 자 이렇게 바꾸어서 키값들을 다 세팅하셨으면 그러면 여기에 case 1에 가셔서 파이썬의 저희가 오토컴피그를 만들어 놨어요 auto mcp json 이라고 만들었는데'},\n",
       " {'start': 1099.44,\n",
       "  'end': 1126.88,\n",
       "  'text': ' 이걸 실행하면 API키를 받아다가 알아서 만들어줘요. 데이슨 파일이 이렇게 나오게 되거든요? 여기 OpenAI의 키값 들어가 있고, 이것을 실행하는 것들도 잘 나와있어요 이거를 어떻게 하시면 되냐면 레그부터 여기까지를 복사하시는 거예요 복사를 해서 세팅스 가실 수가 있거든요 톱니바퀴 누르고 Add New Global MCP Server라고 나와 있거든 요 눌러볼게요 자 눌러 보면 저는 등록이 많이 되어 있어요 원래는 처음 등목하신 분들에게 다 비워져 있을 거에요 그러면 지우고 여기에 붙여 넣으세요'},\n",
       " {'start': 1126.88,\n",
       "  'end': 1145.968,\n",
       "  'text': ' 레그 MCP는 랭체인 코드로 만들어놓은 도구를 사용해서 검색을 해주고 있습니다. 이렇게 하면, MCP에 파란 불이 들어오면 세팅된거에요'},\n",
       " {'start': 1145.968,\n",
       "  'end': 1175.44,\n",
       "  'text': ' 테스트를 해볼 차례인데, 문서가 2025년 3월 인공지능 산업의 최신 동향이거든요. 이렇게 쭉 나와 있어요 여기 있는 내용들을 바로 REG를 해 볼 수가 있죠 제가 13페이지에 있는 GROK3 에 대한 내용을 물어볼게요 여기에 도구 이름은 레그 MCP로 작성해 놨잖아요 그러면 물어 볼때 REGMCP 를 사용해서 그록 쓰리 출시일 언제인지 알려줘'},\n",
       " {'start': 1175.44,\n",
       "  'end': 1200.784,\n",
       "  'text': ' 저희가 만들어 놓은 도구를 호출해서 답변을 주게 돼요 자 얘 하이브리드 서치를 했죠 기본적으로 이제 하이 브리트서치 해서 설치로 리저르트를 보면 pdf 문서를 레그로 가져온 것을 볼 수가 있어요 이렇게 가졌었지요 여기에서 만약에 hybrid search가 아니라 semantic-search로 검색을 하고 싶다 요거를 수정해 볼게요 Semantic Search 해 줘 이렇게 쓰면 은 도구호출날 적에 시멘틱 SEARCH를 할 거예요'},\n",
       " {'start': 1200.784,\n",
       "  'end': 1217.568,\n",
       "  'text': ' 자 이번에는 시멘틱 서치해서 결과를 주는 것을 볼 수가 있습니다 그리고 마지막으로 이것도 한번 해볼까요 출처를 답변에 포함해줘 이렇게 쓰게 되면은, 출렬을 답변을 통해 말해주는 걸 볼 수 있죠 이런 식으로 우리가 만들어 놓은 랭 체인 레그 시스템 기반 으로'},\n",
       " {'start': 1217.568,\n",
       "  'end': 1245.904,\n",
       "  'text': ' 답변하는 거를 컬서에서 바로 테스트 해볼 수가 있어요 근데 이게 컬서의 조그만 창 에서 해 보시기 불편하다 하신 분들은 클로드 데스탑에서도 할 수 있거든요 이렇게 열리게 되면은 상단에 클롸드 의 세팅스 라고 있어요. settings 가서 디벨롭퍼 에딧컴픽을 하고 있습니다 요거를 열 면 은 제이슨 컨피그 파일이 있는데 이거를 펄서로 열으면되요 자,열얼어볼게요 자 열면요로케 나오는데 여기에다가 요렇게 regmcp라고 해서 추가를 해주는 거예요 저렇게 추가했으면 이걸 프로그램 을 껐다 가 다시 한번 켜봐야 돼요'},\n",
       " {'start': 1246.288,\n",
       "  'end': 1273.952,\n",
       "  'text': ' 켜주도록 하겠습니다. 자,켰죠? 그러면 여기 도구에 보면요 저희 하이브리드 서치랑 그 다음에 키워드도 설치 그리고 시멘틱서치가 이렇게 레그 MCP가 잘 들어와 있는 걸 볼 수가 있어요 여기에다가 똑같은 질문을 해보는 거죠 저희가 했던 요거 내용을 복사해 가지고요 여기다 가두 붙여 넣어서 해 볼게요 그럼 이 툴 을 쓰게 되죠.레그렉시피 알로우 폴디스챗! 요렇게 해주고 검색 해오고 주요 정보를 알려주는 것을 볼 수 가있어요 스프리 AI 브루프의 3월호까지 잘 가져오져 그러니까'},\n",
       " {'start': 1273.952,\n",
       "  'end': 1287.92,\n",
       "  'text': ' 우리가 앞으로는 클로드 데스탑에 랭 체인으로 만들어 놓은 레그 시스템을 붙여서 해 볼 수가 있는 거예요. 저기다가 문서들만 넣어주면 저는 클롸드에서 그 문서를 기반한 레그를 구축할 수 있습니다 원래 이 챗볼시스템만 만들기 위해서 어떻게 했었죠? 스트림님,'},\n",
       " {'start': 1287.92,\n",
       "  'end': 1317.2,\n",
       "  'text': ' 프론트 앱 개발해서 만들었잖아요 클로드 데스크탑과 컬사 AI가 있으니까 이 프런트가 필요 없어지는거에요 우리가 백엔드땐 즉 랭체인으로 구현을 하시던 디파이로 구현된다 아니면 그 밖의 어떤걸로 개발되어 있어도 그것만 해 놓으면 쉽게 레그 시스템이 될 수 있는 거에요 하나만 더 보여드릴게요 케이스 3에 대한 내용인데 DeFi 워크플루어랑 물려놨습니다. 우리가 디 파이웍 플러우에서 이것보다 훨씬 복잡한 걸 만들어 둘수있겠지만 저는 테스트 삼아서 아주 간단하게 해봤거든요 이번에는 지식검색 을 추가해놓은 거예요 동일한 파일들'},\n",
       " {'start': 1317.2,\n",
       "  'end': 1345.456,\n",
       "  'text': ' API 액세스에 가서 여기 있는 api 서버와 그 다음에 Apiky를 발급을 받습니다 그래서 이 샘플 에있는 api url 그다음에 APIKY 를 넣어주시면 하면 돼요 그러면 내가 만든 워크플로우를들 클로드 데스크탑 이랑 컬사 AI에서 쓸 수가 있게 되는 상황이거든요 자,그러면 반행 버튼 눌러주고 CD의 케이스 3로 가세요 파이썸 의 오토 MCP 제이슴 만들어줍니다 만들어 주며는 이렇게 생성이 되 거든요 요 내용 중에서'},\n",
       " {'start': 1345.456,\n",
       "  'end': 1374.96,\n",
       "  'text': ' 디파이 워크플로우 부터 복사를 해가지고 클롸드 데스탑 컨피그에다가 이렇게 붙여 넣어줄게요 자, 디 파이워크루 플러우를 붙였습니다 그러면은 이제 저희가 하는 거는 데스탑에서 디 파이어 그 프로를 호출해서 답변을 받을 거에요 저렇게 셋업 해줬고 클론 대사 종료할 게요 다시 껐다 킵니다 다 켰구요 이제 도구가 하나 더 들어 낫거든요 이걸 눌러 보면 은 d5 워크 플라워 라는 도구 가 생긴 것을 볼 수가 있어요 얘를 가지고 사용 해서 그룹'},\n",
       " {'start': 1374.96,\n",
       "  'end': 1404.16,\n",
       "  'text': ' 3의 출시일을 알려줘 라고 해볼게요. 그러면 디파이 워크플로우를 사용해보겠다고 하죠? 허용해주고요 우리가 만든, 디파이에서 만드는 워크 플러워를 통해서 정보를 얻었습니다 그 얻은정보 바탕으로 이렇게 답변을 해주는거에요 자!이렇게 잘 나왔져 여기에 내용도 보면 스프리 AI 브뤼프에서 검색해준 거구요 정리하자면 우리가 이제 디 파이에서 만들어놓으신 수많은 웍클 프로랑 굉장히 손쉽게 우리 클롸드 데스크탑 을 붙여가지고 쓸 수 있게 되는 겁니다 마지막 으로 제가'},\n",
       " {'start': 1404.16,\n",
       "  'end': 1429.984,\n",
       "  'text': ' 여기에 영감을 얻어서요, 만든 랭체인 렝그래프 바이브 코더 프로젝트를 보여드릴게요. 이 RangchainRengraphBiveCoderProject는 RngChainAR에서 최근에 공개한 MCPDoc이라는 Project가 있는데요 거기서 영감 받아서 저만의 방식으로 한번 풀어봤어요 이거 의 동작방식은 저희가 Cursa AR이나 Cloud Desktop을 물릴 건데 제가 구현 한 부분 은 여기서부터 여기까지 라고 보시면 됩니다 결국 목적이 거에요 CurseAR를 사용해서 제가 그동안 만들어 놨던 수많은 튜토리얼'},\n",
       " {'start': 1429.984,\n",
       "  'end': 1459.776,\n",
       "  'text': ' 그 다음에 최근에 진행했던 랭체인 오픈 튜토리얼 자료들 그리고 마지막으로 Rangchain API 레퍼런스 도큐멘테이션을 사용해서 코딩 외 손은 최대한 대지 않고 우리가 프롬프트 입력만 으로도 프로젝트를 얼마나 잘 수행할수 있는 그런 램그래프를 만들 수있는지를 시험해보고자 했던거에요 많은 분들 아시겠지만, 렁 체인나 렌 그래프코드는 굉장히 빨리 바뀌게 되는데 그래서 GPT나 클로드 에 물어봐도 제대로 된 코드를 안 줍니다. 그말은 무조건 reg system 을 써야지 가능한 수준의 코딩을 받을 수 있다는 건데 지금 현재로서는 웹 서치 투를 붙이다 하더라도'},\n",
       " {'start': 1459.776,\n",
       "  'end': 1487.28,\n",
       "  'text': ' 제대로 된 코드에 대한 피드백을 안 줍니다 그래서 랭그래프나, 렝체인코드를 우리가 프롬프트 입력으로 자동화 하는 것은 굉장히 어렵고요 여러분들 혹시 유피티한테 물어봐서 코팅 하신 분들은 대부분이 엄청 옛날 코드는 아웃데이트 되는 코드만 줄 거에요. 거의 시도하는 것 조차 저는 시간 낭비라고 생각이 듭니다 그런데 대세가 바이브코닝 하는 시대 잖아요? 제가 어떻게 하면은 발표할 수 있을까 고민하다가 지금까지 만들어놨던 자료와 그 다음에 링 체인을 제공한 공식 API 레퍼런스 그리고 다양한 쿱북 예제들 쭉'},\n",
       " {'start': 1487.28,\n",
       "  'end': 1516.96,\n",
       "  'text': \" 긁어 모았어요. 문서화 시키니까 엄청나게 내용이 방대하더라구요 그래서 그거를 두 가지로 했는데 먼저 목록형으로 나열된 LLM's text라고 있는데 최근에 웹사이트 다 이걸 두자 이거예요. LLMs들이 웹 사이트 정보를 쉽게 가져갈 수 있는 마크다운 형태라고 보시면 되요 그래서 목록형 으로 lms 텍스트의 형식을 맞춰 만들어놓은 그런 지식 데이터베이스를 구축해놨고 또 하나는 벡터 서치 여기에있는 내용들을 chunking 하니까 뭐 2만개 이상의 chunks가 나오더라고요\"},\n",
       " {'start': 1516.96,\n",
       "  'end': 1540.752,\n",
       "  'text': ' 간단한 리트리버를 하나 만들었어요. 이 레트리 버는 랭그래프 클라우드에 현재 호스팅을 해 놓은 상태고요 그러면 이 플로우가 어떻게 되냐면 만약 사용자가 Rangraph 관련된 코딩 을 해줘 라고 요청을 내리게 되면 서버 가 먼저 란 그래프로 클라우드로 요청하게 됩니다 이를 링크 클라브 안에는 벡터 db 에 이미 저장되어 있는 정보들과 lms 텍스트의 정보 들이 가지고 있어요 그럼 이안 앱래 프레 태워서 지식 검색 수행 하게 되고 2 코디너 쇼 하기 위한 정보를 반환해 줍니다'},\n",
       " {'start': 1540.752,\n",
       "  'end': 1567.952,\n",
       "  'text': ' 현재 제가 간단하게 호스팅해놓은 랜그래프 클라우드에 배포된 이 리트리버를 볼 수가 있는데요. 이 레트리가 바로 지금까지 정리한 자료들과 API 레퍼런스들을 검색해주는 그런 Retriever라고 보시면 돼요 구조 자체는 굉장히 심플합니다 그런데 이 Retieber 가 이제 벡터 딥이 저장되어 있는 곳에서 검색을 해서 결과를 반환해 줄 거에요 예시로 보여드리면 self query retrievers 라고 한번 검색 해 봅시다 이렇게 하면 결과 들이 나오거든요 여기 뭐 Raptor 도 있고 몇 가지가 있습니다'},\n",
       " {'start': 1567.952,\n",
       "  'end': 1591.328,\n",
       "  'text': ' 클릭을 했더니 이제 셀프 커리 검색기 라고 해서 요 내용들이 나오는 걸 볼 수가 있어요 자 그런데 여기서 재미난 사실은 이 관련된 내용들의 검증이 되는데 지금 링크 들만 주고 있는 것을 볼 수 있거든요 이게 왜 그러냐면 우리가 에이전트를 쓰게 되면 은 안에있는 내용들을 한번에 다 줄 필요가 없어요 이렇게 링크들을 반환 해주면 패치 url 도구를 하나 더 만들어 놨거든요 그럼 agent 가 이링 크드를 배치를 해 가지고 결과를 한 번 더 가져옵니다 가져와서'},\n",
       " {'start': 1591.328,\n",
       "  'end': 1621.28,\n",
       "  'text': ' 그 내용들을 하나씩 보면서 얘가 알아서 적용하기 때문에 제가 해보니까 컨텐츠를 바로 반환을 하는 것보다 그냥 이 링크들을 주는 게 더 낫겠더라고요 그래서 이런 식으로 넣어놨고요 지금 랭체인 DevDocs를 새롭게 추가해 놓은 상태구요 자 그러면 여기 빈 파일인데, 여기에 명령어를 한번 내려보죠.'},\n",
       " {'start': 1621.28,\n",
       "  'end': 1650.448,\n",
       "  'text': ' 이렇게 해볼게요. 이 키위의 BM25 사용하는 예제는 정말 없거든요 제가 만들어 놓은 튜토리얼을 잘 검색해서 가지고 오는지 한번 볼게요 자 이러면 Search on RankChain Knowledge Base를 검색하거든요 그래서 이제 Wikidocs랑 몇 가지 예제들을 지금 검색했어요 이렇게 FetchDock 을 해가지고 링크들 쭉 들고 옵니다 얘도 fetch dock 하고요 쭈쯕쫑들고 와서 얘가 키파이파이랑 랭크BM 25를 설치해요 알아서 설치를 하죠 그 다음에 kiwi-bm25 example을 만듭니다 만들었는데 제가 위키독스에 올려놓은 예제가 요거예요 요런 식으로 올려 놨 거든요 이거를 잘 가져오는지 한 번 보께'},\n",
       " {'start': 1650.448,\n",
       "  'end': 1677.664,\n",
       "  'text': ' 자 이런식으로 쭉 가져왔구요. 여기 accept만 누르면 되죠? 요것도 accept 보면은 파일을 만들었습니다, 자 만들었고요 예제에보면 금융보험금융저축보험보험이런예제들쭈욱 가지고 오잖아요 해서 이렇게 만드거든요 얘가 근데 여기에 예제를 보며는 이거 진짜 괴할한.. 예젠데...이거 제가 만들어 놓은 예재에요 요렇게 잘 검색해가지고 가지고 온 거를 볼 수가 있어요 그래서 애가 이르케 만들고 그 다음에 토큰하이즈도 만들구해서 수행까지 해가지구 리절트 까지 보는 거 요롷게 만들어주 구요 요거를 저희가 갖고 있는 레그체인이랑 결합해 가지구'},\n",
       " {'start': 1677.664,\n",
       "  'end': 1699.2,\n",
       "  'text': ' 메인 실행하면 이렇게 나오는 것까지 굉장히 잘 만들어 준 걸 볼 수가 있죠 요거를 나중에 스트리밍으로 배포하고 싶다 이러면은 스트림 위주로 베프 해주면되요 지금 실행하다가 오류가 났거든요 api 키 없어서 그런데 API 키만 넣어주면 될것 같아요 MCP 유스케이스는 퍼블릭 으로 공개를 해서 영상 더보기 란에 링크를 둘겁니다 여러분들이 여기에 나와있는 use case 보시고 한번 해보시구요 아 맞다 이거는'},\n",
       " {'start': 1699.2,\n",
       "  'end': 1716.048,\n",
       "  'text': ' 테디노트 팀의 한택님께서 이렇게 잘 만들어서 공유를 해주셨어요. 영상에 먼저 말씀드린다고 하는 거를 깜빡했네요. 한탱님이 잉글리시랑 한국어 버전도 따로 만들어 줬으니까 굉장히 친절하게 이 문서들을 작성을 해줬거든요 그러니까 한번 보시면서 따라해 보시다가 궁금하신 점 있으시면 또 댓글 남겨주시면 좋을 것 같아요'},\n",
       " {'start': 1716.048,\n",
       "  'end': 1745.488,\n",
       "  'text': ' 5코더를 위한 MCP 랭체인닥스 이 프로젝트는 돌아오는 토요일날 패스트캠퍼스에서 세미나를 합니다. 그래서 그 세미나 안에서 먼저 만드는 과정들 소개해드릴 거고요 강의를 수강하신 분들은 별도 신청하실 필요 없구요, 강의수강하지 않는 분들 중에서는 설문지에 작성을 해주시면은 추첨을 통해서 세미 나 참석할 수 있는 링크로 보내드리고 있어요 세민아만 참여 하실분들은 그렇게 따로 설명 제시해 주시면 되겠습니다 그때 는 이제 mcp 썸버 만든거와 어떤 식으로 진행했는지 좀 더 세세하게 살펴 볼 예정이 마지막으로 5 코더'},\n",
       " {'start': 1745.488,\n",
       "  'end': 1755.408,\n",
       "  'text': ' 드리면서 mcp에 대해서 전반적인 내용들을 한번 살펴봤는데요. 제공해드리는 튜토리얼 예제 코드를 활용해서 꼭 한 번 해보시기 바랍니다 저는 다음 영상으로 또 뵙겠습니다 감사합니다'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import math\n",
    "import requests\n",
    "import tempfile\n",
    "import os\n",
    "import concurrent.futures\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import threading\n",
    "import ffmpeg\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, total_size, desc=\"Downloading\"):\n",
    "        pbar = tqdm(total=total_size, unit=\"iB\", unit_scale=True, desc=desc)\n",
    "        lock = threading.Lock()\n",
    "\n",
    "    def update(self, size):\n",
    "        with lock:\n",
    "            pbar.update(size)\n",
    "\n",
    "    def close(self):\n",
    "        pbar.close()\n",
    "\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.1,\n",
    "        status_forcelist=[500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(\n",
    "        max_retries=retry,\n",
    "        pool_connections=100,\n",
    "        pool_maxsize=100\n",
    "    )\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def download_chunk(args):\n",
    "    url, start, end, chunk_number, temp_dir, progress_bar = args\n",
    "    \n",
    "    headers = {\"Range\": f\"bytes={start}-{end}\"}\n",
    "    session = create_session()\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, headers=headers, stream=True)\n",
    "        chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_number:04d}\")\n",
    "        \n",
    "        with open(chunk_path, \"wb\") as f:\n",
    "            for data in response.iter_content(chunk_size=8192):\n",
    "                size = f.write(data)\n",
    "                progress_bar.update(size)\n",
    "        \n",
    "        return chunk_path, chunk_number\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading chunk {chunk_number}: {str(e)}\")\n",
    "        return None, chunk_number\n",
    "\n",
    "def parallel_download(url, temp_dir, num_chunks=10):\n",
    "    session = create_session()\n",
    "    response = session.head(url)\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "    \n",
    "    if total_size == 0:\n",
    "        raise ValueError(\"Could not determine file size\")\n",
    "    \n",
    "    chunk_size = total_size // num_chunks\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size - 1 if i < num_chunks - 1 else total_size - 1\n",
    "        chunks.append((start, end))\n",
    "    \n",
    "    progress_bar = ProgressBar(total_size, \"Parallel downloading\")\n",
    "    \n",
    "    download_args = [\n",
    "        (url, start, end, i, temp_dir, progress_bar)\n",
    "        for i, (start, end) in enumerate(chunks)\n",
    "    ]\n",
    "    \n",
    "    chunk_paths = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_chunks) as executor:\n",
    "        futures = executor.map(download_chunk, download_args)\n",
    "        chunk_paths = [(path, num) for path, num in futures if path is not None]\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    chunk_paths.sort(key=lambda x: x[1])\n",
    "    output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "    \n",
    "    with open(output_path, \"wb\") as outfile:\n",
    "        for chunk_path, _ in chunk_paths:\n",
    "            with open(chunk_path, \"rb\") as infile:\n",
    "                outfile.write(infile.read())\n",
    "            os.remove(chunk_path)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def convert_to_wav(input_path, output_path):\n",
    "    \"\"\"MP4를 WAV로 변환\"\"\"\n",
    "    try:\n",
    "        stream = ffmpeg.input(input_path)\n",
    "        stream = ffmpeg.output(stream, output_path, \n",
    "                             acodec=\"pcm_s16le\", \n",
    "                             ar=\"16000\",\n",
    "                             ac=\"1\")\n",
    "        ffmpeg.run(stream, capture_stdout=True, capture_stderr=True)\n",
    "        return True\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"FFmpeg error:\", e.stderr.decode())\n",
    "        return False\n",
    "\n",
    "def process_audio_chunk(chunk_data):\n",
    "    \"\"\"개별 오디오 청크 처리\"\"\"\n",
    "    model, audio_path, start_time, duration = chunk_data\n",
    "    try:\n",
    "        segments, info = model.transcribe(\n",
    "            audio_path,\n",
    "            beam_size=5,\n",
    "            batch_size=32,\n",
    "            word_timestamps=True,\n",
    "            initial_prompt=None\n",
    "        )\n",
    "        \n",
    "        # segments를 리스트로 변환하고 시간 조정\n",
    "        chunk_segments = []\n",
    "        for segment in segments:\n",
    "            segment_dict = {\n",
    "                \"start\": segment.start + start_time,\n",
    "                \"end\": segment.end + start_time,\n",
    "                \"text\": segment.text,\n",
    "                \"words\": [\n",
    "                    {\n",
    "                        \"start\": word.start + start_time,\n",
    "                        \"end\": word.end + start_time,\n",
    "                        \"word\": word.word,\n",
    "                        \"probability\": word.probability\n",
    "                    }\n",
    "                    for word in segment.words\n",
    "                ]\n",
    "            }\n",
    "            chunk_segments.append(segment_dict)\n",
    "        \n",
    "        return chunk_segments\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk at {start_time}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def process_with_progress(url, model, chunk_duration=30, num_download_chunks=10):\n",
    "    \"\"\"\n",
    "    전체 처리 프로세스 관리\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        print(\"Starting parallel download...\")\n",
    "        mp4_path = parallel_download(url, temp_dir, num_download_chunks)\n",
    "        print(\"Download complete!\")\n",
    "        \n",
    "        # MP4를 WAV로 변환\n",
    "        wav_path = os.path.join(temp_dir, \"audio.wav\")\n",
    "        if not convert_to_wav(mp4_path, wav_path):\n",
    "            raise Exception(\"Failed to convert audio to WAV format\")\n",
    "        \n",
    "        # WAV 파일 정보 읽기\n",
    "        wav_info = sf.info(wav_path)\n",
    "        total_duration = wav_info.duration\n",
    "        \n",
    "        # 청크 계산\n",
    "        total_chunks = math.ceil(total_duration / chunk_duration)\n",
    "        \n",
    "        # 진행률 표시\n",
    "        pbar = tqdm(total=total_chunks, desc=\"Processing audio chunks\")\n",
    "        \n",
    "        # 청크 처리를 위한 데이터 준비\n",
    "        chunks_data = []\n",
    "        for i in range(total_chunks):\n",
    "            start_time = i * chunk_duration\n",
    "            chunk_wav_path = os.path.join(temp_dir, f\"chunk_{i}.wav\")\n",
    "            \n",
    "            # 청크 추출\n",
    "            duration = min(chunk_duration, total_duration - start_time)\n",
    "            stream = ffmpeg.input(wav_path, ss=start_time, t=duration)\n",
    "            stream = ffmpeg.output(stream, chunk_wav_path, \n",
    "                                 acodec=\"pcm_s16le\", \n",
    "                                 ar=\"16000\",\n",
    "                                 ac=\"1\")\n",
    "            ffmpeg.run(stream, quiet=True)\n",
    "            \n",
    "            chunks_data.append((model, chunk_wav_path, start_time, duration))\n",
    "        \n",
    "        # 청크 처리 및 결과 수집\n",
    "        all_segments = []\n",
    "        for chunk_data in chunks_data:\n",
    "            segments = process_audio_chunk(chunk_data)\n",
    "            all_segments.extend(segments)\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # 사용한 청크 파일 삭제\n",
    "            if os.path.exists(chunk_data[1]):\n",
    "                os.remove(chunk_data[1])\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "    return all_segments\n",
    "\n",
    "# 모델 초기화\n",
    "model = WhisperModel(\n",
    "    \"large-v3\", \n",
    "    device=\"cuda\", \n",
    "    compute_type=\"float16\"  # bfloat16 대신 float16 사용\n",
    ")\n",
    "print(\"Whisper 모델 초기화 완료\")\n",
    "model = BatchedInferencePipeline(model=model)\n",
    "\n",
    "# 트랜스크립션 실행\n",
    "segments = process_with_progress(\n",
    "    audio_stream.url,\n",
    "    model,\n",
    "    chunk_duration=30,\n",
    "    num_download_chunks=10\n",
    ")\n",
    "\n",
    "# 결과 저장\n",
    "for i, segment in enumerate(segments):\n",
    "    print(f\"{segment[\"start\"]:.2f} -> {segment[\"end\"]:.2f}: {segment[\"text\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://youtu.be/AA621UofTUA?si=gn4XutRMWUDSYLFL\"\n",
    "\n",
    "# from faster_whisper import WhisperModel\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# import tempfile\n",
    "# import os\n",
    "# import ffmpeg\n",
    "# import subprocess\n",
    "# from yt_dlp import YoutubeDL\n",
    "# import io\n",
    "\n",
    "# def get_audio_stream(url):\n",
    "#     \"\"\"URL에서 오디오 스트림 정보를 가져옵니다.\"\"\"\n",
    "#     ydl_opts = {\n",
    "#         \"format\": \"bestaudio/best\",\n",
    "#         \"quiet\": True,\n",
    "#         \"no_warnings\": True,\n",
    "#         \"extract_audio\": True\n",
    "#     }\n",
    "    \n",
    "#     with YoutubeDL(ydl_opts) as ydl:\n",
    "#         info = ydl.extract_info(url, download=False)\n",
    "#         audio_url = info[\"url\"]\n",
    "#         duration = info.get(\"duration\", 0)\n",
    "        \n",
    "#         return audio_url, duration\n",
    "\n",
    "# def process_stream_with_progress(url, model, chunk_duration=30):\n",
    "#     \"\"\"\n",
    "#     스트리밍 방식으로 오디오를 처리합니다.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - url: 오디오 URL\n",
    "#     - model: WhisperModel 인스턴스\n",
    "#     - chunk_duration: 각 청크의 길이(초)\n",
    "#     \"\"\"\n",
    "#     # 스트림 URL 가져오기\n",
    "#     audio_url, total_duration = get_audio_stream(url)\n",
    "    \n",
    "#     # ffmpeg 명령어 설정\n",
    "#     ffmpeg_cmd = [\n",
    "#         \"ffmpeg\",\n",
    "#         \"-i\", audio_url,\n",
    "#         \"-f\", \"wav\",\n",
    "#         \"-ar\", \"16000\",\n",
    "#         \"-ac\", \"1\",\n",
    "#         \"-hide_banner\",\n",
    "#         \"-loglevel\", \"error\",\n",
    "#         \"pipe:1\"\n",
    "#     ]\n",
    "    \n",
    "#     # 진행률 표시 설정\n",
    "#     total_chunks = int(np.ceil(total_duration / chunk_duration))\n",
    "#     pbar = tqdm(total=total_chunks, desc=\"Processing audio chunks\")\n",
    "    \n",
    "#     # 결과 저장용 리스트\n",
    "#     all_segments = []\n",
    "    \n",
    "#     try:\n",
    "#         # ffmpeg 프로세스 시작\n",
    "#         process = subprocess.Popen(\n",
    "#             ffmpeg_cmd,\n",
    "#             stdout=subprocess.PIPE,\n",
    "#             bufsize=10**8  # 버퍼 크기 설정\n",
    "#         )\n",
    "        \n",
    "#         # 임시 디렉토리 생성\n",
    "#         with tempfile.TemporaryDirectory() as temp_dir:\n",
    "#             chunk_size = int(16000 * chunk_duration * 2)  # 16000Hz * seconds * 2 bytes per sample\n",
    "#             chunk_number = 0\n",
    "            \n",
    "#             while True:\n",
    "#                 # 청크 읽기\n",
    "#                 audio_chunk = process.stdout.read(chunk_size)\n",
    "#                 if not audio_chunk:\n",
    "#                     break\n",
    "                \n",
    "#                 # 청크를 임시 파일로 저장\n",
    "#                 chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_number}.wav\")\n",
    "#                 with open(chunk_path, \"wb\") as f:\n",
    "#                     # WAV 헤더 작성\n",
    "#                     f.write(b\"RIFF\")\n",
    "#                     f.write((chunk_size + 36).to_bytes(4, \"little\"))\n",
    "#                     f.write(b\"WAVE\")\n",
    "#                     f.write(b\"fmt \")\n",
    "#                     f.write((16).to_bytes(4, \"little\"))\n",
    "#                     f.write((1).to_bytes(2, \"little\"))  # PCM\n",
    "#                     f.write((1).to_bytes(2, \"little\"))  # Mono\n",
    "#                     f.write((16000).to_bytes(4, \"little\"))  # Sample rate\n",
    "#                     f.write((32000).to_bytes(4, \"little\"))  # Byte rate\n",
    "#                     f.write((2).to_bytes(2, \"little\"))  # Block align\n",
    "#                     f.write((16).to_bytes(2, \"little\"))  # Bits per sample\n",
    "#                     f.write(b\"data\")\n",
    "#                     f.write(len(audio_chunk).to_bytes(4, \"little\"))\n",
    "#                     f.write(audio_chunk)\n",
    "                \n",
    "#                 try:\n",
    "#                     # 청크 처리\n",
    "#                     segments, _ = model.transcribe(\n",
    "#                         chunk_path,\n",
    "#                         beam_size=5,\n",
    "#                         batch_size=32,\n",
    "#                         word_timestamps=True,\n",
    "#                         condition_on_previous_text=True\n",
    "#                     )\n",
    "                    \n",
    "#                     # 시간 오프셋 조정 및 세그먼트 저장\n",
    "#                     time_offset = chunk_number * chunk_duration\n",
    "#                     for segment in segments:\n",
    "#                         segment_dict = {\n",
    "#                             \"start\": segment.start + time_offset,\n",
    "#                             \"end\": segment.end + time_offset,\n",
    "#                             \"text\": segment.text,\n",
    "#                             \"words\": [\n",
    "#                                 {\n",
    "#                                     \"start\": word.start + time_offset,\n",
    "#                                     \"end\": word.end + time_offset,\n",
    "#                                     \"word\": word.word,\n",
    "#                                     \"probability\": word.probability\n",
    "#                                 }\n",
    "#                                 for word in segment.words\n",
    "#                             ]\n",
    "#                         }\n",
    "#                         all_segments.append(segment_dict)\n",
    "                \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing chunk {chunk_number}: {str(e)}\")\n",
    "                \n",
    "#                 finally:\n",
    "#                     # 임시 파일 삭제\n",
    "#                     if os.path.exists(chunk_path):\n",
    "#                         os.remove(chunk_path)\n",
    "                \n",
    "#                 # 진행률 업데이트\n",
    "#                 pbar.update(1)\n",
    "#                 chunk_number += 1\n",
    "    \n",
    "#     finally:\n",
    "#         pbar.close()\n",
    "#         if process.poll() is None:\n",
    "#             process.terminate()\n",
    "#             process.wait()\n",
    "    \n",
    "#     return all_segments\n",
    "\n",
    "# # 모델 초기화\n",
    "# model = WhisperModel(\n",
    "#     \"large-v3\", \n",
    "#     device=\"cuda\", \n",
    "#     compute_type=\"float16\"\n",
    "# )\n",
    "# print(\"Whisper 모델 초기화 완료\")\n",
    "\n",
    "# # 트랜스크립션 실행\n",
    "# segments = process_stream_with_progress(\n",
    "#     url,  # 유튜브 URL\n",
    "#     model,\n",
    "#     chunk_duration=30\n",
    "# )\n",
    "\n",
    "# # 결과 출력\n",
    "# for segment in segments:\n",
    "#     print(f\"{segment[\"start\"]:.2f} -> {segment[\"end\"]:.2f}: {segment[\"text\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"script.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "            Document(page_content=\"\\n\".join([t[\"text\"] for t in data]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def calculate_tokens(text, model=\"gpt-4o-mini\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tokens(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, chunk_overlap=500\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(split_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import hub\n",
    "summary_prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7, streaming=True)\n",
    "summary_chain = create_stuff_documents_chain(llm, summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumaries = []\n",
    "for split_doc in split_docs:\n",
    "    print(type(split_doc.page_content))\n",
    "    partial_summary = summary_chain.invoke({\"context\": [split_doc]})\n",
    "    sumaries.append(partial_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_summary = Document(page_content= \"\\n\".join(sumaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_RESULT = summary_chain.invoke(\n",
    "                {\"context\": partial_summaries_doc}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(SUMMARY_RESULT.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_qa_with_sources_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pytubefix import YouTube\n",
    "import asyncio\n",
    "import torch\n",
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # .env 파일에서 환경 변수 로드\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(url):\n",
    "    yt = YouTube(url)\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    return {\n",
    "        \"title\": yt.title,\n",
    "        \"audio_url\": audio_stream.url if audio_stream else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://www.youtube.com/shorts/a--NSC19MXM\"\n",
    "video_info = get_video_info(video_url)\n",
    "print(f\"Video Title: {video_info[\"title\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = WhisperModel(\"large-v3\", device=device, compute_type=compute_type)\n",
    "\n",
    "def transcribe_audio(audio_url):\n",
    "    segments, info = whisper_model.transcribe(audio_url)\n",
    "    transcript = [{\"text\": segment.text, \"start\": segment.start, \"end\": segment.end} for segment in segments]\n",
    "    return {\"script\": transcript, \"language\": info.language}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = transcribe_audio(video_info[\"audio_url\"])\n",
    "print(f\"Transcript Language: {transcript[\"language\"]}\")\n",
    "print(f\"First few lines of transcript: {transcript[\"script\"][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "summary_prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            streaming=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader, TextLoader\n",
    "docs = TextLoader(\"script.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "document = [Document(page_content=\"\\n\".join([t[\"text\"] for t in transcript[\"script\"]]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = create_stuff_documents_chain(llm,summary_prompt)\n",
    "result = await summary_chain.ainvoke({\"context\": document})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.create_documents([t[\"text\"] for t in transcript[\"script\"]])\n",
    "for doc in documents:\n",
    "    doc.page_content += \"\\n\" + summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "you_url = \"https://youtube.com/shorts/a--NSC19MXM?si=yiun-HK_7wX1sNvL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = \"https://api.runpod.ai/v2/uq96boxkzy99ev/runsync\"\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터 (내부적으로 사용할 파라미터 설정)\n",
    "body = {\"input\":{\n",
    "    \"api\":{\n",
    "        \"method\":\"POST\",\n",
    "        \"endpoint\":\"/ping\",\n",
    "    },\n",
    "    \"payload\":{},\n",
    "}}\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=body, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    print(\"Response:\", response.json())\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 작업 ID (작업 완료된 job ID)\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "# RunPod API STATUS 엔드포인트 URL\n",
    "status_url = f\"https://api.runpod.ai/v2/wm1xrz07all039/status/{job_id}\"\n",
    "\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# 작업 상태 및 결과 확인 요청 보내기\n",
    "response = requests.get(status_url, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    job_result = response.json()\n",
    "    if job_result.get(\"status\") == \"COMPLETED\":\n",
    "        print(\"Job Completed! Result:\", job_result.get(\"output\"))\n",
    "    else:\n",
    "        print(f\"Job Status: {job_result.get(\"status\")}\")\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "you_url = \"https://youtu.be/omEk2BNDt1I?si=xjtbYANtlux5CTfB\"\n",
    "\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/get_title_hash\",\n",
    "        \"method\": \"GET\",\n",
    "        # \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"url\": you_url},\n",
    "    }\n",
    "}\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    print(\"Response:\", response.json())\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "# endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# # RunPod RUNSYNC 엔드포인트 URL\n",
    "# url = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "\n",
    "# # FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "# payload = {\n",
    "#     \"input\": {\n",
    "#         \"endpoint\": \"/get_script_summary\",\n",
    "#         \"method\": \"GET\",\n",
    "#         \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "#         \"params\": {\"url\": you_url},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # 요청 헤더에 API 키 추가\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {api_key}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # RUNSYNC 요청 보내기\n",
    "# response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# # 응답 확인\n",
    "# if response.status_code == 200:\n",
    "#     print(\"Response:\", response.json())\n",
    "# else:\n",
    "#     print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/run\"\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/get_script_summary\",\n",
    "        \"method\": \"GET\",\n",
    "        \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"url\": you_url},\n",
    "    }\n",
    "}\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Initial Response:\", result)\n",
    "    \n",
    "    if result.get(\"status\") in [\"IN_PROGRESS\",\"IN_QUEUE\"]:\n",
    "        job_id = result.get(\"id\")\n",
    "        status_url = f\"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}\"\n",
    "        \n",
    "        while True:\n",
    "            status_response = requests.get(status_url, headers=headers)\n",
    "            if status_response.status_code == 200:\n",
    "                status_data = status_response.json()\n",
    "                print(f\"Current status: {status_data.get(\"status\")}\")\n",
    "                \n",
    "                if status_data.get(\"status\") == \"COMPLETED\":\n",
    "                    print(f\"결과값:{status_data}\")\n",
    "                    result_url = f\"https://api.runpod.ai/v2/{endpoint_id}/result/{job_id}\"\n",
    "                    result_response = requests.get(result_url, headers=headers)\n",
    "                    \n",
    "                    if result_response.status_code == 200:\n",
    "                        final_result = result_response.json()\n",
    "                        print(\"Final Result:\", final_result)\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Error fetching results: {result_response.status_code}\")\n",
    "                        print(f\"Error message: {result_response.text}\")\n",
    "                        break\n",
    "                elif status_data.get(\"status\") == \"FAILED\":\n",
    "                    print(\"Job failed\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Error checking status: {status_response.status_code}\")\n",
    "                print(f\"Error message: {status_response.text}\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)  # 5초 대기 후 다시 상태 확인\n",
    "    else:\n",
    "        print(\"Job completed immediately\")\n",
    "        print(\"Final Result:\", result)\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}\"\n",
    "url2 = f\"https://api.runpod.ai/v2/{endpoint_id}/result/{job_id}\"\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "while True:\n",
    "    # RUNSYNC 요청 보내기\n",
    "    response = requests.get(url,headers=headers)\n",
    "\n",
    "    # 응답 확인\n",
    "    if response.status_code == 200:\n",
    "        if response.json().get(\"status\") == \"COMPLETED\":\n",
    "            response = requests.get(url2,headers=headers)\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Job status: {response.json().get(\"status\")}\")\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "RUNPOD_API_URL = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/rag_stream_chat\",\n",
    "        \"method\": \"POST\",\n",
    "        \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"prompt\": \"영상의 주제가 뭔가요?\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    RUNPOD_API_URL, headers=headers, json=payload, stream=True\n",
    ")\n",
    "\n",
    "# for chunk in response.iter_content(chunk_size=None):\n",
    "#     if chunk:\n",
    "#         chunk_data = chunk.decode(\"utf-8\").strip()\n",
    "#         if chunk_data.startswith(\"data: \"):\n",
    "#             chunk_content = chunk_data[6:]\n",
    "#             if chunk_content == \"[DONE]\":\n",
    "#                 break\n",
    "#             try:\n",
    "#                 content = json.loads(chunk_content)\n",
    "#                 print(\"Stream content:\", content)\n",
    "#             except json.JSONDecodeError:\n",
    "#                 print(\"Invalid JSON:\", chunk_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"\"\n",
    "for chunk in response.json().get(\"output\"):\n",
    "    answer += chunk.get(\"content\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"script.txt\",\"r\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "url = \"https://www.youtube.com/watch?v=yF_YIxxjWU4\"\n",
    "yt = YouTube(url, use_po_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1RPJ223J7MY4YHK5UDRZ5F05GBFX7H9C4R8R5RW\n",
    "S1RPJ223J7MY4YHK5UDRZ5FO5GBFX7H9C4R8R5RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "cookie_data = os.getenv(\"YOUTUBE_COOKIES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "cookie_data = base64.b64decode(cookie_data).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'domain': '.youtube.com',\n",
       "  'expirationDate': 1761776434.318973,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': True,\n",
       "  'name': 'LOGIN_INFO',\n",
       "  'path': '/',\n",
       "  'sameSite': 'no_restriction',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'AFmmF2swRQIgTeEWqOY9Z_6OixLw2Vtg4geLBxPWddIDQpA4COLU_w0CIQCqFau5rdTu8mpMmGvpI8uZqqZk8EHqKy8f3wUc9MJP3A:QUQ3MjNmemVwa3pObTg4QmlDa3FMaUdOTEEtZ1dPdWNfUjdYUFI5V3I1OUZ0VjdqRDJWSUM0Q3UwU0NqN3JwZVh1TVctYUpfRWJkRnUyVE1YTHNaT0tmT3dvLTNNbUw1WlFkZ2JRdFZtdndwME55YjQ4elpUdXRHdXZZNENkdjVrQkt5VFBjOVE4YmhjaFBmbmZpNXk4bWk5WXl0TkFzajF3'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1774327408.70996,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': True,\n",
       "  'name': 'HSID',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': False,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'AWsS9hESIEnYMsU-A'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1774327408.710192,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': True,\n",
       "  'name': 'SSID',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'AJi2nJkY2L-Yggyor'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1774327408.710439,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': False,\n",
       "  'name': 'APISID',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': False,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'Ka3V_eFb1cAj_a9a/ApAugbpab_Nmh9joG'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1774327408.710549,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': False,\n",
       "  'name': 'SAPISID',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'MLUWJ6icCQqiORUf/AZpqx2XzVfuKRHFXd'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1774327408.710644,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': False,\n",
       "  'name': '__Secure-1PAPISID',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'MLUWJ6icCQqiORUf/AZpqx2XzVfuKRHFXd'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1774327408.710727,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': False,\n",
       "  'name': '__Secure-3PAPISID',\n",
       "  'path': '/',\n",
       "  'sameSite': 'no_restriction',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'MLUWJ6icCQqiORUf/AZpqx2XzVfuKRHFXd'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1775102280.64494,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': False,\n",
       "  'name': 'PREF',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'f6=40000080&repeat=NONE&tz=Asia.Seoul&f7=100'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1774327408.711261,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': False,\n",
       "  'name': 'SID',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': False,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'g.a000twixe91NaNcQDevwyL_O7gusj1AiqlcIKaejh21ozIM5e1aTtsfFd6-A7F73n1nwJ1qdqgACgYKAbQSARISFQHGX2MiX7fLwCpHI0vqFnWIzuV0nxoVAUF8yKrr5suNTfPF3rBqmbHottKD0076'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1774327408.711331,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': True,\n",
       "  'name': '__Secure-1PSID',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'g.a000twixe91NaNcQDevwyL_O7gusj1AiqlcIKaejh21ozIM5e1aTqpB0K9yBIBaU3ScWiXuoAwACgYKAQISARISFQHGX2MivMNK5wIRTNpnwlYZM3xFNxoVAUF8yKq3LakDSayzg2pCPVgH61lg0076'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1774327408.711403,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': True,\n",
       "  'name': '__Secure-3PSID',\n",
       "  'path': '/',\n",
       "  'sameSite': 'no_restriction',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'g.a000twixe91NaNcQDevwyL_O7gusj1AiqlcIKaejh21ozIM5e1aTMjor4Q-eAMtm9lYcdENxZAACgYKAbgSARISFQHGX2MitDsvozbD751SpkVcz8QKVBoVAUF8yKrwe1k_GE9DrEDX_Tivx-Pm0076'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1740542286,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': False,\n",
       "  'name': 'ST-1b',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': False,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'disableCache=true&itct=CBYQsV4iEwiQ4qSiueCLAxXE7UwCHS0bDKc%3D&csn=e5uzKG5E76jxWGyE&session_logininfo=AFmmF2swRQIgTeEWqOY9Z_6OixLw2Vtg4geLBxPWddIDQpA4COLU_w0CIQCqFau5rdTu8mpMmGvpI8uZqqZk8EHqKy8f3wUc9MJP3A%3AQUQ3MjNmemVwa3pObTg4QmlDa3FMaUdOTEEtZ1dPdWNfUjdYUFI5V3I1OUZ0VjdqRDJWSUM0Q3UwU0NqN3JwZVh1TVctYUpfRWJkRnUyVE1YTHNaT0tmT3dvLTNNbUw1WlFkZ2JRdFZtdndwME55YjQ4elpUdXRHdXZZNENkdjVrQkt5VFBjOVE4YmhjaFBmbmZpNXk4bWk5WXl0TkFzajF3&endpoint=%7B%22clickTrackingParams%22%3A%22CBYQsV4iEwiQ4qSiueCLAxXE7UwCHS0bDKc%3D%22%2C%22commandMetadata%22%3A%7B%22webCommandMetadata%22%3A%7B%22url%22%3A%22%2F%22%2C%22webPageType%22%3A%22WEB_PAGE_TYPE_BROWSE%22%2C%22rootVe%22%3A3854%2C%22apiUrl%22%3A%22%2Fyoutubei%2Fv1%2Fbrowse%22%7D%7D%2C%22browseEndpoint%22%3A%7B%22browseId%22%3A%22FEwhat_to_watch%22%7D%7D'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1740542286,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': False,\n",
       "  'name': 'ST-yve142',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': False,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'session_logininfo=AFmmF2swRQIgTeEWqOY9Z_6OixLw2Vtg4geLBxPWddIDQpA4COLU_w0CIQCqFau5rdTu8mpMmGvpI8uZqqZk8EHqKy8f3wUc9MJP3A%3AQUQ3MjNmemVwa3pObTg4QmlDa3FMaUdOTEEtZ1dPdWNfUjdYUFI5V3I1OUZ0VjdqRDJWSUM0Q3UwU0NqN3JwZVh1TVctYUpfRWJkRnUyVE1YTHNaT0tmT3dvLTNNbUw1WlFkZ2JRdFZtdndwME55YjQ4elpUdXRHdXZZNENkdjVrQkt5VFBjOVE4YmhjaFBmbmZpNXk4bWk5WXl0TkFzajF3'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1740542287,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': False,\n",
       "  'name': 'ST-tladcw',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': False,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'session_logininfo=AFmmF2swRQIgTeEWqOY9Z_6OixLw2Vtg4geLBxPWddIDQpA4COLU_w0CIQCqFau5rdTu8mpMmGvpI8uZqqZk8EHqKy8f3wUc9MJP3A%3AQUQ3MjNmemVwa3pObTg4QmlDa3FMaUdOTEEtZ1dPdWNfUjdYUFI5V3I1OUZ0VjdqRDJWSUM0Q3UwU0NqN3JwZVh1TVctYUpfRWJkRnUyVE1YTHNaT0tmT3dvLTNNbUw1WlFkZ2JRdFZtdndwME55YjQ4elpUdXRHdXZZNENkdjVrQkt5VFBjOVE4YmhjaFBmbmZpNXk4bWk5WXl0TkFzajF3'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1772078282.5309,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': True,\n",
       "  'name': '__Secure-1PSIDTS',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'sidts-CjIBEJ3XVzDf85tuOkEK3FzAPGwKPf1HW4HztizgaYZdgo_IZJWehWhNpX6x8IhFtVd8oBAA'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1772078282.530963,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': True,\n",
       "  'name': '__Secure-3PSIDTS',\n",
       "  'path': '/',\n",
       "  'sameSite': 'no_restriction',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'sidts-CjIBEJ3XVzDf85tuOkEK3FzAPGwKPf1HW4HztizgaYZdgo_IZJWehWhNpX6x8IhFtVd8oBAA'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1740542287,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': False,\n",
       "  'name': 'ST-3opvp5',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': False,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'session_logininfo=AFmmF2swRQIgTeEWqOY9Z_6OixLw2Vtg4geLBxPWddIDQpA4COLU_w0CIQCqFau5rdTu8mpMmGvpI8uZqqZk8EHqKy8f3wUc9MJP3A%3AQUQ3MjNmemVwa3pObTg4QmlDa3FMaUdOTEEtZ1dPdWNfUjdYUFI5V3I1OUZ0VjdqRDJWSUM0Q3UwU0NqN3JwZVh1TVctYUpfRWJkRnUyVE1YTHNaT0tmT3dvLTNNbUw1WlFkZ2JRdFZtdndwME55YjQ4elpUdXRHdXZZNENkdjVrQkt5VFBjOVE4YmhjaFBmbmZpNXk4bWk5WXl0TkFzajF3'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1772078285.12273,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': False,\n",
       "  'name': 'SIDCC',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': False,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'AKEyXzWCPeicpoMl_KFGpPCoMrnr2G2DsgQyXB5m5_UPGDRLkG_drtSc1glzvtGkXqDlF0Abyg'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1772078285.122929,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': True,\n",
       "  'name': '__Secure-1PSIDCC',\n",
       "  'path': '/',\n",
       "  'sameSite': 'unspecified',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'AKEyXzV7S4IegmqpS6ZT9DRWMNvd2EH5uK8QSvCbtl4Zl_oz5yEBL1xAXw8YDMmmRAPmrgbV1e8'},\n",
       " {'domain': '.youtube.com',\n",
       "  'expirationDate': 1772078285.123041,\n",
       "  'hostOnly': False,\n",
       "  'httpOnly': True,\n",
       "  'name': '__Secure-3PSIDCC',\n",
       "  'path': '/',\n",
       "  'sameSite': 'no_restriction',\n",
       "  'secure': True,\n",
       "  'session': False,\n",
       "  'storeId': '0',\n",
       "  'value': 'AKEyXzWksndwB5bSAix4qM7a-MmqBYZeRYBzWKnrVK_JSRv7ketFJ_pohhSQ-jiF8YEPDsR5e14'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cookie_data\n",
    "import json\n",
    "\n",
    "cookie_data = json.loads(cookie_data)\n",
    "cookie_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
