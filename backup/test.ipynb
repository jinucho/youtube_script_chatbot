{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "\n",
    "import tiktoken\n",
    "from langchain_teddynote.callbacks import StreamingCallback\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import VLLM\n",
    "import json\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"beomi/Qwen2.5-7B-Instruct-kowiki-qa-context\", temperature=0.7, streaming=True, base_url=\"http://localhost:8080/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 9, 'total_tokens': 20, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-729a0205-fd3d-44f6-b7e4-3317370ab033-0', usage_metadata={'input_tokens': 9, 'output_tokens': 11, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"í•˜ì´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'beomi/Qwen2.5-7B-Instruct-kowiki-qa-context'}, id='run-51a856e8-4899-400d-841c-f2f2914582ee-0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"í•˜ì´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class CustomStreamingCallbackHandler(BaseCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.partial_result = \"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.partial_result += token\n",
    "        print(token, end=\"\", flush=True)  # ì‹¤ì‹œê°„ ì¶œë ¥\n",
    "        # '}'ê°€ ìƒì„±ë˜ë©´ ì¤‘ë‹¨\n",
    "        if \"}\" in self.partial_result:\n",
    "            raise StopIteration(\"ì¤‘ë‹¨ ì¡°ê±´ '}'ì´ ìƒì„±ë˜ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTIAL_SUMMARY_PROMPT_TEMPLATE = \"\"\"Please summarize the sentence according to the following REQUEST.\n",
    "                                            This task is partial summay, Please Do not summarize too much.\n",
    "    \n",
    "                                            REQUEST:\n",
    "                                            1. Summarize the main points in KOREAN.\n",
    "                                            2. Translate the summary into KOREAN if it is written in ENGLISH.\n",
    "                                            3. DO NOT translate any technical terms.\n",
    "                                            4. DO NOT include any unnecessary information.\n",
    "                                            \n",
    "                                            CONTEXT:\n",
    "                                            {context}\n",
    "                                            \n",
    "                                            SUMMARY:\n",
    "                                            \"\"\"\n",
    "                                            \n",
    "FINAL_SUMMARY_PROMPT_TEMPLATE = \"\"\"\n",
    "ë‹¤ìŒ REQUESTì— ë”°ë¼ CONTEXTë¥¼ ìš”ì•½í•˜ê³ , ì¶œë ¥ì€ ì•„ë˜ì— ì œê³µëœ ì¶œë ¥ í˜•ì‹(OUTPUT_FORMAT)ê³¼ ì •í™•íˆ ë™ì¼í•˜ê²Œ í•œ ë²ˆë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "REQUEST:\n",
    "1. ì£¼ì–´ì§„ OUTPUT(JSON í˜•ì‹) ì™¸ì˜ í…ìŠ¤íŠ¸ë‚˜ ì„¤ëª…ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "2. CONTEXTì™€ HUMAN MESSAGEëŠ” ì¶œë ¥í•˜ì§€ë§ˆì„¸ìš”.\n",
    "3. ë‹¨ í•˜ë‚˜ì˜ OUTPUTë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "4. ì£¼ìš” ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ë˜, ì „ë¬¸, ê¸°ìˆ  ìš©ì–´ëŠ” ì›ë³¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "5. ìš”ì•½ëœ ê° ë¬¸ì¥ì€ í•´ë‹¹ ì˜ë¯¸ì™€ ì˜ ì–´ìš¸ë¦¬ëŠ” ì´ëª¨ì§€ í•˜ë‚˜ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "6. ë‹¤ì–‘í•œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ì„ í¥ë¯¸ë¡­ê²Œ ì‘ì„±í•˜ë˜, ê°„ê²°í•˜ê³  ê´€ë ¨ì„± ìˆê²Œ ìœ ì§€í•˜ì„¸ìš”.\n",
    "7. ë¬¸ì„œì˜ ë‹¨ì¼ ì£¼ìš” ì£¼ì œì™€ ì „ë°˜ì ì¸ ìš”ì•½ì—ë§Œ ì§‘ì¤‘í•˜ì„¸ìš”.\n",
    "8. ê° ìš”ì•½ì—ì„œ ì£¼ìš” ì£¼ì œë¥¼ ëª…í™•íˆ ë‚˜íƒ€ë‚´ì„¸ìš”.\n",
    "9. CONTEXTì˜ ë‚´ìš©ì´ ì¶©ë¶„íˆ ë§ë‹¤ë©´, ìš”ì•½ ë¬¸ì¥ì„ ì¶©ë¶„íˆ ìƒì„±í•˜ì„¸ìš”.\n",
    "10. ìš”ì•½ëœ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì„¸ ê°€ì§€ ì§ˆë¬¸ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "OUTPUT_FORMAT(JSON í˜•ì‹):\n",
    "{{\n",
    "    \"FINAL_SUMMARY\": {{\n",
    "        \"Key_topic\": ì£¼ìš” ì£¼ì œ ë‚´ìš©,\n",
    "        \"Summaries\": [\n",
    "            \"â€¢ Emoji ìš”ì•½ëœ ë‚´ìš©1\",\n",
    "            \"â€¢ Emoji ìš”ì•½ëœ ë‚´ìš©2\",\n",
    "            ...ì¶”ê°€ ìš”ì•½ ë‚´ìš© ë‚˜ì—´\n",
    "        ]\n",
    "    }},\n",
    "    \"RECOMMEND_QUESTIONS\": [\n",
    "        \"ì²« ë²ˆì§¸ ì§ˆë¬¸ (í•œêµ­ì–´)\",\n",
    "        \"ë‘ ë²ˆì§¸ ì§ˆë¬¸ (í•œêµ­ì–´)\",\n",
    "        \"ì„¸ ë²ˆì§¸ ì§ˆë¬¸ (í•œêµ­ì–´)\"\n",
    "    ]\n",
    "}}\n",
    "OUTPUT:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=500\n",
    ")\n",
    "partial_summary_prompt = PromptTemplate.from_template(\n",
    "    PARTIAL_SUMMARY_PROMPT_TEMPLATE\n",
    ")\n",
    "final_summary_prompt = PromptTemplate.from_template(\n",
    "    FINAL_SUMMARY_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../runpod_backend/data/backup.json\",\"r\") as f:\n",
    "#     data = json.load(f)\n",
    "# script = data[\"m25Lz9KWyDkNx6Vv\"][\"script_info\"][\"script\"]\n",
    "with open(\"./test1234/transcript.json\",\"r\") as f:\n",
    "    script = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "            Document(page_content=\"\\n\".join([t[\"text\"] for t in script]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_summary_chain = create_stuff_documents_chain(\n",
    "            llm=llm, prompt=partial_summary_prompt\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, chunk_overlap=400\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tokens(text, model=\"gpt-4o-mini\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "calculate_tokens(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    emoji: str = Field(..., description=\"ìš”ì•½ì— ì‚¬ìš©í•˜ëŠ” ì´ëª¨ì§€\")\n",
    "    content: str = Field(..., description=\"ìš”ì•½ëœ ë‚´ìš©\")\n",
    "\n",
    "class FinalSummary(BaseModel):\n",
    "    key_topic: str = Field(..., description=\"ì£¼ìš” ì£¼ì œ ë‚´ìš©\")\n",
    "    summaries: List[Summary] = Field(..., description=\"ìš”ì•½ëœ ë‚´ìš© ë¦¬ìŠ¤íŠ¸\")\n",
    "\n",
    "class RecommendQuestions(BaseModel):\n",
    "    questions: List[str] = Field(..., description=\"ì¶”ì²œ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸\")\n",
    "\n",
    "class FullStructure(BaseModel):\n",
    "    FINAL_SUMMARY: FinalSummary = Field(..., description=\"ìµœì¢… ìš”ì•½ ì •ë³´\")\n",
    "    RECOMMEND_QUESTIONS: RecommendQuestions = Field(..., description=\"ì¶”ì²œ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser(pydantic_object=FullStructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary_chain = create_stuff_documents_chain(\n",
    "            llm=llm, prompt=final_summary_prompt, output_parser=parser\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = partial_summary_chain.invoke({\"context\":docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_summary = [Document(page_content=summary)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_handler = CustomStreamingCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = final_summary_chain.invoke({\"context\": docs})\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary[\"FINAL_SUMMARY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary[\"RECOMMEND_QUESTIONS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\n",
    "summary += f'{list(final_summary.get(\"FINAL_SUMMARY\").items())[0][0]}:{list(final_summary.get(\"FINAL_SUMMARY\").items())[0][1]}\\n'\n",
    "joined_summary = '\\n'.join(list(final_summary.get(\"FINAL_SUMMARY\").items())[1][1])\n",
    "summary += f'{list(final_summary.get(\"FINAL_SUMMARY\").items())[1][0]}:{joined_summary}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = final_summary.get(\"RECOMMEND_QUESTIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)\n",
    "print(\"------\")\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].page_content += f\"\\n{summary}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = FAISS.from_documents(split_docs, hf_embeddings)\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k = 10\n",
    "vec_retriever = vec_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "retriever = EnsembleRetriever(\n",
    "                retrievers=[bm25_retriever, vec_retriever],\n",
    "                weights=[0.7, 0.3],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€\n",
    "# response_schemas = [\n",
    "#     ResponseSchema(name=\"answer\", description=\"ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€\"),\n",
    "# ]\n",
    "# # ì‘ë‹µ ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™”\n",
    "# output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "# format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "            \"\"\"ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ ê¸°ë°˜ì˜ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "                ë‹¹ì‹ ì˜ ì£¼ìš” ì„ë¬´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "                1. ê¸°ë³¸ì ìœ¼ë¡œ ê²€ìƒ‰ëœ ë¬¸ë§¥(context)ê³¼ ì´ì „ ëŒ€í™” ë‚´ìš©(chat_history)ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ OUTPUT_FORMATì˜ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œì„œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "                2. ì£¼ì–´ì§„ OUTPUT(JSON í˜•ì‹) ì™¸ì˜ í…ìŠ¤íŠ¸ë‚˜ ì„¤ëª…ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "                3. ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì—ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ë‚´ë¶€ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:\n",
    "                    - ê²€ìƒ‰ëœ ë¬¸ë§¥ì´ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì™„ë²½í•˜ê²Œ ì¶©ì¡±í•˜ì§€ ëª»í•  ë•Œ\n",
    "                    - ì˜ìƒì˜ ì „ë°˜ì ì¸ ì£¼ì œì™€ ì—°ê´€ë˜ì§€ë§Œ êµ¬ì²´ì ì¸ ë‹µë³€ì´ ë¬¸ë§¥ì— ì—†ì„ ë•Œ\n",
    "                    - ë¬¸ë§¥ì—ì„œ ë¶€ë¶„ì ì¸ ì •ë³´ë§Œ ì°¾ì„ ìˆ˜ ìˆì„ ë•ŒëŠ” ë¬¸ë§¥ì˜ ì •ë³´ì™€ ë‚´ë¶€ ì§€ì‹ì„ ì¡°í•©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”\n",
    "\n",
    "                4. ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:\n",
    "                - í•­ìƒ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ë¡œ ë‹µë³€í•˜ì„¸ìš”\n",
    "                - ë¬¸ë§¥ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë”ë¼ë„, ê·¸ ì‚¬ì‹¤ì„ ì–¸ê¸‰í•˜ì§€ ë§ê³  ë°”ë¡œ ë‹µë³€í•˜ì„¸ìš”\n",
    "                - ê¸°ìˆ  ìš©ì–´ë‚˜ ê³ ìœ ëª…ì‚¬ëŠ” ì›ì–´ë¥¼ ìœ ì§€í•˜ì„¸ìš”\n",
    "                - ì „ë¬¸ì ì¸ ë‚´ìš©ë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”\n",
    "\n",
    "                5. ë§Œì•½ ì§ˆë¬¸ì´ ì˜ìƒì˜ ì£¼ì œë‚˜ ë‚´ìš©ê³¼ ì „í˜€ ê´€ë ¨ì´ ì—†ë‹¤ë©´ \"ì˜ìƒê³¼ ê´€ê³„ ì—†ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.\"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n",
    "\n",
    "                ì´ì „ ëŒ€í™” ë‚´ìš©:\n",
    "                {chat_history}\n",
    "\n",
    "                ì§ˆë¬¸:\n",
    "                {question}\n",
    "\n",
    "                ë¬¸ë§¥:\n",
    "                {context}\n",
    "\n",
    "                OUTPUT_FORMAT:\n",
    "                {{\"answer\": \"ë‹µë³€\"}}\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chat_parser(BaseModel):\n",
    "    answer: dict = Field(..., description=\"ì±„íŒ… ì‘ë‹µ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "                {\n",
    "                    \"context\": itemgetter(\"question\") | retriever,\n",
    "                    \"question\": itemgetter(\"question\"),\n",
    "                    \"chat_history\": itemgetter(\"chat_history\"),\n",
    "                }\n",
    "                | prompt\n",
    "                # | chat_llm\n",
    "                | llm\n",
    "                | JsonOutputParser(pydantic_object=chat_parser)\n",
    "                # | StrOutputParser()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_message_store = {}\n",
    "def _get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        \"\"\"ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\"\"\"\n",
    "        if session_id not in _message_store:\n",
    "            _message_store[session_id] = ChatMessageHistory()\n",
    "        return _message_store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history = RunnableWithMessageHistory(\n",
    "                chain,\n",
    "                _get_session_history,\n",
    "                input_messages_key=\"question\",\n",
    "                history_messages_key=\"chat_history\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt):\n",
    "    return chain_with_history.invoke(\n",
    "                    {\"question\": prompt},\n",
    "                    config={\"configurable\": {\"session_id\": \"test1234\"}},\n",
    "                    callbacks = [callback_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë©°, ê·¸ ì¤‘ìš”ì„±ì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
    "\n",
    "2. ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ í† í°ì˜ ê³ ì°¨ì› ë²¡í„°ë¥¼ ì–´ë–»ê²Œ ì¡°ì •í•˜ì—¬ ë¬¸ë§¥ì— ë”°ë¥¸ ì˜ë¯¸ë¥¼ ë” í’ë¶€í•˜ê²Œ ë§Œë“œëŠ”ê°€ìš”?\n",
    "\n",
    "3. ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì—¬ëŸ¬ í—¤ë“œëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë©°, ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat('Transformerì˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ ì¤„ ìˆ˜ ìˆë‚˜ìš”?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, chunk_overlap=500\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"script.json\", \"r\") as f:\n",
    "    script_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"test1234\",exist_ok=True)\n",
    "with open(\"test1234/transcript.json\", \"w\") as f:\n",
    "    json.dump(script_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "            Document(page_content=\"\\n\".join([t[\"text\"] for t in script_data]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store.save_local(\"test1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store_load = FAISS.load_local(\"test11\", embeddings=embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[FINAL SUMMARY]\\nKey topic: ë ˆê·¸ì˜ ì´í•´ ë° ì •ë³´ ì²˜ë¦¬\\n\\nâ€¢ ğŸ“š ë ˆê·¸ì˜ ë¹„ë²•ë…¸íŠ¸ì— ë„ë‹¬í•˜ê¸° ìœ„í•œ ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.  \\nâ€¢ ğŸ”„ ë°˜ë³µ í•™ìŠµì„ í†µí•´ ë ˆê·¸ì˜ ê¸°ë³¸ ê°œë…ê³¼ êµ¬í˜„ ë°©ì‹ì„ ì´í•´í•´ì•¼ í•œë‹¤.  \\nâ€¢ âœï¸ ë ˆê·¸ëŠ” ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ì •ë³´ ì°¸ì¡°ë¥¼ í†µí•´ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” AIì´ë‹¤.  \\nâ€¢ âš™ï¸ ì‚¬ì „í•™ìŠµëœ ì •ë³´ì™€ ìµœì‹  ì •ë³´ì˜ ì°¨ì´ë¥¼ ì´í•´í•´ì•¼ í•˜ë©°, ì •ë³´ì˜ íë¦„ì„ ìƒì§€ ì•Šë„ë¡ ì£¼ì˜í•´ì•¼ í•œë‹¤.  \\nâ€¢ ğŸ” íš¨ê³¼ì ì¸ ì •ë³´ ì²˜ë¦¬ë¥¼ ìœ„í•´ ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ì˜ í˜ì´ì§€ë§Œ ì œê³µí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.  \\nâ€¢ ğŸ“„ ë¬¸ì„œì˜ íŠ¹ì • ë‹¨ë½ì„ ì„ íƒí•˜ê³ , ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•œë‹¤.  \\nâ€¢ ğŸ’¡ ì¸ë² ë”© ê³¼ì •ì„ í†µí•´ ë¬¸ì¥ì„ ìˆ˜í•™ì  í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë³´ ê²€ìƒ‰ì„ í–¥ìƒì‹œí‚¨ë‹¤.\\n\\n[RECOMMEND QUESTIONS]\\n1. ë ˆê·¸ë¥¼ í™œìš©í•˜ì—¬ ìµœì‹  ì •ë³´ë¥¼ ì–´ë–»ê²Œ íš¨ìœ¨ì ìœ¼ë¡œ ì œê³µí•  ìˆ˜ ìˆì„ê¹Œ?\\n2. ì‚¬ì „í•™ìŠµëœ ì •ë³´ì™€ ìµœì‹  ì •ë³´ì˜ í™œìš© ì‹œ ê³ ë ¤í•´ì•¼ í•  ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€?\\n3. ì¸ë² ë”© ê³¼ì •ì´ ì •ë³´ ê²€ìƒ‰ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆì„ê¹Œ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[0].strip(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[0].strip(\"\\n\\n\").replace(\"\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.container?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "import re\n",
    "\n",
    "\n",
    "class YouTubeService:\n",
    "    async def get_title_and_hashtags(self, url: str):\n",
    "        yt = await _create_youtube_instance(url)\n",
    "        print(\"ì˜ìƒ ì •ë³´ í™•ì¸\")\n",
    "        title = yt.title\n",
    "        description = yt.description\n",
    "        hashtags = re.findall(r\"#\\w+\", description)\n",
    "        return {\"title\": title, \"hashtags\": \" \".join(hashtags)}\n",
    "\n",
    "    async def get_video_info(self, url: str):\n",
    "        yt = await _create_youtube_instance(url)\n",
    "        audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "        print(\"ìŒì„± ì¶”ì¶œ ì™„ë£Œ\")\n",
    "        return {\n",
    "            \"title\": yt.title,\n",
    "            \"audio_url\": audio_stream.url if audio_stream else None,\n",
    "        }\n",
    "\n",
    "    async def _create_youtube_instance(self, url: str):\n",
    "        print(\"YouTube ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ\")\n",
    "        return YouTube(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import ffmpeg\n",
    "import requests\n",
    "import soundfile as sf\n",
    "from faster_whisper import BatchedInferencePipeline, WhisperModel\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "class WhisperTranscriptionService:\n",
    "    def __init__(self):\n",
    "        model = WhisperModel(\n",
    "            \"large-v3\", device=\"cuda\", compute_type=\"float16\"\n",
    "        )\n",
    "        model = BatchedInferencePipeline(model=model)\n",
    "        language = None\n",
    "        okt = Okt()\n",
    "        print(\"Whisper ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "    def create_session(self):\n",
    "        session = requests.Session()\n",
    "        retry = Retry(\n",
    "            total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        return session\n",
    "\n",
    "    def download_chunk(self, args):\n",
    "        url, start, end, chunk_number, temp_dir = args\n",
    "\n",
    "        headers = {\"Range\": f\"bytes={start}-{end}\"}\n",
    "        session = create_session()\n",
    "\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, stream=True)\n",
    "            chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_number:04d}\")\n",
    "\n",
    "            with open(chunk_path, \"wb\") as f:\n",
    "                for data in response.iter_content(chunk_size=8192):\n",
    "                    f.write(data)\n",
    "\n",
    "            return chunk_path, chunk_number\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading chunk {chunk_number}: {str(e)}\")\n",
    "            return None, chunk_number\n",
    "\n",
    "    def _single_stream_download(self, url: str, temp_dir: str) -> str:\n",
    "        \"\"\"ë‹¨ì¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        print(\"Starting single stream download...\")\n",
    "        session = create_session()\n",
    "        output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "\n",
    "        try:\n",
    "            with session.get(url, stream=True) as response:\n",
    "                response.raise_for_status()\n",
    "                with open(output_path, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to download file: {str(e)}\")\n",
    "\n",
    "    def parallel_download(self, url: str, temp_dir: str, num_chunks: int = 10) -> str:\n",
    "        \"\"\"ë³‘ë ¬ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•˜ê³ , ì‹¤íŒ¨ ì‹œ ë‹¨ì¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ í´ë°±\"\"\"\n",
    "        session = create_session()\n",
    "\n",
    "        try:\n",
    "            # HEAD ìš”ì²­ìœ¼ë¡œ íŒŒì¼ í¬ê¸° í™•ì¸ ì‹œë„\n",
    "            response = session.head(url, allow_redirects=True)\n",
    "            total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "            # HEAD ìš”ì²­ì´ ì‹¤íŒ¨í•˜ë©´ GET ìš”ì²­ìœ¼ë¡œ ì‹œë„\n",
    "            if total_size == 0:\n",
    "                response = session.get(url, stream=True)\n",
    "                total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "            # íŒŒì¼ í¬ê¸°ë¥¼ ì—¬ì „íˆ í™•ì¸í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ë‹¨ì¼ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ\n",
    "            if total_size == 0:\n",
    "                print(\n",
    "                    \"Warning: Could not determine file size. Falling back to single stream download.\"\n",
    "                )\n",
    "                return _single_stream_download(url, temp_dir)\n",
    "            print(\"Starting parallel download...\")\n",
    "            chunk_size = total_size // num_chunks\n",
    "            chunks = []\n",
    "\n",
    "            for i in range(num_chunks):\n",
    "                start = i * chunk_size\n",
    "                end = start + chunk_size - 1 if i < num_chunks - 1 else total_size - 1\n",
    "                chunks.append((start, end))\n",
    "\n",
    "            download_args = [\n",
    "                (url, start, end, i, temp_dir) for i, (start, end) in enumerate(chunks)\n",
    "            ]\n",
    "\n",
    "            chunk_paths = []\n",
    "            with concurrent.futures.ThreadPoolExecutor(\n",
    "                max_workers=num_chunks\n",
    "            ) as executor:\n",
    "                futures = executor.map(download_chunk, download_args)\n",
    "                chunk_paths = [(path, num) for path, num in futures if path is not None]\n",
    "\n",
    "            if not chunk_paths:\n",
    "                raise Exception(\"No chunks were downloaded successfully\")\n",
    "\n",
    "            chunk_paths.sort(key=lambda x: x[1])\n",
    "            output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "\n",
    "            with open(output_path, \"wb\") as outfile:\n",
    "                for chunk_path, _ in chunk_paths:\n",
    "                    with open(chunk_path, \"rb\") as infile:\n",
    "                        outfile.write(infile.read())\n",
    "                    os.remove(chunk_path)\n",
    "\n",
    "            return output_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error in parallel download: {str(e)}. Falling back to single stream download.\"\n",
    "            )\n",
    "            return _single_stream_download(url, temp_dir)\n",
    "\n",
    "    def convert_to_wav(self, input_path: str, output_path: str) -> bool:\n",
    "        try:\n",
    "            stream = ffmpeg.input(input_path)\n",
    "            stream = ffmpeg.output(\n",
    "                stream, output_path, acodec=\"pcm_s16le\", ar=\"16000\", ac=\"1\"\n",
    "            )\n",
    "            ffmpeg.run(stream, capture_stdout=True, capture_stderr=True)\n",
    "            return True\n",
    "        except ffmpeg.Error as e:\n",
    "            print(\"FFmpeg error:\", e.stderr.decode())\n",
    "            return False\n",
    "\n",
    "    def process_audio_chunk(self, chunk_data: tuple,promp:str = None,filtered_words:list = None) -> List[Dict[str, Any]]:\n",
    "        audio_path, start_time, duration = chunk_data\n",
    "        try:\n",
    "            segments, info = model.transcribe(\n",
    "                audio_path,\n",
    "                beam_size=5,\n",
    "                best_of=7,\n",
    "                batch_size=32,\n",
    "                temperature=0.7,\n",
    "                word_timestamps=True,\n",
    "                initial_prompt=f\"ìŒì„± ì œëª©: {promp}\",\n",
    "                repetition_penalty=2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=1.1,\n",
    "                log_prob_threshold=-0.5,\n",
    "                no_speech_threshold=0.7,\n",
    "                patience=1.2,\n",
    "                hotwords=filtered_words\n",
    "            )\n",
    "            if info and hasattr(info, \"language\"):\n",
    "                language = info.language\n",
    "            return _process_segments(segments, start_time)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk at {start_time}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _process_segments(\n",
    "        self, segments, start_time: float = 0\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        transcript = []\n",
    "        for segment in segments:\n",
    "            transcript.append(\n",
    "                {\n",
    "                    \"start\": round(segment.start + start_time, 2),\n",
    "                    \"end\": round(segment.end + start_time, 2),\n",
    "                    \"text\": segment.text,\n",
    "                }\n",
    "            )\n",
    "        return transcript\n",
    "\n",
    "    async def process_with_progress(\n",
    "        self, url: str, prompt:str, filtered_words:str,chunk_duration: int = 30, num_download_chunks: int = 10\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            mp4_path = parallel_download(url, temp_dir, num_download_chunks)\n",
    "            print(\"Download complete!\")\n",
    "\n",
    "            wav_path = os.path.join(temp_dir, \"audio.wav\")\n",
    "            if not convert_to_wav(mp4_path, wav_path):\n",
    "                raise Exception(\"Failed to convert audio to WAV format\")\n",
    "\n",
    "            wav_info = sf.info(wav_path)\n",
    "            total_duration = wav_info.duration\n",
    "            total_chunks = math.ceil(total_duration / chunk_duration)\n",
    "\n",
    "            chunks_data = []\n",
    "            for i in range(total_chunks):\n",
    "                start_time = i * chunk_duration\n",
    "                chunk_wav_path = os.path.join(temp_dir, f\"chunk_{i}.wav\")\n",
    "\n",
    "                duration = min(chunk_duration, total_duration - start_time)\n",
    "                stream = ffmpeg.input(wav_path, ss=start_time, t=duration)\n",
    "                stream = ffmpeg.output(\n",
    "                    stream, chunk_wav_path, acodec=\"pcm_s16le\", ar=\"16000\", ac=\"1\"\n",
    "                )\n",
    "                ffmpeg.run(stream, quiet=True)\n",
    "\n",
    "                chunks_data.append((chunk_wav_path, start_time, duration))\n",
    "\n",
    "            all_segments = []\n",
    "            for chunk_data in chunks_data:\n",
    "                segments = process_audio_chunk(chunk_data,prompt,filtered_words)\n",
    "                all_segments.extend(segments)\n",
    "\n",
    "                if os.path.exists(chunk_data[0]):\n",
    "                    os.remove(chunk_data[0])\n",
    "\n",
    "        return all_segments\n",
    "\n",
    "    async def transcribe(self, audio_url: str,prompt: str = None) -> Dict[str, Any]:\n",
    "        try:\n",
    "            try:\n",
    "                tagged = okt.pos(prompt)\n",
    "                filtered_words = []\n",
    "                for word, tag in tagged:\n",
    "                    if tag == \"Noun\" or tag == \"Hashtag\":\n",
    "                        filtered_words.append(word)\n",
    "            except:\n",
    "                filtered_words = None\n",
    "            segments = await process_with_progress(\n",
    "                audio_url, prompt, filtered_words,chunk_duration=30, num_download_chunks=10\n",
    "            )\n",
    "\n",
    "            print(\"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ\")\n",
    "\n",
    "            return {\"script\": segments, \"language\": language}\n",
    "        except Exception as e:\n",
    "            print(f\"Error in transcribe: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = YouTubeService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = await youtube.get_video_info(\"https://youtu.be/EMMC0ym0QOI?si=bx7raBo-QwR3MGy7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info[\"audio_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper = WhisperTranscriptionService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = await whisper.transcribe(video_info[\"audio_url\"],video_info[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for script in transcript[\"script\"]:\n",
    "    print(script[\"text\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = await whisper.transcribe(video_info[\"audio_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for script in transcript[\"script\"]:\n",
    "    print(script[\"text\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### ì£¼ì œ: ë ˆê·¸ì˜ ê°œë… ë° êµ¬í˜„ ë°©ë²•\\n\\n- ğŸ“š ë ˆê·¸ ë¹„ë²•ë…¸íŠ¸ë¥¼ í†µí•´ ë ˆê·¸ì˜ ê¸°ë³¸ ê°œë…ê³¼ êµ¬í˜„ ë°©ì‹ì— ëŒ€í•œ ì´í•´ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.\\n- ğŸ” ì‹¤ìŠµ íŒŒì¼ë“¤ì„ ë°˜ë³µì ìœ¼ë¡œ ê²€í† í•˜ì—¬ ì´í•´ë„ë¥¼ ë†’ì—¬ì•¼ í•©ë‹ˆë‹¤.\\n- â“ ë ˆê·¸ì˜ ì£¼ìš” ëª©ì ì€ ìµœì‹  ì •ë³´ë¥¼ í¬í•¨í•œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\\n- ğŸ†š GPTëŠ” ì‚¬ì „í•™ìŠµëœ ì •ë³´ì— ì˜ì¡´í•˜ì§€ë§Œ, ë ˆê·¸ëŠ” ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\\n- ğŸ“Š ì˜¤ë˜ëœ ì •ë³´ëŠ” ì •í™•í•œ ë‹µë³€ì„ ë°©í•´í•˜ë©°, ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ë§Œ ì œê³µí•˜ëŠ” ê²ƒì´ ìµœì„ ì…ë‹ˆë‹¤.\\n- ğŸ“‘ ë¬¸ë§¥ì„ í†µí•´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ê³ , ìœ ì‚¬ë„ ê³„ì‚°ì„ í†µí•´ ê´€ë ¨ ë‹¨ë½ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\\n- ğŸ”„ í…ìŠ¤íŠ¸ëŠ” íŠ¹ì • í‚¤ì›Œë“œë¡œ ë¶„í• ë˜ì–´ì•¼ í•˜ë©°, ì²­í¬ ì˜¤ë²„ë©ì„ í†µí•´ ì •ë³´ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.\\n- ğŸ’¾ ì¸ë² ë”© ê³¼ì •ì„ í†µí•´ ê° ë‹¨ë½ì„ ìˆ«ì í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- ğŸ“Š ì¸ë² ë”© ì´í•´ í›„, ë°ì´í„°ë¥¼ ì €ì¥í•´ì•¼ í•˜ë©°, ì´ ê³¼ì •ì—ì„œ ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤.\\n- ğŸ“š ë‹¤ìŒ ì˜ìƒì—ì„œëŠ” ë ˆê·¸ì˜ í›„ë°˜ë¶€ ë‚´ìš©ì„ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤.\\n\\n### ì¶”ì²œ ì§ˆë¬¸:\\n1. ë ˆê·¸ì˜ êµ¬í˜„ ë°©ì‹ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?\\n2. ì¸ë² ë”© ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë¹„ìš©ì€ ì–´ë–»ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆë‚˜ìš”?\\n3. ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"### KEY TOPIC: ë ˆê·¸ì˜ ê¸°ëŠ¥ê³¼ ì •ë³´ ì ‘ê·¼ ë°©ì‹\\n\\n- ğŸ“š ë ˆê·¸ì˜ ë¹„ë²•ë…¸íŠ¸ì— ë„ì°©í•˜ê¸°ê¹Œì§€ ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.  \\n- ğŸ”„ ë ˆê·¸ì— ëŒ€í•œ ì´í•´ë¥¼ ìœ„í•´ ë°˜ë³µ í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.  \\n- ğŸ¨ ë ˆê·¸ì˜ ëª©ì ì„ ì‹œê°ì ìœ¼ë¡œ ì„¤ëª…í•  ì˜ˆì •ì…ë‹ˆë‹¤.  \\n- â“ ë ˆê·¸ëŠ” ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.  \\n- ğŸ“° ê¸°ì¡´ ì±„ì° PTì™€ ë¹„êµí•˜ì—¬ ì •ë³´ ì ‘ê·¼ ë°©ì‹ì„ ì„¤ëª…í•©ë‹ˆë‹¤.  \\n- âš™ï¸ í”„ë¡¬í”„íŠ¸ì˜ ë³€í™”ë¡œ ë ˆê·¸ì˜ ê¸°ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤.  \\n- ğŸ“„ PDFì™€ ê°™ì€ ìë£Œë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•¨.  \\n- ğŸ”‘ ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ë§Œ ì œê³µí•˜ëŠ” ê²ƒì´ ìµœì„ ì„.  \\n- ğŸ“ íŠ¹ì • ë‹¨ë½ë§Œ í•„ìš”í•œ ê²½ìš° ì²­í¬ ì‚¬ì´ì¦ˆë¥¼ ì„¤ì •í•˜ì—¬ ë¶„í• í•¨.  \\n- ğŸ” ìœ ì‚¬ë„ ê³„ì‚°ì„ í†µí•´ ê´€ë ¨ ë‹¨ë½ì„ ë½‘ì•„ëƒ…ë‹ˆë‹¤.  \\n- ğŸ“Š ì„ë² ë”©ì€ ë¬¸ìì—´ì„ ìˆ˜í•™ì  í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„.  \\n- ğŸ”— ë™ì¼í•œ ìˆ«ì ê°œìˆ˜ë¡œ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥í•¨.  \\n- ğŸ’° ì¸ë² ë”© ê³¼ì •ì—ì„œ ë¹„ìš©ì´ ë°œìƒí•˜ë©°, ë§ì€ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ë•Œ ì‹ ì¤‘í•´ì•¼ í•¨.  \\n- ğŸ“½ï¸ ì´ë²ˆ ì˜ìƒì€ ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ë‹¤ë£¨ì—ˆê³ , ë‹¤ìŒ ì˜ìƒì—ì„œëŠ” í›„ë°˜ë¶€ë¥¼ ë‹¤ë£° ì˜ˆì •.\\n\\n### RECOMMENDED QUESTIONS:\\n1. ë ˆê·¸ì˜ ì •ë³´ ì ‘ê·¼ ë°©ì‹ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?\\n2. ì„ë² ë”© ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë¹„ìš©ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?\\n3. ë ˆê·¸ì˜ ê¸°ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ë°˜ë³µ í•™ìŠµì˜ í•„ìš”ì„±ì€ ì–´ë–¤ ì ì—ì„œ ì¤‘ìš”í•œê°€ìš”?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Key Topic: RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œì˜ êµ¬í˜„ ë° ì´í•´\\n\\n- ğŸ‰ ì—¬ëŸ¬ë¶„ì€ RAGì˜ ë¹„ë²•ë…¸íŠ¸ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.\\n- ğŸ”„ RAGì˜ ë‚´ìš©ì„ ë°˜ë³µí•˜ì—¬ ì´í•´ë¥¼ ë†’ì´ì„¸ìš”.\\n- ğŸ“‚ ì‹¤ìŠµ íŒŒì¼ì„ í†µí•´ RAG í”„ë¡œì„¸ìŠ¤ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\\n- ğŸ§ RAGê°€ ë¬´ì—‡ì¸ì§€ì™€ êµ¬í˜„ ë°©ë²•ì„ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.\\n- ğŸ¨ RAG ì‚¬ìš© ëª©ì ì„ ê·¸ë¦¼ìœ¼ë¡œ ì„¤ëª…í•  ì˜ˆì •ì…ë‹ˆë‹¤.\\n- â“ RAGì˜ í•„ìš”ì„±ì„ ì±„ì° PTì™€ ë¹„êµí•˜ì—¬ ì„¤ëª…í•  ê²ƒì…ë‹ˆë‹¤.\\n- ğŸ“„ ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ RAGì˜ ì£¼ìš” ëª©ì ì…ë‹ˆë‹¤.\\n- ğŸ“Š GPDëŠ” ì‚¬ì „í•™ìŠµëœ ì •ë³´ì— ì˜ì¡´í•˜ë¯€ë¡œ ì •ë³´ê°€ ì˜¤ë˜ë˜ë©´ ì •í™•í•œ ë‹µë³€ì„ í•˜ì§€ ëª»í•©ë‹ˆë‹¤.\\n- ğŸ”— RAGëŠ” ì£¼ì–´ì§„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ê°€ ë°”ë€ë‹ˆë‹¤.\\n- ğŸ“š GPDëŠ” ì‚¬ì „í•™ìŠµëœ ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- â³ ì˜¤ë˜ëœ ì‚¬ì „í•™ìŠµ ì •ë³´ëŠ” ì •í™•í•œ ë‹µë³€ì„ ì–´ë µê²Œ ë§Œë“­ë‹ˆë‹¤.\\n- ğŸ” í”„ë¡¬í”„íŠ¸ê°€ ë³€ê²½ë˜ì–´ ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ë„ë¡ í•©ë‹ˆë‹¤.\\n- ğŸ“„ PDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- âš ï¸ ë§ì€ ì •ë³´ë¥¼ ì…ë ¥í•  ê²½ìš° ë¹„ìš©ì´ ì¦ê°€í•˜ê³  ì •ë³´ íƒìƒ‰ì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- ğŸ”‘ ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ë§Œ ì œê³µí•˜ëŠ” ê²ƒì´ ìµœì„ ì…ë‹ˆë‹¤.\\n- ğŸ“ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê¸ì–´ì˜¤ëŠ” ë°©ì‹ìœ¼ë¡œ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\\n- ğŸ” ì§ˆë¬¸ì— ëŒ€í•œ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ë‹¨ë½ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\\n- ğŸ“ˆ ìœ ì‚¬ë„ê°€ ë†’ì€ ë‹¨ë½ì„ ê²€ìƒ‰í•´ ìµœìƒìœ„ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\\n- ğŸ§® ì¸ë² ë”©ì€ ë¬¸ì¥ì„ ìˆ˜í•™ì  í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ì–´ ì •ë³´ ê²€ìƒ‰ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\\n- ğŸ’¾ ì¸ë² ë”© í›„ ë³€í™˜ëœ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\\n- ğŸ” ì €ì¥ëœ ë°ì´í„°ëŠ” ê²€ìƒ‰ì–´ë¥¼ í†µí•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\\n- ğŸ“½ï¸ ì´ë²ˆ ì˜ìƒì€ ì‚¬ì „ ë‹¨ê³„ê¹Œì§€ ì‚´í´ë³´ì•˜ê³ , ë‹¤ìŒ ì˜ìƒì—ì„œ í›„ë°˜ë¶€ë¥¼ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤.\\n\\n### RECOMMENDED QUESTIONS:\\n1. RAG ì‹œìŠ¤í…œì´ ê¸°ì¡´ GPDì™€ ì–´ë–»ê²Œ ì°¨ë³„í™”ë˜ëŠ”ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?\\n2. RAGì˜ ì¸ë² ë”© ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë¹„ìš©ì€ ì–´ë–¤ ìš”ì†Œì— ì˜í•´ ê²°ì •ë˜ë‚˜ìš”?\\n3. PDF ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ë•Œ ìœ ì‚¬ë„ ê³„ì‚°ì€ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ë‚˜ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://youtu.be/AA621UofTUA?si=gn4XutRMWUDSYLFL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Please summarize the sentence according to the following FINAL REQUEST. \n",
    "FINAL REQUEST:\n",
    "1. The provided summary sections are partial summaries of one document. Please combine them into a single cohesive summary.\n",
    "2. Summarize the main points in bullet points in KOREAN.\n",
    "3. Each summarized sentence must start with a single emoji that fits the meaning of the sentence.\n",
    "4. Use various emojis to make the summary more interesting, but keep it concise and relevant.\n",
    "5. Focus on identifying and presenting only one main topic and one overall summary for the document.\n",
    "6. Avoid redundant or repeated points, and ensure that the summary covers all key ideas without introducing multiple conclusions or topics.\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\n",
    "FINAL SUMMARY:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please summarize the sentence according to the following REQUEST.\\nREQUEST:\\n1. Summarize the main points in bullet points in KOREAN.\\n2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\\n3. Use various emojis to make the summary more interesting.\\n4. Translate the summary into KOREAN if it is written in ENGLISH.\\n5. DO NOT translate any technical terms.\\n6. DO NOT include any unnecessary information.\\n\\nCONTEXT:\\n{context}\\n\\nSUMMARY:\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please summarize the sentence according to the following FINAL REQUEST. \\nFINAL REQUEST:\\n1. The provided summary sections are partial summaries of one document. Please combine them into a single cohesive summary.\\n2. Summarize the main points in bullet points in KOREAN, but DO NOT translate any technical terms.\\n3. Each summarized sentence must start with a single emoji that fits the meaning of the sentence.\\n4. Use various emojis to make the summary more interesting, but keep it concise and relevant.\\n5. Focus on identifying and presenting only one main topic and one overall summary for the document.\\n6. Avoid redundant or repeated points, and ensure that the summary covers all key ideas without introducing multiple conclusions or topics.\\n7. Please refer to each summary and indicate the key topic.\\n8. If the original text is in English, we have already provided a summary translated into Korean, so please do not provide a separate translation.\\n\\nCONTEXT: \\n{context}\\n\\nFINAL SUMMARY:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[FINAL SUMMARY]\\nâ€¢ ğŸ“š ë ˆê·¸ì˜ ë¹„ë²•ë…¸íŠ¸ëŠ” ë°˜ë³µ í•™ìŠµê³¼ ë ˆê·¸ì— ëŒ€í•œ ê¹Šì€ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤.\\nâ€¢ ğŸ–¼ï¸ RAGì˜ ë„ì…ì€ ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´ ì¤‘ìš”í•˜ë©°, ì¢Œì¸¡ì˜ ë ˆê·¸ì™€ ìš°ì¸¡ì˜ ê¸°ì¡´ ë°©ë²•ì„ ë¹„êµí•´ ì„¤ëª…í•©ë‹ˆë‹¤.\\nâ€¢ ğŸ“ˆ í”„ë¡¬í”„íŠ¸ì˜ ë³€í™”ì™€ ì •ë³´ì˜ ì •í™•ì„±ì´ ì¤‘ìš”í•˜ë©°, ì‚¬ì „í•™ìŠµëœ ì •ë³´ëŠ” ì‹œê°„ì´ ì§€ë‚˜ë©´ ì‹ ë¢°ì„±ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.\\nâ€¢ ğŸ”‘ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ë‹¨ë½ì„ ì°¾ì•„ë‚´ê³ , í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°ì™€ ì¸ë² ë”© ê³¼ì •ì„ í†µí•´ ì •ë³´ì˜ ì •í™•ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.\\nâ€¢ ğŸ’° ë§ì€ ì…ë ¥ ì •ë³´ëŠ” ë¹„ìš© ì¦ê°€ì™€ ì •ë³´ íƒìƒ‰ì˜ ì–´ë ¤ì›€ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n[RECOMMEND QUESTIONS]\\n1. RAGì˜ ë„ì…ì´ ì™œ ì¤‘ìš”í•œê°€ìš”?\\n2. ìœ ì‚¬ë„ ê²€ìƒ‰ì—ì„œ ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ ê´€ë ¨ ë‹¨ë½ì„ ì°¾ë‚˜ìš”?\\n3. ì…ë ¥ ì •ë³´ê°€ ë§ì„ ë•Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[1].split(\"\\n\")[1].split(\".\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = [{\"start\": 0.0, \"end\": 29.42, \"text\": \" ì—¬ëŸ¬ë¶„ ì•ˆë…•í•˜ì„¸ìš” ë“œë””ì–´ ë ˆê·¸ì˜ ë¹„ë²•ë…¸íŠ¸ì— ë ˆê·¸ íŒŒíŠ¸ê¹Œì§€ ì˜¤ì‹œëŠë¼ ì •ë§ ê³ ìƒ ë§ìœ¼ì…¨ìŠµë‹ˆë‹¤ ë ˆê·¸ì˜ ì „ë°˜ì ì¸ ë‚´ìš©ì„ ë¨¼ì € í•œë²ˆ ë“¤ì–´ë³´ì‹œê³ ìš” ê·¸ë¦¬ê³  ì˜ ì´í•´ê°€ ì•ˆë˜ë©´ ë˜ ë°˜ë³µí•´ì„œ ë“¤ì–´ë³´ì‹¤ ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ ë°˜ë³µí•´ì„œ ë“¤ì–´ë³´ì‹œê³  ê·¸ë¦¬ê³  ë” ì¤‘ìš”í•œ ê±°ëŠ” ì´ ì‹¤ìŠµ íŒŒì¼ë“¤ì„ ì—¬ëŸ¬ë¶„ë“¤ì´ ë°˜ë³µí•´ì„œ ë³´ì‹œë©´ì„œ ê³„ì† ë ˆê·¸ì— ëŒ€í•œ í”„ë¡œì„¸ìŠ¤ ì´í•´ê°€ ìˆì–´ì•¼ ê·¸ ë‹¤ìŒì— ë‹¤ì‹œ ì—­ìœ¼ë¡œ ëŒì•„ê°€ì„œ ìš°ë¦¬ê°€ ì´ëŸ° ê²ƒë“¤ì„ ì‚´í´ë³¼ ê±°ì—ìš” ì•„í”„íŒŒì„œë‘ ëª¨ë¸ ë©”ëª¨ë¦¬ ì²´ì¸ë“¤ ì´ëŸ° ê²ƒë“¤ì„ ì­‰ ì‚´í´ë³¼ ë•Œ ì—­ìœ¼ë¡œ ë” ì´í•´ê°€ ì˜ ë˜ì‹¤ ê±°ë¼ëŠ” ìƒê°ì´ ë“¤ë”ë¼êµ¬ìš”\"}, {\"start\": 30.0, \"end\": 42.5, \"text\": \" ìš°ë¦¬ê°€ ì—¬ê¸° ì²˜ìŒë¶€í„° ë‹¤ í•˜ê³  ê°€ë ¤ë©´ ë„ˆë¬´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë‹ˆê¹Œ ì´ë²ˆ ì‹œê°„ì—ëŠ” ë ˆê·¸ë¥¼ ì¢€ ê¹Šê²Œ ë‹¤ë¤„ë³´ê¸° ë³´ë‹¤ëŠ” ì¼ë‹¨ì€ ë ˆê·¸ê°€ ë­”ì§€ ê·¸ë¦¬ê³  ì–´ë–¤ ì‹ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ”ì§€ ëŒ€ì¶© ê°ì„ ì¡ëŠ”ë‹¤ ê·¸ëŸ° ìƒê°ìœ¼ë¡œ ì˜¤ì‹œë©´ ë©ë‹ˆë‹¤.\"}, {\"start\": 43.21, \"end\": 60.03, \"text\": \" ì €í¬ê°€ ë¨¼ì € ì—¬ê¸° ë ˆê·¸ì˜ ë² ì´ì‹, ì´ ì •ë„ ìˆ˜ì¤€ì—ì„œ ë¨¼ì € ë³¼ ê±´ë°ìš”. ë¨¼ì € ê·¸ëŸ¬ë ¤ë©´ì€ ìš°ë¦¬ê°€ ë ˆê·¸ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•  ê²ƒ ê°™ì•„ìš”. ê·¸ë˜ì„œ ì œê°€ ì¢€ ê·¸ë¦¼ìœ¼ë¡œ ê·¸ë ¤ì™”ì–´ìš”. ì œê°€ ê·¸ë¦¼ìœ¼ë¡œ ê·¸ë¦¬ëŠ” ê±¸ ë˜ê²Œ ì¢‹ì•„í•˜ëŠ”ë° ì´ ë ˆê·¸ë¼ëŠ” ê±¸ ë„ëŒ€ì²´ ì™œ ì“°ëŠëƒ, ìš°ë¦¬ê°€ ê·¸ ê°•ì˜ ì´ˆë°˜ì—ë„ ë§ì”€ë“œë ¸ì–ì•„ìš”.\"}, {\"start\": 60.0, \"end\": 74.72, \"text\": \" ë ˆê·¸ë¥¼ ì“°ëŠ” ëª©ì ì— ëŒ€í•´ì„œ ë‹¤ì‹œ í•œ ë²ˆë§Œ ì§šê³  ë„˜ì–´ê°€ ë³¼ê²Œìš”. ìš°ë¦¬ê°€ ë ˆê·¸ë¥¼ ì•ˆ ì“°ê³  ì±„ì° PT ê°™ì€ ê±¸ í†µí•´ì„œ ì§ˆë¬¸ì„ í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì¼ë°˜ì ìœ¼ë¡œ ì±„ì° PTì—ì„œ ì“°ëŠ” ê±°ëŠ” ì´ëŸ° ë°©ì‹ì´ê±°ë“ ìš”. ì—¬ê¸°ì— ì—¬ëŸ¬ë¶„ë“¤ì´ ì´ëŸ¬í•œ í€˜ì…˜ë“¤ì„ ë„£ì–´ì¤˜ìš”.\"}, {\"start\": 76.47, \"end\": 89.55, \"text\": \" í”„ë¡¬í”„íŠ¸ë¡œ ë“¤ì–´ê°€ì£ . ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ëŸ° ê²ƒë“¤ì´ ë“¤ì–´ê°€ê³ ìš”. ê·¸ ë‹¤ìŒì— ì¢€ ë” í™•ëŒ€í•´ì„œ ë³´ì—¬ë“œë¦¬ë©´ ì´ë ‡ê²Œ ë“¤ì–´ê°€ì£ . ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë‹µë³€ì„ í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸.\"}, {\"start\": 90.82, \"end\": 117.02, \"text\": \" ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì—¬ê¸° ë“¤ì–´ì™€ìš”. ê·¸ëŸ¬ë©´ ìš°ë¦¬ê°€ ìŠ¤íŠ¸ë¦¬ë°‹ìœ¼ë¡œ êµ¬í˜„í•œ ê²ƒì²˜ëŸ¼ ìš”ê±°ì— ëŒ€í•´ì„œ í”„ë¡¬í”„íŠ¸ ì™„ì„±ì„ í•´ì„œ ê²°êµ­ì—ëŠ” ì´ LLMí•œí…Œ ì „ë‹¬ì´ ëœë‹¤ëŠ” ê±°ì˜ˆìš”. ìš°ë¦¬ê°€ ê·¸ê±¸ GPTë¥¼ ì“¸ ìˆ˜ë„ ìˆê³  ì•„ë‹ˆë©´ ë­ í´ë¡œë“œë¼ëŠ” ëª¨ë¸ì„ ì“¸ ìˆ˜ë„ ìˆê³  ë¼ë§ˆ3ë¼ëŠ” ì˜¤í”ˆëª¨ë¸ì„ ì“¸ ìˆ˜ë„ ìˆê³ ìš”. ì–´ì¨Œë“  ì´ê±¸ ë„£ì–´ì„œ ìš°ë¦¬ê°€ ì–»ëŠ” ë‹µë³€ì€ ë­ëƒë©´ ì‚¼ì„±ì „ìê°€ ìì²´ ê°œë°œí•œ AIì˜ ì´ë¦„ì€ ìš”ê±°ëŠ” ì œê°€ ì±„ì°PTí•œí…Œ ë¬¼ì–´ë³¸ ê±°ê±°ë“ ìš”. ë‹µë³€ì„ ì´ì œ ì´ëŸ° ì‹ìœ¼ë¡œ ì¤€ë‹¤ëŠ” ê±°ì˜ˆìš”.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = [script[\"text\"] for script in scripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",torch_dtype=torch.bfloat16,device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë” ëª…í™•í•œ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "prompt_template = \"\"\"ì´ ë¬¸ì„œì˜ ì˜¤íƒˆìì™€ ì–´ìƒ‰í•œ í‘œí˜„ì„ ì „ë¬¸ êµì •ìì˜ ì…ì¥ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\n",
    "ë‹¤ìŒê³¼ ê°™ì€ ì‚¬í•­ì„ ì¤‘ì ì ìœ¼ë¡œ ê²€í† í•´ì£¼ì„¸ìš”:\n",
    "1. ë¬¸ë§¥ì— ë§ì§€ ì•ŠëŠ” ë‹¨ì–´ë¥¼ ìˆ˜ì •\n",
    "2. ì˜ì–´ ë°œìŒì€ ì•ŒíŒŒë²³ìœ¼ë¡œ ë³€ê²½\n",
    "3. ê¸°ìˆ ì ì¸ ìš©ì–´ëŠ” ì›ì–´ë¡œ ë³€ê²½\n",
    "4. ì›ë³¸ í…ìŠ¤íŠ¸ì˜ êµ¬ì¡°ë¥¼ ìˆ˜ì •í•˜ì§€ ë§ ê²ƒ\n",
    "\n",
    "ì›ë¬¸: {prompt}\n",
    "\n",
    "êµì • ê²°ê³¼:\"\"\"\n",
    "\n",
    "# ì…ë ¥ í…ì„œ ìƒì„± ë° GPU ì´ë™\n",
    "text = scripts[0]\n",
    "model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors=\"pt\")\n",
    "model_inputs = {k: v.to(\"cuda\") for k, v in model_inputs.items()}\n",
    "\n",
    "# ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.7,        # ì°½ì˜ì„± ì¡°ì ˆ (0.0-1.0)\n",
    "    \"top_p\": 0.9,             # nucleus sampling\n",
    "    \"do_sample\": True,        # ë‹¤ì–‘í•œ ì¶œë ¥ì„ ìœ„í•´ ìƒ˜í”Œë§ ì‚¬ìš©\n",
    "    \"num_return_sequences\": 1, # ìƒì„±í•  ê²°ê³¼ ìˆ˜\n",
    "    \"top_k\": 50,              # top-k sampling\n",
    "    \"repetition_penalty\": 1.2, # ë°˜ë³µ ë°©ì§€\n",
    "    \"no_repeat_ngram_size\": 3  # n-gram ë°˜ë³µ ë°©ì§€\n",
    "}\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìƒì„±\n",
    "outputs = model.generate(**model_inputs, **generation_config)\n",
    "\n",
    "# ê²°ê³¼ ë””ì½”ë”©\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "# ì›ë³¸ê³¼ êµì •ë³¸ ë¹„êµ ì¶œë ¥\n",
    "print(\"=== ì›ë³¸ ===\")\n",
    "print(text)\n",
    "print(\"\\n=== êµì •ë³¸ ===\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "\n",
    "yt = YouTube(url)\n",
    "audio_stream = yt.streams.filter(only_audio=True).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# model = WhisperModel(\n",
    "#     \"large-v3\", device=\"cuda\", compute_type=\"bfloat16\"\n",
    "# )\n",
    "# model = BatchedInferencePipeline(model=model)  # ë°°ì¹˜ ëª¨ë¸ì¼ ê²½ìš°\n",
    "# print(\"Whisper ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "\n",
    "# segments, info = model.transcribe(\n",
    "#     audio_stream.url,\n",
    "#     batch_size=64,  # ë°°ì¹˜ ëª¨ë¸ì¸ ê²½ìš°\n",
    "#     repetition_penalty=1.5,\n",
    "#     beam_size=10,\n",
    "#     patience=2,\n",
    "#     no_repeat_ngram_size=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import math\n",
    "import requests\n",
    "import tempfile\n",
    "import os\n",
    "import concurrent.futures\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import threading\n",
    "import ffmpeg\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, total_size, desc=\"Downloading\"):\n",
    "        pbar = tqdm(total=total_size, unit=\"iB\", unit_scale=True, desc=desc)\n",
    "        lock = threading.Lock()\n",
    "\n",
    "    def update(self, size):\n",
    "        with lock:\n",
    "            pbar.update(size)\n",
    "\n",
    "    def close(self):\n",
    "        pbar.close()\n",
    "\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.1,\n",
    "        status_forcelist=[500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(\n",
    "        max_retries=retry,\n",
    "        pool_connections=100,\n",
    "        pool_maxsize=100\n",
    "    )\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def download_chunk(args):\n",
    "    url, start, end, chunk_number, temp_dir, progress_bar = args\n",
    "    \n",
    "    headers = {\"Range\": f\"bytes={start}-{end}\"}\n",
    "    session = create_session()\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, headers=headers, stream=True)\n",
    "        chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_number:04d}\")\n",
    "        \n",
    "        with open(chunk_path, \"wb\") as f:\n",
    "            for data in response.iter_content(chunk_size=8192):\n",
    "                size = f.write(data)\n",
    "                progress_bar.update(size)\n",
    "        \n",
    "        return chunk_path, chunk_number\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading chunk {chunk_number}: {str(e)}\")\n",
    "        return None, chunk_number\n",
    "\n",
    "def parallel_download(url, temp_dir, num_chunks=10):\n",
    "    session = create_session()\n",
    "    response = session.head(url)\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "    \n",
    "    if total_size == 0:\n",
    "        raise ValueError(\"Could not determine file size\")\n",
    "    \n",
    "    chunk_size = total_size // num_chunks\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size - 1 if i < num_chunks - 1 else total_size - 1\n",
    "        chunks.append((start, end))\n",
    "    \n",
    "    progress_bar = ProgressBar(total_size, \"Parallel downloading\")\n",
    "    \n",
    "    download_args = [\n",
    "        (url, start, end, i, temp_dir, progress_bar)\n",
    "        for i, (start, end) in enumerate(chunks)\n",
    "    ]\n",
    "    \n",
    "    chunk_paths = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_chunks) as executor:\n",
    "        futures = executor.map(download_chunk, download_args)\n",
    "        chunk_paths = [(path, num) for path, num in futures if path is not None]\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    chunk_paths.sort(key=lambda x: x[1])\n",
    "    output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "    \n",
    "    with open(output_path, \"wb\") as outfile:\n",
    "        for chunk_path, _ in chunk_paths:\n",
    "            with open(chunk_path, \"rb\") as infile:\n",
    "                outfile.write(infile.read())\n",
    "            os.remove(chunk_path)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def convert_to_wav(input_path, output_path):\n",
    "    \"\"\"MP4ë¥¼ WAVë¡œ ë³€í™˜\"\"\"\n",
    "    try:\n",
    "        stream = ffmpeg.input(input_path)\n",
    "        stream = ffmpeg.output(stream, output_path, \n",
    "                             acodec=\"pcm_s16le\", \n",
    "                             ar=\"16000\",\n",
    "                             ac=\"1\")\n",
    "        ffmpeg.run(stream, capture_stdout=True, capture_stderr=True)\n",
    "        return True\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"FFmpeg error:\", e.stderr.decode())\n",
    "        return False\n",
    "\n",
    "def process_audio_chunk(chunk_data):\n",
    "    \"\"\"ê°œë³„ ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬\"\"\"\n",
    "    model, audio_path, start_time, duration = chunk_data\n",
    "    try:\n",
    "        segments, info = model.transcribe(\n",
    "            audio_path,\n",
    "            beam_size=5,\n",
    "            batch_size=32,\n",
    "            word_timestamps=True,\n",
    "            initial_prompt=None\n",
    "        )\n",
    "        \n",
    "        # segmentsë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì‹œê°„ ì¡°ì •\n",
    "        chunk_segments = []\n",
    "        for segment in segments:\n",
    "            segment_dict = {\n",
    "                \"start\": segment.start + start_time,\n",
    "                \"end\": segment.end + start_time,\n",
    "                \"text\": segment.text,\n",
    "                \"words\": [\n",
    "                    {\n",
    "                        \"start\": word.start + start_time,\n",
    "                        \"end\": word.end + start_time,\n",
    "                        \"word\": word.word,\n",
    "                        \"probability\": word.probability\n",
    "                    }\n",
    "                    for word in segment.words\n",
    "                ]\n",
    "            }\n",
    "            chunk_segments.append(segment_dict)\n",
    "        \n",
    "        return chunk_segments\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk at {start_time}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def process_with_progress(url, model, chunk_duration=30, num_download_chunks=10):\n",
    "    \"\"\"\n",
    "    ì „ì²´ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        print(\"Starting parallel download...\")\n",
    "        mp4_path = parallel_download(url, temp_dir, num_download_chunks)\n",
    "        print(\"Download complete!\")\n",
    "        \n",
    "        # MP4ë¥¼ WAVë¡œ ë³€í™˜\n",
    "        wav_path = os.path.join(temp_dir, \"audio.wav\")\n",
    "        if not convert_to_wav(mp4_path, wav_path):\n",
    "            raise Exception(\"Failed to convert audio to WAV format\")\n",
    "        \n",
    "        # WAV íŒŒì¼ ì •ë³´ ì½ê¸°\n",
    "        wav_info = sf.info(wav_path)\n",
    "        total_duration = wav_info.duration\n",
    "        \n",
    "        # ì²­í¬ ê³„ì‚°\n",
    "        total_chunks = math.ceil(total_duration / chunk_duration)\n",
    "        \n",
    "        # ì§„í–‰ë¥  í‘œì‹œ\n",
    "        pbar = tqdm(total=total_chunks, desc=\"Processing audio chunks\")\n",
    "        \n",
    "        # ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„\n",
    "        chunks_data = []\n",
    "        for i in range(total_chunks):\n",
    "            start_time = i * chunk_duration\n",
    "            chunk_wav_path = os.path.join(temp_dir, f\"chunk_{i}.wav\")\n",
    "            \n",
    "            # ì²­í¬ ì¶”ì¶œ\n",
    "            duration = min(chunk_duration, total_duration - start_time)\n",
    "            stream = ffmpeg.input(wav_path, ss=start_time, t=duration)\n",
    "            stream = ffmpeg.output(stream, chunk_wav_path, \n",
    "                                 acodec=\"pcm_s16le\", \n",
    "                                 ar=\"16000\",\n",
    "                                 ac=\"1\")\n",
    "            ffmpeg.run(stream, quiet=True)\n",
    "            \n",
    "            chunks_data.append((model, chunk_wav_path, start_time, duration))\n",
    "        \n",
    "        # ì²­í¬ ì²˜ë¦¬ ë° ê²°ê³¼ ìˆ˜ì§‘\n",
    "        all_segments = []\n",
    "        for chunk_data in chunks_data:\n",
    "            segments = process_audio_chunk(chunk_data)\n",
    "            all_segments.extend(segments)\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # ì‚¬ìš©í•œ ì²­í¬ íŒŒì¼ ì‚­ì œ\n",
    "            if os.path.exists(chunk_data[1]):\n",
    "                os.remove(chunk_data[1])\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "    return all_segments\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = WhisperModel(\n",
    "    \"large-v3\", \n",
    "    device=\"cuda\", \n",
    "    compute_type=\"float16\"  # bfloat16 ëŒ€ì‹  float16 ì‚¬ìš©\n",
    ")\n",
    "print(\"Whisper ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "model = BatchedInferencePipeline(model=model)\n",
    "\n",
    "# íŠ¸ëœìŠ¤í¬ë¦½ì…˜ ì‹¤í–‰\n",
    "segments = process_with_progress(\n",
    "    audio_stream.url,\n",
    "    model,\n",
    "    chunk_duration=30,\n",
    "    num_download_chunks=10\n",
    ")\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "for i, segment in enumerate(segments):\n",
    "    print(f\"{segment[\"start\"]:.2f} -> {segment[\"end\"]:.2f}: {segment[\"text\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://youtu.be/AA621UofTUA?si=gn4XutRMWUDSYLFL\"\n",
    "\n",
    "# from faster_whisper import WhisperModel\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# import tempfile\n",
    "# import os\n",
    "# import ffmpeg\n",
    "# import subprocess\n",
    "# from yt_dlp import YoutubeDL\n",
    "# import io\n",
    "\n",
    "# def get_audio_stream(url):\n",
    "#     \"\"\"URLì—ì„œ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
    "#     ydl_opts = {\n",
    "#         \"format\": \"bestaudio/best\",\n",
    "#         \"quiet\": True,\n",
    "#         \"no_warnings\": True,\n",
    "#         \"extract_audio\": True\n",
    "#     }\n",
    "    \n",
    "#     with YoutubeDL(ydl_opts) as ydl:\n",
    "#         info = ydl.extract_info(url, download=False)\n",
    "#         audio_url = info[\"url\"]\n",
    "#         duration = info.get(\"duration\", 0)\n",
    "        \n",
    "#         return audio_url, duration\n",
    "\n",
    "# def process_stream_with_progress(url, model, chunk_duration=30):\n",
    "#     \"\"\"\n",
    "#     ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì˜¤ë””ì˜¤ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - url: ì˜¤ë””ì˜¤ URL\n",
    "#     - model: WhisperModel ì¸ìŠ¤í„´ìŠ¤\n",
    "#     - chunk_duration: ê° ì²­í¬ì˜ ê¸¸ì´(ì´ˆ)\n",
    "#     \"\"\"\n",
    "#     # ìŠ¤íŠ¸ë¦¼ URL ê°€ì ¸ì˜¤ê¸°\n",
    "#     audio_url, total_duration = get_audio_stream(url)\n",
    "    \n",
    "#     # ffmpeg ëª…ë ¹ì–´ ì„¤ì •\n",
    "#     ffmpeg_cmd = [\n",
    "#         \"ffmpeg\",\n",
    "#         \"-i\", audio_url,\n",
    "#         \"-f\", \"wav\",\n",
    "#         \"-ar\", \"16000\",\n",
    "#         \"-ac\", \"1\",\n",
    "#         \"-hide_banner\",\n",
    "#         \"-loglevel\", \"error\",\n",
    "#         \"pipe:1\"\n",
    "#     ]\n",
    "    \n",
    "#     # ì§„í–‰ë¥  í‘œì‹œ ì„¤ì •\n",
    "#     total_chunks = int(np.ceil(total_duration / chunk_duration))\n",
    "#     pbar = tqdm(total=total_chunks, desc=\"Processing audio chunks\")\n",
    "    \n",
    "#     # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "#     all_segments = []\n",
    "    \n",
    "#     try:\n",
    "#         # ffmpeg í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
    "#         process = subprocess.Popen(\n",
    "#             ffmpeg_cmd,\n",
    "#             stdout=subprocess.PIPE,\n",
    "#             bufsize=10**8  # ë²„í¼ í¬ê¸° ì„¤ì •\n",
    "#         )\n",
    "        \n",
    "#         # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "#         with tempfile.TemporaryDirectory() as temp_dir:\n",
    "#             chunk_size = int(16000 * chunk_duration * 2)  # 16000Hz * seconds * 2 bytes per sample\n",
    "#             chunk_number = 0\n",
    "            \n",
    "#             while True:\n",
    "#                 # ì²­í¬ ì½ê¸°\n",
    "#                 audio_chunk = process.stdout.read(chunk_size)\n",
    "#                 if not audio_chunk:\n",
    "#                     break\n",
    "                \n",
    "#                 # ì²­í¬ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥\n",
    "#                 chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_number}.wav\")\n",
    "#                 with open(chunk_path, \"wb\") as f:\n",
    "#                     # WAV í—¤ë” ì‘ì„±\n",
    "#                     f.write(b\"RIFF\")\n",
    "#                     f.write((chunk_size + 36).to_bytes(4, \"little\"))\n",
    "#                     f.write(b\"WAVE\")\n",
    "#                     f.write(b\"fmt \")\n",
    "#                     f.write((16).to_bytes(4, \"little\"))\n",
    "#                     f.write((1).to_bytes(2, \"little\"))  # PCM\n",
    "#                     f.write((1).to_bytes(2, \"little\"))  # Mono\n",
    "#                     f.write((16000).to_bytes(4, \"little\"))  # Sample rate\n",
    "#                     f.write((32000).to_bytes(4, \"little\"))  # Byte rate\n",
    "#                     f.write((2).to_bytes(2, \"little\"))  # Block align\n",
    "#                     f.write((16).to_bytes(2, \"little\"))  # Bits per sample\n",
    "#                     f.write(b\"data\")\n",
    "#                     f.write(len(audio_chunk).to_bytes(4, \"little\"))\n",
    "#                     f.write(audio_chunk)\n",
    "                \n",
    "#                 try:\n",
    "#                     # ì²­í¬ ì²˜ë¦¬\n",
    "#                     segments, _ = model.transcribe(\n",
    "#                         chunk_path,\n",
    "#                         beam_size=5,\n",
    "#                         batch_size=32,\n",
    "#                         word_timestamps=True,\n",
    "#                         condition_on_previous_text=True\n",
    "#                     )\n",
    "                    \n",
    "#                     # ì‹œê°„ ì˜¤í”„ì…‹ ì¡°ì • ë° ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥\n",
    "#                     time_offset = chunk_number * chunk_duration\n",
    "#                     for segment in segments:\n",
    "#                         segment_dict = {\n",
    "#                             \"start\": segment.start + time_offset,\n",
    "#                             \"end\": segment.end + time_offset,\n",
    "#                             \"text\": segment.text,\n",
    "#                             \"words\": [\n",
    "#                                 {\n",
    "#                                     \"start\": word.start + time_offset,\n",
    "#                                     \"end\": word.end + time_offset,\n",
    "#                                     \"word\": word.word,\n",
    "#                                     \"probability\": word.probability\n",
    "#                                 }\n",
    "#                                 for word in segment.words\n",
    "#                             ]\n",
    "#                         }\n",
    "#                         all_segments.append(segment_dict)\n",
    "                \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing chunk {chunk_number}: {str(e)}\")\n",
    "                \n",
    "#                 finally:\n",
    "#                     # ì„ì‹œ íŒŒì¼ ì‚­ì œ\n",
    "#                     if os.path.exists(chunk_path):\n",
    "#                         os.remove(chunk_path)\n",
    "                \n",
    "#                 # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸\n",
    "#                 pbar.update(1)\n",
    "#                 chunk_number += 1\n",
    "    \n",
    "#     finally:\n",
    "#         pbar.close()\n",
    "#         if process.poll() is None:\n",
    "#             process.terminate()\n",
    "#             process.wait()\n",
    "    \n",
    "#     return all_segments\n",
    "\n",
    "# # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "# model = WhisperModel(\n",
    "#     \"large-v3\", \n",
    "#     device=\"cuda\", \n",
    "#     compute_type=\"float16\"\n",
    "# )\n",
    "# print(\"Whisper ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "\n",
    "# # íŠ¸ëœìŠ¤í¬ë¦½ì…˜ ì‹¤í–‰\n",
    "# segments = process_stream_with_progress(\n",
    "#     url,  # ìœ íŠœë¸Œ URL\n",
    "#     model,\n",
    "#     chunk_duration=30\n",
    "# )\n",
    "\n",
    "# # ê²°ê³¼ ì¶œë ¥\n",
    "# for segment in segments:\n",
    "#     print(f\"{segment[\"start\"]:.2f} -> {segment[\"end\"]:.2f}: {segment[\"text\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"script.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "            Document(page_content=\"\\n\".join([t[\"text\"] for t in data]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def calculate_tokens(text, model=\"gpt-4o-mini\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tokens(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, chunk_overlap=500\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(split_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import hub\n",
    "summary_prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7, streaming=True)\n",
    "summary_chain = create_stuff_documents_chain(llm, summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumaries = []\n",
    "for split_doc in split_docs:\n",
    "    print(type(split_doc.page_content))\n",
    "    partial_summary = summary_chain.invoke({\"context\": [split_doc]})\n",
    "    sumaries.append(partial_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_summary = Document(page_content= \"\\n\".join(sumaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_RESULT = summary_chain.invoke(\n",
    "                {\"context\": partial_summaries_doc}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(SUMMARY_RESULT.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_qa_with_sources_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pytubefix import YouTube\n",
    "import asyncio\n",
    "import torch\n",
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "\n",
    "# OpenAI API í‚¤ ì„¤ì •\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(url):\n",
    "    yt = YouTube(url)\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    return {\n",
    "        \"title\": yt.title,\n",
    "        \"audio_url\": audio_stream.url if audio_stream else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://www.youtube.com/shorts/a--NSC19MXM\"\n",
    "video_info = get_video_info(video_url)\n",
    "print(f\"Video Title: {video_info[\"title\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = WhisperModel(\"large-v3\", device=device, compute_type=compute_type)\n",
    "\n",
    "def transcribe_audio(audio_url):\n",
    "    segments, info = whisper_model.transcribe(audio_url)\n",
    "    transcript = [{\"text\": segment.text, \"start\": segment.start, \"end\": segment.end} for segment in segments]\n",
    "    return {\"script\": transcript, \"language\": info.language}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = transcribe_audio(video_info[\"audio_url\"])\n",
    "print(f\"Transcript Language: {transcript[\"language\"]}\")\n",
    "print(f\"First few lines of transcript: {transcript[\"script\"][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "summary_prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            streaming=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader, TextLoader\n",
    "docs = TextLoader(\"script.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "document = [Document(page_content=\"\\n\".join([t[\"text\"] for t in transcript[\"script\"]]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = create_stuff_documents_chain(llm,summary_prompt)\n",
    "result = await summary_chain.ainvoke({\"context\": document})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.create_documents([t[\"text\"] for t in transcript[\"script\"]])\n",
    "for doc in documents:\n",
    "    doc.page_content += \"\\n\" + summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "you_url = \"https://youtube.com/shorts/a--NSC19MXM?si=yiun-HK_7wX1sNvL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# RunPod RUNSYNC ì—”ë“œí¬ì¸íŠ¸ URL\n",
    "url = \"https://api.runpod.ai/v2/uq96boxkzy99ev/runsync\"\n",
    "\n",
    "# FastAPIì˜ /hello ì—”ë“œí¬ì¸íŠ¸ë¡œ ìš”ì²­í•˜ê¸° ìœ„í•œ ë°ì´í„° (ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•  íŒŒë¼ë¯¸í„° ì„¤ì •)\n",
    "body = {\"input\":{\n",
    "    \"api\":{\n",
    "        \"method\":\"POST\",\n",
    "        \"endpoint\":\"/ping\",\n",
    "    },\n",
    "    \"payload\":{},\n",
    "}}\n",
    "# ìš”ì²­ í—¤ë”ì— API í‚¤ ì¶”ê°€\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC ìš”ì²­ ë³´ë‚´ê¸°\n",
    "response = requests.post(url, json=body, headers=headers)\n",
    "\n",
    "# ì‘ë‹µ í™•ì¸\n",
    "if response.status_code == 200:\n",
    "    print(\"Response:\", response.json())\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# ì‘ì—… ID (ì‘ì—… ì™„ë£Œëœ job ID)\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "# RunPod API STATUS ì—”ë“œí¬ì¸íŠ¸ URL\n",
    "status_url = f\"https://api.runpod.ai/v2/wm1xrz07all039/status/{job_id}\"\n",
    "\n",
    "\n",
    "# ìš”ì²­ í—¤ë”ì— API í‚¤ ì¶”ê°€\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# ì‘ì—… ìƒíƒœ ë° ê²°ê³¼ í™•ì¸ ìš”ì²­ ë³´ë‚´ê¸°\n",
    "response = requests.get(status_url, headers=headers)\n",
    "\n",
    "# ì‘ë‹µ í™•ì¸\n",
    "if response.status_code == 200:\n",
    "    job_result = response.json()\n",
    "    if job_result.get(\"status\") == \"COMPLETED\":\n",
    "        print(\"Job Completed! Result:\", job_result.get(\"output\"))\n",
    "    else:\n",
    "        print(f\"Job Status: {job_result.get(\"status\")}\")\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC ì—”ë“œí¬ì¸íŠ¸ URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "you_url = \"https://youtu.be/omEk2BNDt1I?si=xjtbYANtlux5CTfB\"\n",
    "\n",
    "\n",
    "# FastAPIì˜ /hello ì—”ë“œí¬ì¸íŠ¸ë¡œ ìš”ì²­í•˜ê¸° ìœ„í•œ ë°ì´í„°\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/get_title_hash\",\n",
    "        \"method\": \"GET\",\n",
    "        # \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"url\": you_url},\n",
    "    }\n",
    "}\n",
    "\n",
    "# ìš”ì²­ í—¤ë”ì— API í‚¤ ì¶”ê°€\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC ìš”ì²­ ë³´ë‚´ê¸°\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# ì‘ë‹µ í™•ì¸\n",
    "if response.status_code == 200:\n",
    "    print(\"Response:\", response.json())\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "# endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# # RunPod RUNSYNC ì—”ë“œí¬ì¸íŠ¸ URL\n",
    "# url = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "\n",
    "# # FastAPIì˜ /hello ì—”ë“œí¬ì¸íŠ¸ë¡œ ìš”ì²­í•˜ê¸° ìœ„í•œ ë°ì´í„°\n",
    "# payload = {\n",
    "#     \"input\": {\n",
    "#         \"endpoint\": \"/get_script_summary\",\n",
    "#         \"method\": \"GET\",\n",
    "#         \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "#         \"params\": {\"url\": you_url},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # ìš”ì²­ í—¤ë”ì— API í‚¤ ì¶”ê°€\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {api_key}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # RUNSYNC ìš”ì²­ ë³´ë‚´ê¸°\n",
    "# response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# # ì‘ë‹µ í™•ì¸\n",
    "# if response.status_code == 200:\n",
    "#     print(\"Response:\", response.json())\n",
    "# else:\n",
    "#     print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC ì—”ë“œí¬ì¸íŠ¸ URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/run\"\n",
    "\n",
    "# FastAPIì˜ /hello ì—”ë“œí¬ì¸íŠ¸ë¡œ ìš”ì²­í•˜ê¸° ìœ„í•œ ë°ì´í„°\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/get_script_summary\",\n",
    "        \"method\": \"GET\",\n",
    "        \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"url\": you_url},\n",
    "    }\n",
    "}\n",
    "\n",
    "# ìš”ì²­ í—¤ë”ì— API í‚¤ ì¶”ê°€\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC ìš”ì²­ ë³´ë‚´ê¸°\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# ì‘ë‹µ í™•ì¸\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Initial Response:\", result)\n",
    "    \n",
    "    if result.get(\"status\") in [\"IN_PROGRESS\",\"IN_QUEUE\"]:\n",
    "        job_id = result.get(\"id\")\n",
    "        status_url = f\"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}\"\n",
    "        \n",
    "        while True:\n",
    "            status_response = requests.get(status_url, headers=headers)\n",
    "            if status_response.status_code == 200:\n",
    "                status_data = status_response.json()\n",
    "                print(f\"Current status: {status_data.get(\"status\")}\")\n",
    "                \n",
    "                if status_data.get(\"status\") == \"COMPLETED\":\n",
    "                    print(f\"ê²°ê³¼ê°’:{status_data}\")\n",
    "                    result_url = f\"https://api.runpod.ai/v2/{endpoint_id}/result/{job_id}\"\n",
    "                    result_response = requests.get(result_url, headers=headers)\n",
    "                    \n",
    "                    if result_response.status_code == 200:\n",
    "                        final_result = result_response.json()\n",
    "                        print(\"Final Result:\", final_result)\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Error fetching results: {result_response.status_code}\")\n",
    "                        print(f\"Error message: {result_response.text}\")\n",
    "                        break\n",
    "                elif status_data.get(\"status\") == \"FAILED\":\n",
    "                    print(\"Job failed\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Error checking status: {status_response.status_code}\")\n",
    "                print(f\"Error message: {status_response.text}\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)  # 5ì´ˆ ëŒ€ê¸° í›„ ë‹¤ì‹œ ìƒíƒœ í™•ì¸\n",
    "    else:\n",
    "        print(\"Job completed immediately\")\n",
    "        print(\"Final Result:\", result)\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC ì—”ë“œí¬ì¸íŠ¸ URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}\"\n",
    "url2 = f\"https://api.runpod.ai/v2/{endpoint_id}/result/{job_id}\"\n",
    "\n",
    "# ìš”ì²­ í—¤ë”ì— API í‚¤ ì¶”ê°€\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "while True:\n",
    "    # RUNSYNC ìš”ì²­ ë³´ë‚´ê¸°\n",
    "    response = requests.get(url,headers=headers)\n",
    "\n",
    "    # ì‘ë‹µ í™•ì¸\n",
    "    if response.status_code == 200:\n",
    "        if response.json().get(\"status\") == \"COMPLETED\":\n",
    "            response = requests.get(url2,headers=headers)\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Job status: {response.json().get(\"status\")}\")\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "RUNPOD_API_URL = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/rag_stream_chat\",\n",
    "        \"method\": \"POST\",\n",
    "        \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"prompt\": \"ì˜ìƒì˜ ì£¼ì œê°€ ë­”ê°€ìš”?\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    RUNPOD_API_URL, headers=headers, json=payload, stream=True\n",
    ")\n",
    "\n",
    "# for chunk in response.iter_content(chunk_size=None):\n",
    "#     if chunk:\n",
    "#         chunk_data = chunk.decode(\"utf-8\").strip()\n",
    "#         if chunk_data.startswith(\"data: \"):\n",
    "#             chunk_content = chunk_data[6:]\n",
    "#             if chunk_content == \"[DONE]\":\n",
    "#                 break\n",
    "#             try:\n",
    "#                 content = json.loads(chunk_content)\n",
    "#                 print(\"Stream content:\", content)\n",
    "#             except json.JSONDecodeError:\n",
    "#                 print(\"Invalid JSON:\", chunk_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"\"\n",
    "for chunk in response.json().get(\"output\"):\n",
    "    answer += chunk.get(\"content\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"script.txt\",\"r\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "url = \"https://www.youtube.com/watch?v=yF_YIxxjWU4\"\n",
    "yt = YouTube(url, use_po_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1RPJ223J7MY4YHK5UDRZ5F05GBFX7H9C4R8R5RW\n",
    "S1RPJ223J7MY4YHK5UDRZ5FO5GBFX7H9C4R8R5RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
