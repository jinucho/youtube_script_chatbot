{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "\n",
    "import tiktoken\n",
    "from langchain_teddynote.callbacks import StreamingCallback\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import VLLM\n",
    "import json\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"beomi/Qwen2.5-7B-Instruct-kowiki-qa-context\", temperature=0.7, streaming=True, base_url=\"http://localhost:8080/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 9, 'total_tokens': 20, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-729a0205-fd3d-44f6-b7e4-3317370ab033-0', usage_metadata={'input_tokens': 9, 'output_tokens': 11, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"하이\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 어떻게 도와드릴까요?', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'beomi/Qwen2.5-7B-Instruct-kowiki-qa-context'}, id='run-51a856e8-4899-400d-841c-f2f2914582ee-0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"하이\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class CustomStreamingCallbackHandler(BaseCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.partial_result = \"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.partial_result += token\n",
    "        print(token, end=\"\", flush=True)  # 실시간 출력\n",
    "        # '}'가 생성되면 중단\n",
    "        if \"}\" in self.partial_result:\n",
    "            raise StopIteration(\"중단 조건 '}'이 생성되어 종료합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTIAL_SUMMARY_PROMPT_TEMPLATE = \"\"\"Please summarize the sentence according to the following REQUEST.\n",
    "                                            This task is partial summay, Please Do not summarize too much.\n",
    "    \n",
    "                                            REQUEST:\n",
    "                                            1. Summarize the main points in KOREAN.\n",
    "                                            2. Translate the summary into KOREAN if it is written in ENGLISH.\n",
    "                                            3. DO NOT translate any technical terms.\n",
    "                                            4. DO NOT include any unnecessary information.\n",
    "                                            \n",
    "                                            CONTEXT:\n",
    "                                            {context}\n",
    "                                            \n",
    "                                            SUMMARY:\n",
    "                                            \"\"\"\n",
    "                                            \n",
    "FINAL_SUMMARY_PROMPT_TEMPLATE = \"\"\"\n",
    "다음 REQUEST에 따라 CONTEXT를 요약하고, 출력은 아래에 제공된 출력 형식(OUTPUT_FORMAT)과 정확히 동일하게 한 번만 작성해주세요.\n",
    "\n",
    "REQUEST:\n",
    "1. 주어진 OUTPUT(JSON 형식) 외의 텍스트나 설명을 포함하지 마세요.\n",
    "2. CONTEXT와 HUMAN MESSAGE는 출력하지마세요.\n",
    "3. 단 하나의 OUTPUT만 출력하세요.\n",
    "4. 주요 내용을 한국어로 요약하되, 전문, 기술 용어는 원본을 사용하세요.\n",
    "5. 요약된 각 문장은 해당 의미와 잘 어울리는 이모지 하나로 시작해야 합니다.\n",
    "6. 다양한 이모지를 사용하여 요약을 흥미롭게 작성하되, 간결하고 관련성 있게 유지하세요.\n",
    "7. 문서의 단일 주요 주제와 전반적인 요약에만 집중하세요.\n",
    "8. 각 요약에서 주요 주제를 명확히 나타내세요.\n",
    "9. CONTEXT의 내용이 충분히 많다면, 요약 문장을 충분히 생성하세요.\n",
    "10. 요약된 내용을 기반으로 가장 관련성 높은 세 가지 질문을 한국어로 작성하세요.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "OUTPUT_FORMAT(JSON 형식):\n",
    "{{\n",
    "    \"FINAL_SUMMARY\": {{\n",
    "        \"Key_topic\": 주요 주제 내용,\n",
    "        \"Summaries\": [\n",
    "            \"• Emoji 요약된 내용1\",\n",
    "            \"• Emoji 요약된 내용2\",\n",
    "            ...추가 요약 내용 나열\n",
    "        ]\n",
    "    }},\n",
    "    \"RECOMMEND_QUESTIONS\": [\n",
    "        \"첫 번째 질문 (한국어)\",\n",
    "        \"두 번째 질문 (한국어)\",\n",
    "        \"세 번째 질문 (한국어)\"\n",
    "    ]\n",
    "}}\n",
    "OUTPUT:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=500\n",
    ")\n",
    "partial_summary_prompt = PromptTemplate.from_template(\n",
    "    PARTIAL_SUMMARY_PROMPT_TEMPLATE\n",
    ")\n",
    "final_summary_prompt = PromptTemplate.from_template(\n",
    "    FINAL_SUMMARY_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../runpod_backend/data/backup.json\",\"r\") as f:\n",
    "#     data = json.load(f)\n",
    "# script = data[\"m25Lz9KWyDkNx6Vv\"][\"script_info\"][\"script\"]\n",
    "with open(\"./test1234/transcript.json\",\"r\") as f:\n",
    "    script = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "            Document(page_content=\"\\n\".join([t[\"text\"] for t in script]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_summary_chain = create_stuff_documents_chain(\n",
    "            llm=llm, prompt=partial_summary_prompt\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, chunk_overlap=400\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tokens(text, model=\"gpt-4o-mini\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "calculate_tokens(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    emoji: str = Field(..., description=\"요약에 사용하는 이모지\")\n",
    "    content: str = Field(..., description=\"요약된 내용\")\n",
    "\n",
    "class FinalSummary(BaseModel):\n",
    "    key_topic: str = Field(..., description=\"주요 주제 내용\")\n",
    "    summaries: List[Summary] = Field(..., description=\"요약된 내용 리스트\")\n",
    "\n",
    "class RecommendQuestions(BaseModel):\n",
    "    questions: List[str] = Field(..., description=\"추천 질문 리스트\")\n",
    "\n",
    "class FullStructure(BaseModel):\n",
    "    FINAL_SUMMARY: FinalSummary = Field(..., description=\"최종 요약 정보\")\n",
    "    RECOMMEND_QUESTIONS: RecommendQuestions = Field(..., description=\"추천 질문 리스트\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser(pydantic_object=FullStructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary_chain = create_stuff_documents_chain(\n",
    "            llm=llm, prompt=final_summary_prompt, output_parser=parser\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = partial_summary_chain.invoke({\"context\":docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_summary = [Document(page_content=summary)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_handler = CustomStreamingCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = final_summary_chain.invoke({\"context\": docs})\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary[\"FINAL_SUMMARY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary[\"RECOMMEND_QUESTIONS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\n",
    "summary += f'{list(final_summary.get(\"FINAL_SUMMARY\").items())[0][0]}:{list(final_summary.get(\"FINAL_SUMMARY\").items())[0][1]}\\n'\n",
    "joined_summary = '\\n'.join(list(final_summary.get(\"FINAL_SUMMARY\").items())[1][1])\n",
    "summary += f'{list(final_summary.get(\"FINAL_SUMMARY\").items())[1][0]}:{joined_summary}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = final_summary.get(\"RECOMMEND_QUESTIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)\n",
    "print(\"------\")\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].page_content += f\"\\n{summary}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = FAISS.from_documents(split_docs, hf_embeddings)\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k = 10\n",
    "vec_retriever = vec_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "retriever = EnsembleRetriever(\n",
    "                retrievers=[bm25_retriever, vec_retriever],\n",
    "                weights=[0.7, 0.3],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 사용자의 질문에 대한 답변\n",
    "# response_schemas = [\n",
    "#     ResponseSchema(name=\"answer\", description=\"사용자의 질문에 대한 답변\"),\n",
    "# ]\n",
    "# # 응답 스키마를 기반으로 한 구조화된 출력 파서 초기화\n",
    "# output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "# format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "            \"\"\"당신은 유튜브 스크립트 기반의 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다.\n",
    "                당신의 주요 임무는 다음과 같습니다:\n",
    "                1. 기본적으로 검색된 문맥(context)과 이전 대화 내용(chat_history)을 바탕으로 질문에 대해 OUTPUT_FORMAT의 형식을 반드시 지켜서 답변하세요.\n",
    "                2. 주어진 OUTPUT(JSON 형식) 외의 텍스트나 설명을 포함하지 마세요.\n",
    "                3. 다음과 같은 경우에는 자연스럽게 내부 지식을 활용하여 답변하세요:\n",
    "                    - 검색된 문맥이 질문의 의도를 완벽하게 충족하지 못할 때\n",
    "                    - 영상의 전반적인 주제와 연관되지만 구체적인 답변이 문맥에 없을 때\n",
    "                    - 문맥에서 부분적인 정보만 찾을 수 있을 때는 문맥의 정보와 내부 지식을 조합하여 답변하세요\n",
    "\n",
    "                4. 답변 시 다음 사항을 지켜주세요:\n",
    "                - 항상 자연스러운 대화체로 답변하세요\n",
    "                - 문맥에서 답을 찾을 수 없더라도, 그 사실을 언급하지 말고 바로 답변하세요\n",
    "                - 기술 용어나 고유명사는 원어를 유지하세요\n",
    "                - 전문적인 내용도 이해하기 쉽게 설명하세요\n",
    "\n",
    "                5. 만약 질문이 영상의 주제나 내용과 전혀 관련이 없다면 \"영상과 관계 없는 질문입니다.\"라고 답변하세요.\n",
    "\n",
    "                이전 대화 내용:\n",
    "                {chat_history}\n",
    "\n",
    "                질문:\n",
    "                {question}\n",
    "\n",
    "                문맥:\n",
    "                {context}\n",
    "\n",
    "                OUTPUT_FORMAT:\n",
    "                {{\"answer\": \"답변\"}}\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chat_parser(BaseModel):\n",
    "    answer: dict = Field(..., description=\"채팅 응답\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "                {\n",
    "                    \"context\": itemgetter(\"question\") | retriever,\n",
    "                    \"question\": itemgetter(\"question\"),\n",
    "                    \"chat_history\": itemgetter(\"chat_history\"),\n",
    "                }\n",
    "                | prompt\n",
    "                # | chat_llm\n",
    "                | llm\n",
    "                | JsonOutputParser(pydantic_object=chat_parser)\n",
    "                # | StrOutputParser()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_message_store = {}\n",
    "def _get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        \"\"\"세션 ID를 기반으로 대화 기록을 가져오는 함수\"\"\"\n",
    "        if session_id not in _message_store:\n",
    "            _message_store[session_id] = ChatMessageHistory()\n",
    "        return _message_store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history = RunnableWithMessageHistory(\n",
    "                chain,\n",
    "                _get_session_history,\n",
    "                input_messages_key=\"question\",\n",
    "                history_messages_key=\"chat_history\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt):\n",
    "    return chain_with_history.invoke(\n",
    "                    {\"question\": prompt},\n",
    "                    config={\"configurable\": {\"session_id\": \"test1234\"}},\n",
    "                    callbacks = [callback_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 트랜스포머의 어텐션 메커니즘은 어떻게 작동하며, 그 중요성은 무엇인가요?\n",
    "\n",
    "2. 어텐션 메커니즘은 토큰의 고차원 벡터를 어떻게 조정하여 문맥에 따른 의미를 더 풍부하게 만드는가요?\n",
    "\n",
    "3. 어텐션 메커니즘의 여러 헤드는 어떻게 작동하며, 그 이유는 무엇인가요? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat('Transformer의 어텐션 메커니즘이 어떻게 작동하는지 간단하게 설명해 줄 수 있나요?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, chunk_overlap=500\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"script.json\", \"r\") as f:\n",
    "    script_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"test1234\",exist_ok=True)\n",
    "with open(\"test1234/transcript.json\", \"w\") as f:\n",
    "    json.dump(script_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "            Document(page_content=\"\\n\".join([t[\"text\"] for t in script_data]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store.save_local(\"test1234\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store_load = FAISS.load_local(\"test11\", embeddings=embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[FINAL SUMMARY]\\nKey topic: 레그의 이해 및 정보 처리\\n\\n• 📚 레그의 비법노트에 도달하기 위한 과정이 필요하다.  \\n• 🔄 반복 학습을 통해 레그의 기본 개념과 구현 방식을 이해해야 한다.  \\n• ✏️ 레그는 최신 정보를 제공하고, 정보 참조를 통해 질문에 답변할 수 있는 AI이다.  \\n• ⚙️ 사전학습된 정보와 최신 정보의 차이를 이해해야 하며, 정보의 흐름을 잃지 않도록 주의해야 한다.  \\n• 🔍 효과적인 정보 처리를 위해 관련성 있는 정보의 페이지만 제공하는 것이 중요하다.  \\n• 📄 문서의 특정 단락을 선택하고, 유사도를 계산하여 필요한 정보를 추출한다.  \\n• 💡 인베딩 과정을 통해 문장을 수학적 표현으로 변환하고, 이를 바탕으로 정보 검색을 향상시킨다.\\n\\n[RECOMMEND QUESTIONS]\\n1. 레그를 활용하여 최신 정보를 어떻게 효율적으로 제공할 수 있을까?\\n2. 사전학습된 정보와 최신 정보의 활용 시 고려해야 할 요소는 무엇인가?\\n3. 인베딩 과정이 정보 검색에 미치는 영향은 어떤 것들이 있을까?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[0].strip(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[0].strip(\"\\n\\n\").replace(\"\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.container?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "import re\n",
    "\n",
    "\n",
    "class YouTubeService:\n",
    "    async def get_title_and_hashtags(self, url: str):\n",
    "        yt = await _create_youtube_instance(url)\n",
    "        print(\"영상 정보 확인\")\n",
    "        title = yt.title\n",
    "        description = yt.description\n",
    "        hashtags = re.findall(r\"#\\w+\", description)\n",
    "        return {\"title\": title, \"hashtags\": \" \".join(hashtags)}\n",
    "\n",
    "    async def get_video_info(self, url: str):\n",
    "        yt = await _create_youtube_instance(url)\n",
    "        audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "        print(\"음성 추출 완료\")\n",
    "        return {\n",
    "            \"title\": yt.title,\n",
    "            \"audio_url\": audio_stream.url if audio_stream else None,\n",
    "        }\n",
    "\n",
    "    async def _create_youtube_instance(self, url: str):\n",
    "        print(\"YouTube 인스턴스 생성 완료\")\n",
    "        return YouTube(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import ffmpeg\n",
    "import requests\n",
    "import soundfile as sf\n",
    "from faster_whisper import BatchedInferencePipeline, WhisperModel\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "class WhisperTranscriptionService:\n",
    "    def __init__(self):\n",
    "        model = WhisperModel(\n",
    "            \"large-v3\", device=\"cuda\", compute_type=\"float16\"\n",
    "        )\n",
    "        model = BatchedInferencePipeline(model=model)\n",
    "        language = None\n",
    "        okt = Okt()\n",
    "        print(\"Whisper 모델 초기화 완료\")\n",
    "\n",
    "    def create_session(self):\n",
    "        session = requests.Session()\n",
    "        retry = Retry(\n",
    "            total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        return session\n",
    "\n",
    "    def download_chunk(self, args):\n",
    "        url, start, end, chunk_number, temp_dir = args\n",
    "\n",
    "        headers = {\"Range\": f\"bytes={start}-{end}\"}\n",
    "        session = create_session()\n",
    "\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, stream=True)\n",
    "            chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_number:04d}\")\n",
    "\n",
    "            with open(chunk_path, \"wb\") as f:\n",
    "                for data in response.iter_content(chunk_size=8192):\n",
    "                    f.write(data)\n",
    "\n",
    "            return chunk_path, chunk_number\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading chunk {chunk_number}: {str(e)}\")\n",
    "            return None, chunk_number\n",
    "\n",
    "    def _single_stream_download(self, url: str, temp_dir: str) -> str:\n",
    "        \"\"\"단일 스트림으로 파일을 다운로드합니다.\"\"\"\n",
    "        print(\"Starting single stream download...\")\n",
    "        session = create_session()\n",
    "        output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "\n",
    "        try:\n",
    "            with session.get(url, stream=True) as response:\n",
    "                response.raise_for_status()\n",
    "                with open(output_path, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to download file: {str(e)}\")\n",
    "\n",
    "    def parallel_download(self, url: str, temp_dir: str, num_chunks: int = 10) -> str:\n",
    "        \"\"\"병렬 다운로드를 시도하고, 실패 시 단일 스트림으로 폴백\"\"\"\n",
    "        session = create_session()\n",
    "\n",
    "        try:\n",
    "            # HEAD 요청으로 파일 크기 확인 시도\n",
    "            response = session.head(url, allow_redirects=True)\n",
    "            total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "            # HEAD 요청이 실패하면 GET 요청으로 시도\n",
    "            if total_size == 0:\n",
    "                response = session.get(url, stream=True)\n",
    "                total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "            # 파일 크기를 여전히 확인할 수 없는 경우 단일 스트림으로 다운로드\n",
    "            if total_size == 0:\n",
    "                print(\n",
    "                    \"Warning: Could not determine file size. Falling back to single stream download.\"\n",
    "                )\n",
    "                return _single_stream_download(url, temp_dir)\n",
    "            print(\"Starting parallel download...\")\n",
    "            chunk_size = total_size // num_chunks\n",
    "            chunks = []\n",
    "\n",
    "            for i in range(num_chunks):\n",
    "                start = i * chunk_size\n",
    "                end = start + chunk_size - 1 if i < num_chunks - 1 else total_size - 1\n",
    "                chunks.append((start, end))\n",
    "\n",
    "            download_args = [\n",
    "                (url, start, end, i, temp_dir) for i, (start, end) in enumerate(chunks)\n",
    "            ]\n",
    "\n",
    "            chunk_paths = []\n",
    "            with concurrent.futures.ThreadPoolExecutor(\n",
    "                max_workers=num_chunks\n",
    "            ) as executor:\n",
    "                futures = executor.map(download_chunk, download_args)\n",
    "                chunk_paths = [(path, num) for path, num in futures if path is not None]\n",
    "\n",
    "            if not chunk_paths:\n",
    "                raise Exception(\"No chunks were downloaded successfully\")\n",
    "\n",
    "            chunk_paths.sort(key=lambda x: x[1])\n",
    "            output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "\n",
    "            with open(output_path, \"wb\") as outfile:\n",
    "                for chunk_path, _ in chunk_paths:\n",
    "                    with open(chunk_path, \"rb\") as infile:\n",
    "                        outfile.write(infile.read())\n",
    "                    os.remove(chunk_path)\n",
    "\n",
    "            return output_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error in parallel download: {str(e)}. Falling back to single stream download.\"\n",
    "            )\n",
    "            return _single_stream_download(url, temp_dir)\n",
    "\n",
    "    def convert_to_wav(self, input_path: str, output_path: str) -> bool:\n",
    "        try:\n",
    "            stream = ffmpeg.input(input_path)\n",
    "            stream = ffmpeg.output(\n",
    "                stream, output_path, acodec=\"pcm_s16le\", ar=\"16000\", ac=\"1\"\n",
    "            )\n",
    "            ffmpeg.run(stream, capture_stdout=True, capture_stderr=True)\n",
    "            return True\n",
    "        except ffmpeg.Error as e:\n",
    "            print(\"FFmpeg error:\", e.stderr.decode())\n",
    "            return False\n",
    "\n",
    "    def process_audio_chunk(self, chunk_data: tuple,promp:str = None,filtered_words:list = None) -> List[Dict[str, Any]]:\n",
    "        audio_path, start_time, duration = chunk_data\n",
    "        try:\n",
    "            segments, info = model.transcribe(\n",
    "                audio_path,\n",
    "                beam_size=5,\n",
    "                best_of=7,\n",
    "                batch_size=32,\n",
    "                temperature=0.7,\n",
    "                word_timestamps=True,\n",
    "                initial_prompt=f\"음성 제목: {promp}\",\n",
    "                repetition_penalty=2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=1.1,\n",
    "                log_prob_threshold=-0.5,\n",
    "                no_speech_threshold=0.7,\n",
    "                patience=1.2,\n",
    "                hotwords=filtered_words\n",
    "            )\n",
    "            if info and hasattr(info, \"language\"):\n",
    "                language = info.language\n",
    "            return _process_segments(segments, start_time)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk at {start_time}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _process_segments(\n",
    "        self, segments, start_time: float = 0\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        transcript = []\n",
    "        for segment in segments:\n",
    "            transcript.append(\n",
    "                {\n",
    "                    \"start\": round(segment.start + start_time, 2),\n",
    "                    \"end\": round(segment.end + start_time, 2),\n",
    "                    \"text\": segment.text,\n",
    "                }\n",
    "            )\n",
    "        return transcript\n",
    "\n",
    "    async def process_with_progress(\n",
    "        self, url: str, prompt:str, filtered_words:str,chunk_duration: int = 30, num_download_chunks: int = 10\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            mp4_path = parallel_download(url, temp_dir, num_download_chunks)\n",
    "            print(\"Download complete!\")\n",
    "\n",
    "            wav_path = os.path.join(temp_dir, \"audio.wav\")\n",
    "            if not convert_to_wav(mp4_path, wav_path):\n",
    "                raise Exception(\"Failed to convert audio to WAV format\")\n",
    "\n",
    "            wav_info = sf.info(wav_path)\n",
    "            total_duration = wav_info.duration\n",
    "            total_chunks = math.ceil(total_duration / chunk_duration)\n",
    "\n",
    "            chunks_data = []\n",
    "            for i in range(total_chunks):\n",
    "                start_time = i * chunk_duration\n",
    "                chunk_wav_path = os.path.join(temp_dir, f\"chunk_{i}.wav\")\n",
    "\n",
    "                duration = min(chunk_duration, total_duration - start_time)\n",
    "                stream = ffmpeg.input(wav_path, ss=start_time, t=duration)\n",
    "                stream = ffmpeg.output(\n",
    "                    stream, chunk_wav_path, acodec=\"pcm_s16le\", ar=\"16000\", ac=\"1\"\n",
    "                )\n",
    "                ffmpeg.run(stream, quiet=True)\n",
    "\n",
    "                chunks_data.append((chunk_wav_path, start_time, duration))\n",
    "\n",
    "            all_segments = []\n",
    "            for chunk_data in chunks_data:\n",
    "                segments = process_audio_chunk(chunk_data,prompt,filtered_words)\n",
    "                all_segments.extend(segments)\n",
    "\n",
    "                if os.path.exists(chunk_data[0]):\n",
    "                    os.remove(chunk_data[0])\n",
    "\n",
    "        return all_segments\n",
    "\n",
    "    async def transcribe(self, audio_url: str,prompt: str = None) -> Dict[str, Any]:\n",
    "        try:\n",
    "            try:\n",
    "                tagged = okt.pos(prompt)\n",
    "                filtered_words = []\n",
    "                for word, tag in tagged:\n",
    "                    if tag == \"Noun\" or tag == \"Hashtag\":\n",
    "                        filtered_words.append(word)\n",
    "            except:\n",
    "                filtered_words = None\n",
    "            segments = await process_with_progress(\n",
    "                audio_url, prompt, filtered_words,chunk_duration=30, num_download_chunks=10\n",
    "            )\n",
    "\n",
    "            print(\"텍스트 추출 완료\")\n",
    "\n",
    "            return {\"script\": segments, \"language\": language}\n",
    "        except Exception as e:\n",
    "            print(f\"Error in transcribe: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = YouTubeService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = await youtube.get_video_info(\"https://youtu.be/EMMC0ym0QOI?si=bx7raBo-QwR3MGy7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info[\"audio_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper = WhisperTranscriptionService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = await whisper.transcribe(video_info[\"audio_url\"],video_info[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for script in transcript[\"script\"]:\n",
    "    print(script[\"text\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = await whisper.transcribe(video_info[\"audio_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for script in transcript[\"script\"]:\n",
    "    print(script[\"text\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### 주제: 레그의 개념 및 구현 방법\\n\\n- 📚 레그 비법노트를 통해 레그의 기본 개념과 구현 방식에 대한 이해가 중요합니다.\\n- 🔍 실습 파일들을 반복적으로 검토하여 이해도를 높여야 합니다.\\n- ❓ 레그의 주요 목적은 최신 정보를 포함한 정확한 답변을 제공하는 것입니다.\\n- 🆚 GPT는 사전학습된 정보에 의존하지만, 레그는 제공된 정보를 바탕으로 더 정확한 답변을 생성합니다.\\n- 📊 오래된 정보는 정확한 답변을 방해하며, 관련성 있는 정보만 제공하는 것이 최선입니다.\\n- 📑 문맥을 통해 질문에 대한 답변을 찾고, 유사도 계산을 통해 관련 단락을 추출합니다.\\n- 🔄 텍스트는 특정 키워드로 분할되어야 하며, 청크 오버랩을 통해 정보의 일관성을 유지합니다.\\n- 💾 인베딩 과정을 통해 각 단락을 숫자 표현으로 변환하고 저장하여 나중에 검색할 수 있습니다.\\n- 📊 인베딩 이해 후, 데이터를 저장해야 하며, 이 과정에서 비용이 발생합니다.\\n- 📚 다음 영상에서는 레그의 후반부 내용을 다룰 예정입니다.\\n\\n### 추천 질문:\\n1. 레그의 구현 방식에서 가장 중요한 요소는 무엇인가요?\\n2. 인베딩 과정에서 발생하는 비용은 어떻게 관리할 수 있나요?\\n3. 관련성 있는 정보를 효과적으로 추출하기 위한 방법은 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"### KEY TOPIC: 레그의 기능과 정보 접근 방식\\n\\n- 📚 레그의 비법노트에 도착하기까지 수고하셨습니다.  \\n- 🔄 레그에 대한 이해를 위해 반복 학습이 필요합니다.  \\n- 🎨 레그의 목적을 시각적으로 설명할 예정입니다.  \\n- ❓ 레그는 최신 정보를 제공하기 위해 사용됩니다.  \\n- 📰 기존 채찍 PT와 비교하여 정보 접근 방식을 설명합니다.  \\n- ⚙️ 프롬프트의 변화로 레그의 기능이 향상됩니다.  \\n- 📄 PDF와 같은 자료를 활용하여 질문에 답변함.  \\n- 🔑 관련성 있는 정보만 제공하는 것이 최선임.  \\n- 📏 특정 단락만 필요한 경우 청크 사이즈를 설정하여 분할함.  \\n- 🔍 유사도 계산을 통해 관련 단락을 뽑아냅니다.  \\n- 📊 임베딩은 문자열을 수학적 표현으로 변환하는 과정임.  \\n- 🔗 동일한 숫자 개수로 유사도 계산 가능함.  \\n- 💰 인베딩 과정에서 비용이 발생하며, 많은 문서를 처리할 때 신중해야 함.  \\n- 📽️ 이번 영상은 전처리 단계를 다루었고, 다음 영상에서는 후반부를 다룰 예정.\\n\\n### RECOMMENDED QUESTIONS:\\n1. 레그의 정보 접근 방식에서 가장 중요한 요소는 무엇인가요?\\n2. 임베딩 과정에서 발생하는 비용을 줄일 수 있는 방법은 무엇인가요?\\n3. 레그의 기능을 향상시키기 위한 반복 학습의 필요성은 어떤 점에서 중요한가요?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Key Topic: RAG (Retrieval-Augmented Generation) 시스템의 구현 및 이해\\n\\n- 🎉 여러분은 RAG의 비법노트에 도달했습니다.\\n- 🔄 RAG의 내용을 반복하여 이해를 높이세요.\\n- 📂 실습 파일을 통해 RAG 프로세스를 이해하는 것이 중요합니다.\\n- 🧐 RAG가 무엇인지와 구현 방법을 알아야 합니다.\\n- 🎨 RAG 사용 목적을 그림으로 설명할 예정입니다.\\n- ❓ RAG의 필요성을 채찍 PT와 비교하여 설명할 것입니다.\\n- 📄 최신 정보를 제공하는 것이 RAG의 주요 목적입니다.\\n- 📊 GPD는 사전학습된 정보에 의존하므로 정보가 오래되면 정확한 답변을 하지 못합니다.\\n- 🔗 RAG는 주어진 정보를 참고하여 답변하는 방식으로 프롬프트가 바뀝니다.\\n- 📚 GPD는 사전학습된 정보에 기반하여만 답변할 수 있습니다.\\n- ⏳ 오래된 사전학습 정보는 정확한 답변을 어렵게 만듭니다.\\n- 🔍 프롬프트가 변경되어 주어진 정보를 바탕으로 질문에 답변하도록 합니다.\\n- 📄 PDF 문서에서 관련 정보를 검색하여 답변할 수 있습니다.\\n- ⚠️ 많은 정보를 입력할 경우 비용이 증가하고 정보 탐색이 어려워질 수 있습니다.\\n- 🔑 관련성 있는 정보만 제공하는 것이 최선입니다.\\n- 📝 문서에서 텍스트를 긁어오는 방식으로 정보를 로드합니다.\\n- 🔍 질문에 대한 유사도 검색을 통해 관련 단락을 추출합니다.\\n- 📈 유사도가 높은 단락을 검색해 최상위 결과를 제공합니다.\\n- 🧮 인베딩은 문장을 수학적 표현으로 바꾸어 정보 검색을 향상시킵니다.\\n- 💾 인베딩 후 변환된 데이터를 저장하는 과정이 필요합니다.\\n- 🔍 저장된 데이터는 검색어를 통해 관련 문서를 찾는 데 사용됩니다.\\n- 📽️ 이번 영상은 사전 단계까지 살펴보았고, 다음 영상에서 후반부를 다룰 예정입니다.\\n\\n### RECOMMENDED QUESTIONS:\\n1. RAG 시스템이 기존 GPD와 어떻게 차별화되는지 설명할 수 있나요?\\n2. RAG의 인베딩 과정에서 발생하는 비용은 어떤 요소에 의해 결정되나요?\\n3. PDF 문서에서 정보를 검색할 때 유사도 계산은 어떻게 이루어지나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://youtu.be/AA621UofTUA?si=gn4XutRMWUDSYLFL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Please summarize the sentence according to the following FINAL REQUEST. \n",
    "FINAL REQUEST:\n",
    "1. The provided summary sections are partial summaries of one document. Please combine them into a single cohesive summary.\n",
    "2. Summarize the main points in bullet points in KOREAN.\n",
    "3. Each summarized sentence must start with a single emoji that fits the meaning of the sentence.\n",
    "4. Use various emojis to make the summary more interesting, but keep it concise and relevant.\n",
    "5. Focus on identifying and presenting only one main topic and one overall summary for the document.\n",
    "6. Avoid redundant or repeated points, and ensure that the summary covers all key ideas without introducing multiple conclusions or topics.\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\n",
    "FINAL SUMMARY:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please summarize the sentence according to the following REQUEST.\\nREQUEST:\\n1. Summarize the main points in bullet points in KOREAN.\\n2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\\n3. Use various emojis to make the summary more interesting.\\n4. Translate the summary into KOREAN if it is written in ENGLISH.\\n5. DO NOT translate any technical terms.\\n6. DO NOT include any unnecessary information.\\n\\nCONTEXT:\\n{context}\\n\\nSUMMARY:\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please summarize the sentence according to the following FINAL REQUEST. \\nFINAL REQUEST:\\n1. The provided summary sections are partial summaries of one document. Please combine them into a single cohesive summary.\\n2. Summarize the main points in bullet points in KOREAN, but DO NOT translate any technical terms.\\n3. Each summarized sentence must start with a single emoji that fits the meaning of the sentence.\\n4. Use various emojis to make the summary more interesting, but keep it concise and relevant.\\n5. Focus on identifying and presenting only one main topic and one overall summary for the document.\\n6. Avoid redundant or repeated points, and ensure that the summary covers all key ideas without introducing multiple conclusions or topics.\\n7. Please refer to each summary and indicate the key topic.\\n8. If the original text is in English, we have already provided a summary translated into Korean, so please do not provide a separate translation.\\n\\nCONTEXT: \\n{context}\\n\\nFINAL SUMMARY:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[FINAL SUMMARY]\\n• 📚 레그의 비법노트는 반복 학습과 레그에 대한 깊은 이해가 필요합니다.\\n• 🖼️ RAG의 도입은 최신 정보를 제공하기 위해 중요하며, 좌측의 레그와 우측의 기존 방법을 비교해 설명합니다.\\n• 📈 프롬프트의 변화와 정보의 정확성이 중요하며, 사전학습된 정보는 시간이 지나면 신뢰성이 떨어집니다.\\n• 🔑 유사도 검색을 통해 관련 단락을 찾아내고, 텍스트 스플리터와 인베딩 과정을 통해 정보의 정확성을 확보합니다.\\n• 💰 많은 입력 정보는 비용 증가와 정보 탐색의 어려움을 초래할 수 있습니다.\\n\\n[RECOMMEND QUESTIONS]\\n1. RAG의 도입이 왜 중요한가요?\\n2. 유사도 검색에서 어떤 방법으로 관련 단락을 찾나요?\\n3. 입력 정보가 많을 때 발생할 수 있는 문제는 무엇인가요?\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split(\"[FINAL SUMMARY]\")[1].split(\"[RECOMMEND QUESTIONS]\")[1].split(\"\\n\")[1].split(\".\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = [{\"start\": 0.0, \"end\": 29.42, \"text\": \" 여러분 안녕하세요 드디어 레그의 비법노트에 레그 파트까지 오시느라 정말 고생 많으셨습니다 레그의 전반적인 내용을 먼저 한번 들어보시고요 그리고 잘 이해가 안되면 또 반복해서 들어보실 수 있으니까 반복해서 들어보시고 그리고 더 중요한 거는 이 실습 파일들을 여러분들이 반복해서 보시면서 계속 레그에 대한 프로세스 이해가 있어야 그 다음에 다시 역으로 돌아가서 우리가 이런 것들을 살펴볼 거에요 아프파서랑 모델 메모리 체인들 이런 것들을 쭉 살펴볼 때 역으로 더 이해가 잘 되실 거라는 생각이 들더라구요\"}, {\"start\": 30.0, \"end\": 42.5, \"text\": \" 우리가 여기 처음부터 다 하고 가려면 너무 시간이 오래 걸리니까 이번 시간에는 레그를 좀 깊게 다뤄보기 보다는 일단은 레그가 뭔지 그리고 어떤 식으로 구현하는지 대충 감을 잡는다 그런 생각으로 오시면 됩니다.\"}, {\"start\": 43.21, \"end\": 60.03, \"text\": \" 저희가 먼저 여기 레그의 베이식, 이 정도 수준에서 먼저 볼 건데요. 먼저 그러려면은 우리가 레그에 대한 이해가 필요할 것 같아요. 그래서 제가 좀 그림으로 그려왔어요. 제가 그림으로 그리는 걸 되게 좋아하는데 이 레그라는 걸 도대체 왜 쓰느냐, 우리가 그 강의 초반에도 말씀드렸잖아요.\"}, {\"start\": 60.0, \"end\": 74.72, \"text\": \" 레그를 쓰는 목적에 대해서 다시 한 번만 짚고 넘어가 볼게요. 우리가 레그를 안 쓰고 채찍 PT 같은 걸 통해서 질문을 합니다. 우리가 일반적으로 채찍 PT에서 쓰는 거는 이런 방식이거든요. 여기에 여러분들이 이러한 퀘션들을 넣어줘요.\"}, {\"start\": 76.47, \"end\": 89.55, \"text\": \" 프롬프트로 들어가죠. 당신은 친절한 답변하는 AI 어시스턴트입니다. 이런 것들이 들어가고요. 그 다음에 좀 더 확대해서 보여드리면 이렇게 들어가죠. 당신은 친절한 답변을 하는 어시스턴트.\"}, {\"start\": 90.82, \"end\": 117.02, \"text\": \" 사용자의 질문이 여기 들어와요. 그러면 우리가 스트리밋으로 구현한 것처럼 요거에 대해서 프롬프트 완성을 해서 결국에는 이 LLM한테 전달이 된다는 거예요. 우리가 그걸 GPT를 쓸 수도 있고 아니면 뭐 클로드라는 모델을 쓸 수도 있고 라마3라는 오픈모델을 쓸 수도 있고요. 어쨌든 이걸 넣어서 우리가 얻는 답변은 뭐냐면 삼성전자가 자체 개발한 AI의 이름은 요거는 제가 채찍PT한테 물어본 거거든요. 답변을 이제 이런 식으로 준다는 거예요.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = [script[\"text\"] for script in scripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",torch_dtype=torch.bfloat16,device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 명확한 한국어 프롬프트 템플릿\n",
    "prompt_template = \"\"\"이 문서의 오탈자와 어색한 표현을 전문 교정자의 입장에서 자연스럽게 수정해주세요.\n",
    "다음과 같은 사항을 중점적으로 검토해주세요:\n",
    "1. 문맥에 맞지 않는 단어를 수정\n",
    "2. 영어 발음은 알파벳으로 변경\n",
    "3. 기술적인 용어는 원어로 변경\n",
    "4. 원본 텍스트의 구조를 수정하지 말 것\n",
    "\n",
    "원문: {prompt}\n",
    "\n",
    "교정 결과:\"\"\"\n",
    "\n",
    "# 입력 텐서 생성 및 GPU 이동\n",
    "text = scripts[0]\n",
    "model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors=\"pt\")\n",
    "model_inputs = {k: v.to(\"cuda\") for k, v in model_inputs.items()}\n",
    "\n",
    "# 생성 파라미터 설정\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.7,        # 창의성 조절 (0.0-1.0)\n",
    "    \"top_p\": 0.9,             # nucleus sampling\n",
    "    \"do_sample\": True,        # 다양한 출력을 위해 샘플링 사용\n",
    "    \"num_return_sequences\": 1, # 생성할 결과 수\n",
    "    \"top_k\": 50,              # top-k sampling\n",
    "    \"repetition_penalty\": 1.2, # 반복 방지\n",
    "    \"no_repeat_ngram_size\": 3  # n-gram 반복 방지\n",
    "}\n",
    "\n",
    "# 텍스트 생성\n",
    "outputs = model.generate(**model_inputs, **generation_config)\n",
    "\n",
    "# 결과 디코딩\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "# 원본과 교정본 비교 출력\n",
    "print(\"=== 원본 ===\")\n",
    "print(text)\n",
    "print(\"\\n=== 교정본 ===\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "\n",
    "yt = YouTube(url)\n",
    "audio_stream = yt.streams.filter(only_audio=True).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# model = WhisperModel(\n",
    "#     \"large-v3\", device=\"cuda\", compute_type=\"bfloat16\"\n",
    "# )\n",
    "# model = BatchedInferencePipeline(model=model)  # 배치 모델일 경우\n",
    "# print(\"Whisper 모델 초기화 완료\")\n",
    "\n",
    "\n",
    "# segments, info = model.transcribe(\n",
    "#     audio_stream.url,\n",
    "#     batch_size=64,  # 배치 모델인 경우\n",
    "#     repetition_penalty=1.5,\n",
    "#     beam_size=10,\n",
    "#     patience=2,\n",
    "#     no_repeat_ngram_size=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import math\n",
    "import requests\n",
    "import tempfile\n",
    "import os\n",
    "import concurrent.futures\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import threading\n",
    "import ffmpeg\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, total_size, desc=\"Downloading\"):\n",
    "        pbar = tqdm(total=total_size, unit=\"iB\", unit_scale=True, desc=desc)\n",
    "        lock = threading.Lock()\n",
    "\n",
    "    def update(self, size):\n",
    "        with lock:\n",
    "            pbar.update(size)\n",
    "\n",
    "    def close(self):\n",
    "        pbar.close()\n",
    "\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.1,\n",
    "        status_forcelist=[500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(\n",
    "        max_retries=retry,\n",
    "        pool_connections=100,\n",
    "        pool_maxsize=100\n",
    "    )\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def download_chunk(args):\n",
    "    url, start, end, chunk_number, temp_dir, progress_bar = args\n",
    "    \n",
    "    headers = {\"Range\": f\"bytes={start}-{end}\"}\n",
    "    session = create_session()\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, headers=headers, stream=True)\n",
    "        chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_number:04d}\")\n",
    "        \n",
    "        with open(chunk_path, \"wb\") as f:\n",
    "            for data in response.iter_content(chunk_size=8192):\n",
    "                size = f.write(data)\n",
    "                progress_bar.update(size)\n",
    "        \n",
    "        return chunk_path, chunk_number\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading chunk {chunk_number}: {str(e)}\")\n",
    "        return None, chunk_number\n",
    "\n",
    "def parallel_download(url, temp_dir, num_chunks=10):\n",
    "    session = create_session()\n",
    "    response = session.head(url)\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "    \n",
    "    if total_size == 0:\n",
    "        raise ValueError(\"Could not determine file size\")\n",
    "    \n",
    "    chunk_size = total_size // num_chunks\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size - 1 if i < num_chunks - 1 else total_size - 1\n",
    "        chunks.append((start, end))\n",
    "    \n",
    "    progress_bar = ProgressBar(total_size, \"Parallel downloading\")\n",
    "    \n",
    "    download_args = [\n",
    "        (url, start, end, i, temp_dir, progress_bar)\n",
    "        for i, (start, end) in enumerate(chunks)\n",
    "    ]\n",
    "    \n",
    "    chunk_paths = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_chunks) as executor:\n",
    "        futures = executor.map(download_chunk, download_args)\n",
    "        chunk_paths = [(path, num) for path, num in futures if path is not None]\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    chunk_paths.sort(key=lambda x: x[1])\n",
    "    output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "    \n",
    "    with open(output_path, \"wb\") as outfile:\n",
    "        for chunk_path, _ in chunk_paths:\n",
    "            with open(chunk_path, \"rb\") as infile:\n",
    "                outfile.write(infile.read())\n",
    "            os.remove(chunk_path)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def convert_to_wav(input_path, output_path):\n",
    "    \"\"\"MP4를 WAV로 변환\"\"\"\n",
    "    try:\n",
    "        stream = ffmpeg.input(input_path)\n",
    "        stream = ffmpeg.output(stream, output_path, \n",
    "                             acodec=\"pcm_s16le\", \n",
    "                             ar=\"16000\",\n",
    "                             ac=\"1\")\n",
    "        ffmpeg.run(stream, capture_stdout=True, capture_stderr=True)\n",
    "        return True\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"FFmpeg error:\", e.stderr.decode())\n",
    "        return False\n",
    "\n",
    "def process_audio_chunk(chunk_data):\n",
    "    \"\"\"개별 오디오 청크 처리\"\"\"\n",
    "    model, audio_path, start_time, duration = chunk_data\n",
    "    try:\n",
    "        segments, info = model.transcribe(\n",
    "            audio_path,\n",
    "            beam_size=5,\n",
    "            batch_size=32,\n",
    "            word_timestamps=True,\n",
    "            initial_prompt=None\n",
    "        )\n",
    "        \n",
    "        # segments를 리스트로 변환하고 시간 조정\n",
    "        chunk_segments = []\n",
    "        for segment in segments:\n",
    "            segment_dict = {\n",
    "                \"start\": segment.start + start_time,\n",
    "                \"end\": segment.end + start_time,\n",
    "                \"text\": segment.text,\n",
    "                \"words\": [\n",
    "                    {\n",
    "                        \"start\": word.start + start_time,\n",
    "                        \"end\": word.end + start_time,\n",
    "                        \"word\": word.word,\n",
    "                        \"probability\": word.probability\n",
    "                    }\n",
    "                    for word in segment.words\n",
    "                ]\n",
    "            }\n",
    "            chunk_segments.append(segment_dict)\n",
    "        \n",
    "        return chunk_segments\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk at {start_time}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def process_with_progress(url, model, chunk_duration=30, num_download_chunks=10):\n",
    "    \"\"\"\n",
    "    전체 처리 프로세스 관리\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        print(\"Starting parallel download...\")\n",
    "        mp4_path = parallel_download(url, temp_dir, num_download_chunks)\n",
    "        print(\"Download complete!\")\n",
    "        \n",
    "        # MP4를 WAV로 변환\n",
    "        wav_path = os.path.join(temp_dir, \"audio.wav\")\n",
    "        if not convert_to_wav(mp4_path, wav_path):\n",
    "            raise Exception(\"Failed to convert audio to WAV format\")\n",
    "        \n",
    "        # WAV 파일 정보 읽기\n",
    "        wav_info = sf.info(wav_path)\n",
    "        total_duration = wav_info.duration\n",
    "        \n",
    "        # 청크 계산\n",
    "        total_chunks = math.ceil(total_duration / chunk_duration)\n",
    "        \n",
    "        # 진행률 표시\n",
    "        pbar = tqdm(total=total_chunks, desc=\"Processing audio chunks\")\n",
    "        \n",
    "        # 청크 처리를 위한 데이터 준비\n",
    "        chunks_data = []\n",
    "        for i in range(total_chunks):\n",
    "            start_time = i * chunk_duration\n",
    "            chunk_wav_path = os.path.join(temp_dir, f\"chunk_{i}.wav\")\n",
    "            \n",
    "            # 청크 추출\n",
    "            duration = min(chunk_duration, total_duration - start_time)\n",
    "            stream = ffmpeg.input(wav_path, ss=start_time, t=duration)\n",
    "            stream = ffmpeg.output(stream, chunk_wav_path, \n",
    "                                 acodec=\"pcm_s16le\", \n",
    "                                 ar=\"16000\",\n",
    "                                 ac=\"1\")\n",
    "            ffmpeg.run(stream, quiet=True)\n",
    "            \n",
    "            chunks_data.append((model, chunk_wav_path, start_time, duration))\n",
    "        \n",
    "        # 청크 처리 및 결과 수집\n",
    "        all_segments = []\n",
    "        for chunk_data in chunks_data:\n",
    "            segments = process_audio_chunk(chunk_data)\n",
    "            all_segments.extend(segments)\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # 사용한 청크 파일 삭제\n",
    "            if os.path.exists(chunk_data[1]):\n",
    "                os.remove(chunk_data[1])\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "    return all_segments\n",
    "\n",
    "# 모델 초기화\n",
    "model = WhisperModel(\n",
    "    \"large-v3\", \n",
    "    device=\"cuda\", \n",
    "    compute_type=\"float16\"  # bfloat16 대신 float16 사용\n",
    ")\n",
    "print(\"Whisper 모델 초기화 완료\")\n",
    "model = BatchedInferencePipeline(model=model)\n",
    "\n",
    "# 트랜스크립션 실행\n",
    "segments = process_with_progress(\n",
    "    audio_stream.url,\n",
    "    model,\n",
    "    chunk_duration=30,\n",
    "    num_download_chunks=10\n",
    ")\n",
    "\n",
    "# 결과 저장\n",
    "for i, segment in enumerate(segments):\n",
    "    print(f\"{segment[\"start\"]:.2f} -> {segment[\"end\"]:.2f}: {segment[\"text\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://youtu.be/AA621UofTUA?si=gn4XutRMWUDSYLFL\"\n",
    "\n",
    "# from faster_whisper import WhisperModel\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# import tempfile\n",
    "# import os\n",
    "# import ffmpeg\n",
    "# import subprocess\n",
    "# from yt_dlp import YoutubeDL\n",
    "# import io\n",
    "\n",
    "# def get_audio_stream(url):\n",
    "#     \"\"\"URL에서 오디오 스트림 정보를 가져옵니다.\"\"\"\n",
    "#     ydl_opts = {\n",
    "#         \"format\": \"bestaudio/best\",\n",
    "#         \"quiet\": True,\n",
    "#         \"no_warnings\": True,\n",
    "#         \"extract_audio\": True\n",
    "#     }\n",
    "    \n",
    "#     with YoutubeDL(ydl_opts) as ydl:\n",
    "#         info = ydl.extract_info(url, download=False)\n",
    "#         audio_url = info[\"url\"]\n",
    "#         duration = info.get(\"duration\", 0)\n",
    "        \n",
    "#         return audio_url, duration\n",
    "\n",
    "# def process_stream_with_progress(url, model, chunk_duration=30):\n",
    "#     \"\"\"\n",
    "#     스트리밍 방식으로 오디오를 처리합니다.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - url: 오디오 URL\n",
    "#     - model: WhisperModel 인스턴스\n",
    "#     - chunk_duration: 각 청크의 길이(초)\n",
    "#     \"\"\"\n",
    "#     # 스트림 URL 가져오기\n",
    "#     audio_url, total_duration = get_audio_stream(url)\n",
    "    \n",
    "#     # ffmpeg 명령어 설정\n",
    "#     ffmpeg_cmd = [\n",
    "#         \"ffmpeg\",\n",
    "#         \"-i\", audio_url,\n",
    "#         \"-f\", \"wav\",\n",
    "#         \"-ar\", \"16000\",\n",
    "#         \"-ac\", \"1\",\n",
    "#         \"-hide_banner\",\n",
    "#         \"-loglevel\", \"error\",\n",
    "#         \"pipe:1\"\n",
    "#     ]\n",
    "    \n",
    "#     # 진행률 표시 설정\n",
    "#     total_chunks = int(np.ceil(total_duration / chunk_duration))\n",
    "#     pbar = tqdm(total=total_chunks, desc=\"Processing audio chunks\")\n",
    "    \n",
    "#     # 결과 저장용 리스트\n",
    "#     all_segments = []\n",
    "    \n",
    "#     try:\n",
    "#         # ffmpeg 프로세스 시작\n",
    "#         process = subprocess.Popen(\n",
    "#             ffmpeg_cmd,\n",
    "#             stdout=subprocess.PIPE,\n",
    "#             bufsize=10**8  # 버퍼 크기 설정\n",
    "#         )\n",
    "        \n",
    "#         # 임시 디렉토리 생성\n",
    "#         with tempfile.TemporaryDirectory() as temp_dir:\n",
    "#             chunk_size = int(16000 * chunk_duration * 2)  # 16000Hz * seconds * 2 bytes per sample\n",
    "#             chunk_number = 0\n",
    "            \n",
    "#             while True:\n",
    "#                 # 청크 읽기\n",
    "#                 audio_chunk = process.stdout.read(chunk_size)\n",
    "#                 if not audio_chunk:\n",
    "#                     break\n",
    "                \n",
    "#                 # 청크를 임시 파일로 저장\n",
    "#                 chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_number}.wav\")\n",
    "#                 with open(chunk_path, \"wb\") as f:\n",
    "#                     # WAV 헤더 작성\n",
    "#                     f.write(b\"RIFF\")\n",
    "#                     f.write((chunk_size + 36).to_bytes(4, \"little\"))\n",
    "#                     f.write(b\"WAVE\")\n",
    "#                     f.write(b\"fmt \")\n",
    "#                     f.write((16).to_bytes(4, \"little\"))\n",
    "#                     f.write((1).to_bytes(2, \"little\"))  # PCM\n",
    "#                     f.write((1).to_bytes(2, \"little\"))  # Mono\n",
    "#                     f.write((16000).to_bytes(4, \"little\"))  # Sample rate\n",
    "#                     f.write((32000).to_bytes(4, \"little\"))  # Byte rate\n",
    "#                     f.write((2).to_bytes(2, \"little\"))  # Block align\n",
    "#                     f.write((16).to_bytes(2, \"little\"))  # Bits per sample\n",
    "#                     f.write(b\"data\")\n",
    "#                     f.write(len(audio_chunk).to_bytes(4, \"little\"))\n",
    "#                     f.write(audio_chunk)\n",
    "                \n",
    "#                 try:\n",
    "#                     # 청크 처리\n",
    "#                     segments, _ = model.transcribe(\n",
    "#                         chunk_path,\n",
    "#                         beam_size=5,\n",
    "#                         batch_size=32,\n",
    "#                         word_timestamps=True,\n",
    "#                         condition_on_previous_text=True\n",
    "#                     )\n",
    "                    \n",
    "#                     # 시간 오프셋 조정 및 세그먼트 저장\n",
    "#                     time_offset = chunk_number * chunk_duration\n",
    "#                     for segment in segments:\n",
    "#                         segment_dict = {\n",
    "#                             \"start\": segment.start + time_offset,\n",
    "#                             \"end\": segment.end + time_offset,\n",
    "#                             \"text\": segment.text,\n",
    "#                             \"words\": [\n",
    "#                                 {\n",
    "#                                     \"start\": word.start + time_offset,\n",
    "#                                     \"end\": word.end + time_offset,\n",
    "#                                     \"word\": word.word,\n",
    "#                                     \"probability\": word.probability\n",
    "#                                 }\n",
    "#                                 for word in segment.words\n",
    "#                             ]\n",
    "#                         }\n",
    "#                         all_segments.append(segment_dict)\n",
    "                \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing chunk {chunk_number}: {str(e)}\")\n",
    "                \n",
    "#                 finally:\n",
    "#                     # 임시 파일 삭제\n",
    "#                     if os.path.exists(chunk_path):\n",
    "#                         os.remove(chunk_path)\n",
    "                \n",
    "#                 # 진행률 업데이트\n",
    "#                 pbar.update(1)\n",
    "#                 chunk_number += 1\n",
    "    \n",
    "#     finally:\n",
    "#         pbar.close()\n",
    "#         if process.poll() is None:\n",
    "#             process.terminate()\n",
    "#             process.wait()\n",
    "    \n",
    "#     return all_segments\n",
    "\n",
    "# # 모델 초기화\n",
    "# model = WhisperModel(\n",
    "#     \"large-v3\", \n",
    "#     device=\"cuda\", \n",
    "#     compute_type=\"float16\"\n",
    "# )\n",
    "# print(\"Whisper 모델 초기화 완료\")\n",
    "\n",
    "# # 트랜스크립션 실행\n",
    "# segments = process_stream_with_progress(\n",
    "#     url,  # 유튜브 URL\n",
    "#     model,\n",
    "#     chunk_duration=30\n",
    "# )\n",
    "\n",
    "# # 결과 출력\n",
    "# for segment in segments:\n",
    "#     print(f\"{segment[\"start\"]:.2f} -> {segment[\"end\"]:.2f}: {segment[\"text\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"script.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "            Document(page_content=\"\\n\".join([t[\"text\"] for t in data]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def calculate_tokens(text, model=\"gpt-4o-mini\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_tokens(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, chunk_overlap=500\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(split_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import hub\n",
    "summary_prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7, streaming=True)\n",
    "summary_chain = create_stuff_documents_chain(llm, summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumaries = []\n",
    "for split_doc in split_docs:\n",
    "    print(type(split_doc.page_content))\n",
    "    partial_summary = summary_chain.invoke({\"context\": [split_doc]})\n",
    "    sumaries.append(partial_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_summary = Document(page_content= \"\\n\".join(sumaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_RESULT = summary_chain.invoke(\n",
    "                {\"context\": partial_summaries_doc}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(SUMMARY_RESULT.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_qa_with_sources_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pytubefix import YouTube\n",
    "import asyncio\n",
    "import torch\n",
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # .env 파일에서 환경 변수 로드\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(url):\n",
    "    yt = YouTube(url)\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    return {\n",
    "        \"title\": yt.title,\n",
    "        \"audio_url\": audio_stream.url if audio_stream else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://www.youtube.com/shorts/a--NSC19MXM\"\n",
    "video_info = get_video_info(video_url)\n",
    "print(f\"Video Title: {video_info[\"title\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = WhisperModel(\"large-v3\", device=device, compute_type=compute_type)\n",
    "\n",
    "def transcribe_audio(audio_url):\n",
    "    segments, info = whisper_model.transcribe(audio_url)\n",
    "    transcript = [{\"text\": segment.text, \"start\": segment.start, \"end\": segment.end} for segment in segments]\n",
    "    return {\"script\": transcript, \"language\": info.language}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = transcribe_audio(video_info[\"audio_url\"])\n",
    "print(f\"Transcript Language: {transcript[\"language\"]}\")\n",
    "print(f\"First few lines of transcript: {transcript[\"script\"][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "summary_prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            streaming=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader, TextLoader\n",
    "docs = TextLoader(\"script.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "document = [Document(page_content=\"\\n\".join([t[\"text\"] for t in transcript[\"script\"]]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = create_stuff_documents_chain(llm,summary_prompt)\n",
    "result = await summary_chain.ainvoke({\"context\": document})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.create_documents([t[\"text\"] for t in transcript[\"script\"]])\n",
    "for doc in documents:\n",
    "    doc.page_content += \"\\n\" + summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "you_url = \"https://youtube.com/shorts/a--NSC19MXM?si=yiun-HK_7wX1sNvL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = \"https://api.runpod.ai/v2/uq96boxkzy99ev/runsync\"\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터 (내부적으로 사용할 파라미터 설정)\n",
    "body = {\"input\":{\n",
    "    \"api\":{\n",
    "        \"method\":\"POST\",\n",
    "        \"endpoint\":\"/ping\",\n",
    "    },\n",
    "    \"payload\":{},\n",
    "}}\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=body, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    print(\"Response:\", response.json())\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 작업 ID (작업 완료된 job ID)\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "# RunPod API STATUS 엔드포인트 URL\n",
    "status_url = f\"https://api.runpod.ai/v2/wm1xrz07all039/status/{job_id}\"\n",
    "\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# 작업 상태 및 결과 확인 요청 보내기\n",
    "response = requests.get(status_url, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    job_result = response.json()\n",
    "    if job_result.get(\"status\") == \"COMPLETED\":\n",
    "        print(\"Job Completed! Result:\", job_result.get(\"output\"))\n",
    "    else:\n",
    "        print(f\"Job Status: {job_result.get(\"status\")}\")\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "you_url = \"https://youtu.be/omEk2BNDt1I?si=xjtbYANtlux5CTfB\"\n",
    "\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/get_title_hash\",\n",
    "        \"method\": \"GET\",\n",
    "        # \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"url\": you_url},\n",
    "    }\n",
    "}\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    print(\"Response:\", response.json())\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "# endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# # RunPod RUNSYNC 엔드포인트 URL\n",
    "# url = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "\n",
    "# # FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "# payload = {\n",
    "#     \"input\": {\n",
    "#         \"endpoint\": \"/get_script_summary\",\n",
    "#         \"method\": \"GET\",\n",
    "#         \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "#         \"params\": {\"url\": you_url},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # 요청 헤더에 API 키 추가\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {api_key}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # RUNSYNC 요청 보내기\n",
    "# response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# # 응답 확인\n",
    "# if response.status_code == 200:\n",
    "#     print(\"Response:\", response.json())\n",
    "# else:\n",
    "#     print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/run\"\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/get_script_summary\",\n",
    "        \"method\": \"GET\",\n",
    "        \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"url\": you_url},\n",
    "    }\n",
    "}\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Initial Response:\", result)\n",
    "    \n",
    "    if result.get(\"status\") in [\"IN_PROGRESS\",\"IN_QUEUE\"]:\n",
    "        job_id = result.get(\"id\")\n",
    "        status_url = f\"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}\"\n",
    "        \n",
    "        while True:\n",
    "            status_response = requests.get(status_url, headers=headers)\n",
    "            if status_response.status_code == 200:\n",
    "                status_data = status_response.json()\n",
    "                print(f\"Current status: {status_data.get(\"status\")}\")\n",
    "                \n",
    "                if status_data.get(\"status\") == \"COMPLETED\":\n",
    "                    print(f\"결과값:{status_data}\")\n",
    "                    result_url = f\"https://api.runpod.ai/v2/{endpoint_id}/result/{job_id}\"\n",
    "                    result_response = requests.get(result_url, headers=headers)\n",
    "                    \n",
    "                    if result_response.status_code == 200:\n",
    "                        final_result = result_response.json()\n",
    "                        print(\"Final Result:\", final_result)\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Error fetching results: {result_response.status_code}\")\n",
    "                        print(f\"Error message: {result_response.text}\")\n",
    "                        break\n",
    "                elif status_data.get(\"status\") == \"FAILED\":\n",
    "                    print(\"Job failed\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Error checking status: {status_response.status_code}\")\n",
    "                print(f\"Error message: {status_response.text}\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)  # 5초 대기 후 다시 상태 확인\n",
    "    else:\n",
    "        print(\"Job completed immediately\")\n",
    "        print(\"Final Result:\", result)\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}\"\n",
    "url2 = f\"https://api.runpod.ai/v2/{endpoint_id}/result/{job_id}\"\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "while True:\n",
    "    # RUNSYNC 요청 보내기\n",
    "    response = requests.get(url,headers=headers)\n",
    "\n",
    "    # 응답 확인\n",
    "    if response.status_code == 200:\n",
    "        if response.json().get(\"status\") == \"COMPLETED\":\n",
    "            response = requests.get(url2,headers=headers)\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Job status: {response.json().get(\"status\")}\")\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "RUNPOD_API_URL = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/rag_stream_chat\",\n",
    "        \"method\": \"POST\",\n",
    "        \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"prompt\": \"영상의 주제가 뭔가요?\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    RUNPOD_API_URL, headers=headers, json=payload, stream=True\n",
    ")\n",
    "\n",
    "# for chunk in response.iter_content(chunk_size=None):\n",
    "#     if chunk:\n",
    "#         chunk_data = chunk.decode(\"utf-8\").strip()\n",
    "#         if chunk_data.startswith(\"data: \"):\n",
    "#             chunk_content = chunk_data[6:]\n",
    "#             if chunk_content == \"[DONE]\":\n",
    "#                 break\n",
    "#             try:\n",
    "#                 content = json.loads(chunk_content)\n",
    "#                 print(\"Stream content:\", content)\n",
    "#             except json.JSONDecodeError:\n",
    "#                 print(\"Invalid JSON:\", chunk_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"\"\n",
    "for chunk in response.json().get(\"output\"):\n",
    "    answer += chunk.get(\"content\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"script.txt\",\"r\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "url = \"https://www.youtube.com/watch?v=yF_YIxxjWU4\"\n",
    "yt = YouTube(url, use_po_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1RPJ223J7MY4YHK5UDRZ5F05GBFX7H9C4R8R5RW\n",
    "S1RPJ223J7MY4YHK5UDRZ5FO5GBFX7H9C4R8R5RW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
