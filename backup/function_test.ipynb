{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://youtube.com/shorts/a--NSC19MXM?si=0o-IP4UwUZMPmd_6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def x(url):\n",
    "    api_endpoint = \"https://api.runpod.ai/v2/b6wkrofoagngld/run/extract_info\"\n",
    "    response = requests.get(api_endpoint, params={\"url\": url})\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 5 (char 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.10/site-packages/requests/models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.10/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 5 (char 4)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://youtube.com/shorts/a--NSC19MXM?si=yiun-HK_7wX1sNvL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m\u001b[43mx\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mx\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      4\u001b[0m api_endpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.runpod.ai/v2/b6wkrofoagngld/run/extract_info\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(api_endpoint, params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/youtube/lib/python3.10/site-packages/requests/models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 5 (char 4)"
     ]
    }
   ],
   "source": [
    "url = \"https://youtube.com/shorts/a--NSC19MXM?si=yiun-HK_7wX1sNvL\"\n",
    "response =x(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 404 - 404 page not found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.runpod.ai/v2/b6wkrofoagngld/run/\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"path\": \"/get_title_hash\",\n",
    "    \"parameters\": {\n",
    "        \"url\": \"https://youtube.com/shorts/a--NSC19MXM?si=yiun-HK_7wX1sNvL\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 401 - \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.runpod.ai/v2/b6wkrofoagngld/run\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"input\": {\n",
    "        \"path\": \"/youtube_get\",\n",
    "        \"parameters\": {\n",
    "            \"url\": \"https://youtube.com/shorts/a--NSC19MXM?si=yiun-HK_7wX1sNvL\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행시간: 2.1851508617401123 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "result = get_script(\"https://youtube.com/shorts/31aKBb7ioZ8?si=BDLcpYFOuqLOYvvc\")\n",
    "print(f\"실행시간: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '여성 리포터 옆으로 딜런 데닉스가 오는데요.', 'start': 0.0, 'end': 2.38},\n",
       " {'text': '리포터는 기발한 생각이 떠올랐습니다.', 'start': 2.74, 'end': 5.34},\n",
       " {'text': '다른 매니저도 말리지만 좋은 클립이 될 거라면서 계속 해달라고 합니다.',\n",
       "  'start': 9.96,\n",
       "  'end': 14.88},\n",
       " {'text': '결국에는 들어가는데', 'start': 15.8, 'end': 17.04},\n",
       " {'text': '표정이 점점 이상해지다가 그대로 가버리죠.', 'start': 17.04, 'end': 22.56},\n",
       " {'text': '여자는 믿을 수 없다는 눈치입니다. 본인도 사실 인지를 못하거든요. 언제 가는지.',\n",
       "  'start': 30.0,\n",
       "  'end': 36.06},\n",
       " {'text': '리포터는 고맙다며 껴안아줍니다. 어이가 없네요.', 'start': 36.28, 'end': 39.1},\n",
       " {'text': '이렇게 경험을 못해봐서 선수들에게 많이 해달라고들 하는데', 'start': 39.5, 'end': 43.06},\n",
       " {'text': '머릿속에 기억이 끊기는 느낌이 상당히 이상하죠.', 'start': 43.06, 'end': 46.14}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "if os.path.exists('script.json'):\n",
    "    os.remove('script.json')\n",
    "\n",
    "with open('script.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(result['script'], json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain API Key가 설정되지 않았습니다. 참고: https://wikidocs.net/250954\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "load_dotenv()  # .env 파일에서 환경 변수를 로드합니다\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Youtube_Script_Chatbot\"\n",
    "logging.langsmith(\"Youtube_Script_Chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please summarize the sentence according to the following REQUEST.\n",
      "REQUEST:\n",
      "1. Summarize the main points in bullet points in KOREAN.\n",
      "2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\n",
      "3. Use various emojis to make the summary more interesting.\n",
      "4. Translate the summary into KOREAN if it is written in ENGLISH.\n",
      "5. DO NOT translate any technical terms.\n",
      "6. DO NOT include any unnecessary information.\n",
      "7. Please refer to each summary and indicate the key topic.\n",
      "\n",
      "CONTEXT:\n",
      "\u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "\n",
      "SUMMARY:\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_teddynote.callbacks import StreamingCallback\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "summary_prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "summary_prompt.template = 'Please summarize the sentence according to the following REQUEST.\\nREQUEST:\\n1. Summarize the main points in bullet points in KOREAN.\\n2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\\n3. Use various emojis to make the summary more interesting.\\n4. Translate the summary into KOREAN if it is written in ENGLISH.\\n5. DO NOT translate any technical terms.\\n6. DO NOT include any unnecessary information.\\n7. Please refer to each summary and indicate the key topic.\\n\\nCONTEXT:\\n{context}\\n\\nSUMMARY:\"\\n'\n",
    "summary_prompt.pretty_print()\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    streaming=True,\n",
    "    temperature=0.7,\n",
    "    callbacks=[StreamingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseChatModel.stream at 0x7493c86548e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.stream(\"메롱\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 어떻게 도와드릴까요?"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"안녕하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 어떻게 도와드릴까요?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader,TextLoader\n",
    "\n",
    "# JSONLoader로 임시 파일 로드\n",
    "# jq_schema = '.[] | {content: .text, start_time: .start, end_time: .end}'\n",
    "jq_schema = \".[].text\"\n",
    "docs = JSONLoader(file_path=\"script.json\", jq_schema=jq_schema,text_content=False).load()\n",
    "\n",
    "# # 각 Document의 page_content를 디코딩하여 한글로 변환\n",
    "# for doc in docs:\n",
    "#     # page_content를 JSON 객체로 변환 후 문자열 디코딩\n",
    "#     decoded_content = json.loads(doc.page_content) \n",
    "#     doc.page_content = decoded_content # JSON 문자열 -> Python dict 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/home/jinu/my_ws/youtube_script_chatbot/script.json', 'seq_num': 1}, page_content='In the last chapter, you and I started to step through the internal workings of a transformer.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"script.txt\",\"w\") as file:\n",
    "    for text in [doc.page_content for doc in docs]:\n",
    "        file.write(text+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = TextLoader(\"script.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27899"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 주요 주제: Transformer 및 Attention 메커니즘\n",
      "\n",
      "- 🧠 Transformer는 대형 언어 모델의 핵심 기술이다.  \n",
      "- 📄 2017년 논문 \"Attention is All You Need\"에서 처음 소개되었다.  \n",
      "- 🔍 입력 텍스트는 토큰으로 나누어지고, 각 토큰은 고차원 벡터인 임베딩과 연결된다.  \n",
      "- 🔄 Transformer의 목표는 임베딩을 조정하여 개별 단어의 의미를 넘어 더 풍부한 맥락적 의미를 반영하는 것이다.  \n",
      "- ⚖️ Attention 메커니즘은 특정 문맥에서 단어의 의미를 업데이트하도록 돕는다.  \n",
      "- 🔄 \"fluffy blue creature\"와 같은 문장에서 형용사가 명사를 어떻게 조정하는지를 설명한다.  \n",
      "- 📊 Query, Key, Value 행렬을 사용하여 단어 간의 상관관계를 측정하고 업데이트한다.  \n",
      "- 🔒 마스킹을 통해 이후의 단어가 이전 단어에 영향을 미치지 않도록 한다.  \n",
      "- 📈 Multi-headed attention은 여러 개의 Attention 헤드를 사용하여 다양한 맥락을 반영한다.  \n",
      "- 🔍 GPT-3는 96개의 Attention 헤드를 사용하여 매번 다양한 방식으로 의미를 업데이트한다.  \n",
      "- 💻 Attention 메커니즘은 병렬 처리 가능성이 높아 효율적인 계산을 가능하게 한다.  \n",
      "- 📊 전체 모델의 매개변수 수는 58억 개에 달하며, 이는 모델의 성능을 높이는 데 기여한다.  \n",
      "- 🔗 향후 장에서는 다른 블록과 훈련 과정에 대해 다룰 예정이다.  \n",
      "\n",
      "이 요약은 Transformer 및 Attention 메커니즘의 핵심 개념을 설명하며, 각 문장은 관련된 이모지로 시작하여 내용을 더욱 흥미롭게 표현하고 있습니다."
     ]
    }
   ],
   "source": [
    "stuff_chain = create_stuff_documents_chain(llm, summary_prompt)\n",
    "summary_result = stuff_chain.invoke({\"context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"REQUEST:\n",
    "1. Separate the entire content based on the main topics and group sentences into logical paragraphs.\n",
    "2. Each paragraph should be structured as a key-value pair in a dictionary. \n",
    "3. The keys should be \"paragraph1\", \"paragraph2\", and so on. The values should be the corresponding paragraph text.\n",
    "4. Each sentence in the paragraph should be separated by a newline character.\n",
    "5. Do not distort the original content under any circumstances.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "RESULT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_separate_template = PromptTemplate(input_variables=['context'],input_types={},partial_variables={},metadata={'lc_hub_owner':\"jinu\",\"lc_hub_repo\":None,\"lc_hub_commit_hash\":None},template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "{\n",
      "    \"paragraph1\": \"In the last chapter, you and I started to step through the internal workings of a transformer.\\nThis is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.\\nIt first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter, you and I will dig into what this attention mechanism is, visualizing how it processes data.\\nAs a quick recap, here's the important context I want you to have in mind.\",\n",
      "    \n",
      "    \"paragraph2\": \"The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.\\nThe input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.\\nThe first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.\",\n",
      "    \n",
      "    \"paragraph3\": \"Now the most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.\\nIn the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.\\nJust one example, you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.\\nThe aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.\",\n",
      "    \n",
      "    \"paragraph4\": \"I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.\\nI think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.\\nConsider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.\",\n",
      "    \n",
      "    \"paragraph5\": \"You and I know that the word mole has different meanings in each one of these, based on the context.\\nBut after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all three of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.\\nIt's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.\",\n",
      "    \n",
      "    \"paragraph6\": \"The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these more specific directions, as a function of the context.\\nTo take another example, consider the embedding of the word tower.\\nThis is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.\",\n",
      "    \n",
      "    \"paragraph7\": \"If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel Tower, maybe correlated with vectors associated with Paris and France and things made of steel.\\nIf it was also preceded by the word miniature, then the vector should be updated even further so that it no longer correlates with large tall things.\\nMore generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.\",\n",
      "    \n",
      "    \"paragraph8\": \"What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation that you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.\\nSo imagine, for example, that the text you input is most of an entire mystery novel, way up to a point near the end which reads, therefore the murderer was, if the model is going to accurately predict the next word, that final vector in the sequence which began its life simply embedding the word was will have to have been updated by all of the attention blocks to represent much much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.\",\n",
      "    \n",
      "    \"paragraph9\": \"To step through the computations though let's take a much simpler example.\\nImagine that the input includes the phrase a fluffy blue creature roamed the verdant forest and for the moment suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.\\nWhat I'm about to describe is what we would call a single head of attention and later we will see how the attention block consists of many different heads run in parallel.\",\n",
      "    \n",
      "    \"paragraph10\": \"Again, the initial embedding for each word is some high-dimensional vector that only encodes the meaning of that particular word with no context.\\nActually, that's not quite true.\\nThey also encode the position of the word.\\nThere's a lot more to say about the specific way that positions are encoded, but right now all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.\",\n",
      "    \n",
      "    \"paragraph11\": \"Let's go ahead and denote these embeddings with the letter E, the goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.\\nAnd playing the deep learning game, we want most of the computations involved to look like matrix-vector products where the matrices are full of tunable weights, things that the model will learn based on data.\",\n",
      "    \n",
      "    \"paragraph12\": \"To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an intention had doing.\\nAs with so much deep learning, the true behavior is much harder to parse, because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.\\nIt's just that as we step through all of the different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.\",\n",
      "    \n",
      "    \"paragraph13\": \"For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me, and for the words fluffy and blue to each be able to answer, yeah, I'm an adjective and I'm in that position.\\nThat question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.\\nThis query vector, though, has a much smaller dimension than the embedding vector, say 128.\",\n",
      "    \n",
      "    \"paragraph14\": \"Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.\\nCompressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.\\nIn this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.\",\n",
      "    \n",
      "    \"paragraph15\": \"The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice what this matrix does in a particular attention head is challenging to parse.\\nBut for our sake, imagining an example that we might hope it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.\",\n",
      "    \n",
      "    \"paragraph16\": \"As to what it does to other embeddings, who knows, maybe it simultaneously tries to accomplish some other goal with those, right now we're laser focused on the nouns.\\nAt the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.\\nThis produces a second sequence of vectors that we call the keys.\",\n",
      "    \n",
      "    \"paragraph17\": \"Conceptually you want to think of the keys as potentially answering the queries.\\nThis key matrix is also full of tunable parameters, and just like the query matrix it maps the embedding vectors to that same smaller dimensional space.\\nYou think of the keys as matching the queries whenever they closely align with each other.\",\n",
      "    \n",
      "    \"paragraph18\": \"In our example, you would imagine that the key matrix maps the adjectives, like fluffy and blue, to vectors that are closely aligned with the query produced by the word creature.\\nTo measure how well each key matches each query, you compute a dot product between each possible key-query pair.\\nI like to visualize a grid full of a bunch of dots, where the bigger dots correspond the larger dot products, the places where the keys and queries align.\",\n",
      "    \n",
      "    \"paragraph19\": \"For our adjective-noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.\\nIn the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.\\nBy contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that these are unrelated to each other.\",\n",
      "    \n",
      "    \"paragraph20\": \"So we have this grid of values that can be any real number from negative infinity to infinity giving us a score for how relevant each word is to updating the meaning of every other word.\\nThe way we're about to use these scores is to take a certain weighted sum along each column weighted by the relevance.\\nSo instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.\",\n",
      "    \n",
      "    \"paragraph21\": \"If you're coming in from the last chapter, you know what we need to do then.\\nWe compute a softmax along each one of these columns to normalize the values.\\nIn our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.\",\n",
      "    \n",
      "    \"paragraph22\": \"At this point, you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.\\nWe call this grid an attention pattern.\\nNow, if you look at the original Transformer paper, there's a really compact way that they write this all down.\",\n",
      "    \n",
      "    \"paragraph23\": \"Here, the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.\\nThis expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.\\nA small technical detail that I didn't mention is that for numerical stability it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.\",\n",
      "    \n",
      "    \"paragraph24\": \"Then this softmax that's wrapped around the full expression, is meant to be understood to apply column by column.\\nAs to that V term, we'll talk about it in just a second.\\nBefore that, there's one other technical detail that so far I've skipped.\",\n",
      "    \n",
      "    \"paragraph25\": \"During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial sub-sequence of tokens in this passage.\\nFor example, with the phrase that we've been focusing on, it might also be predicting what words follow creature, and what words follow the.\",\n",
      "    \n",
      "    \"paragraph26\": \"This is really nice, because it means what would otherwise be a single training example effectively acts as many.\\nFor the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.\\nWhat this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.\",\n",
      "    \n",
      "    \"paragraph27\": \"The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.\\nSo instead a common way to do this is that before applying softmax you set all of those entries to be negative infinity.\\nIf you do that then after applying softmax, all of those get turned into zero, but the columns stay normalized.\",\n",
      "    \n",
      "    \"paragraph28\": \"This process is called masking.\\nThere are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.\\nAnother fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.\",\n",
      "    \n",
      "    \"paragraph29\": \"So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.\\nAs you might imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable.\\nBut right here, you and I are staying focused on the basics.\",\n",
      "    \n",
      "    \"paragraph30\": \"Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.\\nNow you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.\\nFor example, you want the embedding of fluffy to somehow cause a change to creature that moves it to a different part of this 12,000 dimensional embedding space that more specifically encodes a fluffy creature.\",\n",
      "    \n",
      "    \"paragraph31\": \"What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.\\nThis most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example fluffy.\\nThe result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of creature.\",\n",
      "    \n",
      "    \"paragraph32\": \"So, this value vector lives in the same very high dimensional space as the embeddings.\\nWhen you multiply this value matrix by the embedding of a word, you might think of it as saying if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?\\nLooking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.\",\n",
      "    \n",
      "    \"paragraph33\": \"You might think of these value vectors as being kind of associated with the corresponding keys.\\nFor each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.\\nFor example, here, under the embedding of creature, you would be adding large proportions of the value vectors for fluffy and blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.\",\n",
      "    \n",
      "    \"paragraph34\": \"And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of creature, you add together all of these rescaled values in the column, producing a change that you want to add that I'll label delta E, and then you add that to the original embedding.\\nHopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.\\nAnd of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes.\",\n",
      "    \n",
      "    \"paragraph35\": \"Adding all of those changes to the corresponding embeddings produces a full sequence of more refined embeddings popping out of the attention block.\\nZooming out, this whole process is what you would describe as a single head of attention.\\nAs I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.\",\n",
      "    \n",
      "    \"paragraph36\": \"I want to take a moment to continue what we started in the last chapter with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.\\nThese key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.\\nThis gives us an additional 1.5 million or so parameters for each one.\",\n",
      "    \n",
      "    \"paragraph37\": \"If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and its outputs live in this very large embedding space.\\nIf true, that would mean about 150 million added parameters.\\nAnd to be clear, you could do that, you could devote orders of magnitude more parameters to the value map than to the key and query.\",\n",
      "    \n",
      "    \"paragraph38\": \"But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key in the query.\\nThis is especially relevant in the setting of running multiple attention heads in parallel.\\nThe way this looks is that the value map is factored as a product of two smaller matrices.\",\n",
      "    \n",
      "    \"paragraph39\": \"Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.\\nIt's just that it's broken up into two separate steps.\\nThe first matrix on the right here has a smaller number of rows, typically the same size as the key query space.\",\n",
      "    \n",
      "    \"paragraph40\": \"What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.\\nThis is not the conventional naming, but I'm going to call this the value down matrix.\\nThe second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.\",\n",
      "    \n",
      "    \"paragraph41\": \"I'm going to call this one the value-up matrix, which, again, is not conventional.\\nThe way that you would see this written in most papers looks a little different.\\nI'll talk about it in a minute.\",\n",
      "    \n",
      "    \"paragraph42\": \"In my opinion, it tends to make things a little more conceptually confusing.\\nTo throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low-rank transformation.\\nTurning back to the parameter count, all four of these matrices have the same size, and then adding them all up, we get about 6.3 million parameters for one attention head.\",\n",
      "    \n",
      "    \"paragraph43\": \"As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.\\nThis isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation.\",\n",
      "    \n",
      "    \"paragraph44\": \"Or maybe audio input of speech, and an ongoing transcription.\\nA cross-attention head looks almost identical.\\nThe only difference is that the key and query maps act on different datasets.\",\n",
      "    \n",
      "    \"paragraph45\": \"In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.\\nAnd in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.\\nStaying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.\",\n",
      "    \n",
      "    \"paragraph46\": \"All that's really left to us is to lay out the sense in which you do this many, many different times.\\nIn our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.\\nIf the words they crashed the preceded the word car, it has implications for the shape and the structure of that car, and a lot of associations might be less grammatical.\",\n",
      "    \n",
      "    \"paragraph47\": \"If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.\\nFor every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.\",\n",
      "    \n",
      "    \"paragraph48\": \"And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.\\nAs I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention where you run a lot of these operations in parallel each with its own distinct key query and value maps.\\nGPT-3 for example uses 96 attention heads inside each block.\",\n",
      "    \n",
      "    \"paragraph49\": \"Considering that each one is already a bit confusing it's certainly a lot to hold in your head.\\nJust to spell it all out very explicitly this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.\\nThen each head has its own distinct value matrices used to produce 96 sequences of value vectors.\",\n",
      "    \n",
      "    \"paragraph50\": \"These are all added together using the corresponding attention patterns as weights.\\nWhat this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.\\nSo what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.\",\n",
      "    \n",
      "    \"paragraph51\": \"This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.\\nAgain, this is a lot to think about, so don't worry at all if it takes some time to sink in.\\nThe overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.\",\n",
      "    \n",
      "    \"paragraph52\": \"Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.\\nThere's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.\\nYou remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.\",\n",
      "    \n",
      "    \"paragraph53\": \"The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.\\nThat would be a valid design.\\nBut the way that you see this written in papers and the way that it's implemented in practice looks a little different.\",\n",
      "    \n",
      "    \"paragraph54\": \"All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.\\nAnd when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.\\nFor the curious among you, I've left an on-screen note about it.\",\n",
      "    \n",
      "    \"paragraph55\": \"It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.\\nSetting aside all the technical nuances, in the preview from the last chapter, we saw how data flowing through a transformer doesn't just flow through a single attention block.\\nFor one thing, it also goes through these other operations called multi-layer perceptrons.\",\n",
      "    \n",
      "    \"paragraph56\": \"We'll talk more about those in the next chapter.\\nAnd then it repeatedly goes through many, many copies of both of these operations.\\nWhat this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.\",\n",
      "    \n",
      "    \"paragraph57\": \"The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.\\nThings like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece, and things like that.\",\n",
      "    \n",
      "    \"paragraph58\": \"Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key, query, and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.\\nThat is a lot, to be sure, but it's only about a third of the 175 billion that are in the network in total.\\nSo even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.\",\n",
      "    \n",
      "    \"paragraph59\": \"In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.\\nA big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.\\nThat one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance.\",\n",
      "    \n",
      "    \"paragraph60\": \"There's a huge advantage to parallelizable architectures that let you do this.\\nIf you want to learn more about this stuff, I've left lots of links in the description.\\nIn particular, anything produced by Andre Karpathy or Chris Ola tend to be pure gold.\",\n",
      "    \n",
      "    \"paragraph61\": \"In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.\\nAlso, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.\"\n",
      "}\n",
      "```"
     ]
    }
   ],
   "source": [
    "stuff_chain = create_stuff_documents_chain(llm, paragraph_separate_template)\n",
    "paragraph_separate_result = stuff_chain.invoke({\"context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph1': \"In the last chapter, you and I started to step through the internal workings of a transformer.\\nThis is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI.\\nIt first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter, you and I will dig into what this attention mechanism is, visualizing how it processes data.\\nAs a quick recap, here's the important context I want you to have in mind.\",\n",
       " 'paragraph2': \"The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next.\\nThe input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words.\\nThe first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding.\",\n",
       " 'paragraph3': \"Now the most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning.\\nIn the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun.\\nJust one example, you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning.\\nThe aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning.\",\n",
       " 'paragraph4': \"I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in.\\nI think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable.\\nConsider the phrases American true mole, one mole of carbon dioxide, and take a biopsy of the mole.\",\n",
       " 'paragraph5': \"You and I know that the word mole has different meanings in each one of these, based on the context.\\nBut after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all three of these cases, because this initial token embedding is effectively a lookup table with no reference to the context.\\nIt's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one.\",\n",
       " 'paragraph6': 'The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these more specific directions, as a function of the context.\\nTo take another example, consider the embedding of the word tower.\\nThis is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns.',\n",
       " 'paragraph7': \"If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel Tower, maybe correlated with vectors associated with Paris and France and things made of steel.\\nIf it was also preceded by the word miniature, then the vector should be updated even further so that it no longer correlates with large tall things.\\nMore generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word.\",\n",
       " 'paragraph8': \"What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation that you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence.\\nSo imagine, for example, that the text you input is most of an entire mystery novel, way up to a point near the end which reads, therefore the murderer was, if the model is going to accurately predict the next word, that final vector in the sequence which began its life simply embedding the word was will have to have been updated by all of the attention blocks to represent much much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word.\",\n",
       " 'paragraph9': \"To step through the computations though let's take a much simpler example.\\nImagine that the input includes the phrase a fluffy blue creature roamed the verdant forest and for the moment suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns.\\nWhat I'm about to describe is what we would call a single head of attention and later we will see how the attention block consists of many different heads run in parallel.\",\n",
       " 'paragraph10': \"Again, the initial embedding for each word is some high-dimensional vector that only encodes the meaning of that particular word with no context.\\nActually, that's not quite true.\\nThey also encode the position of the word.\\nThere's a lot more to say about the specific way that positions are encoded, but right now all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context.\",\n",
       " 'paragraph11': \"Let's go ahead and denote these embeddings with the letter E, the goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives.\\nAnd playing the deep learning game, we want most of the computations involved to look like matrix-vector products where the matrices are full of tunable weights, things that the model will learn based on data.\",\n",
       " 'paragraph12': \"To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an intention had doing.\\nAs with so much deep learning, the true behavior is much harder to parse, because it's based on tweaking and tuning a huge number of parameters to minimize some cost function.\\nIt's just that as we step through all of the different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete.\",\n",
       " 'paragraph13': \"For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me, and for the words fluffy and blue to each be able to answer, yeah, I'm an adjective and I'm in that position.\\nThat question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word.\\nThis query vector, though, has a much smaller dimension than the embedding vector, say 128.\",\n",
       " 'paragraph14': \"Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding.\\nCompressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end.\\nIn this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token.\",\n",
       " 'paragraph15': \"The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice what this matrix does in a particular attention head is challenging to parse.\\nBut for our sake, imagining an example that we might hope it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions.\",\n",
       " 'paragraph16': \"As to what it does to other embeddings, who knows, maybe it simultaneously tries to accomplish some other goal with those, right now we're laser focused on the nouns.\\nAt the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings.\\nThis produces a second sequence of vectors that we call the keys.\",\n",
       " 'paragraph17': 'Conceptually you want to think of the keys as potentially answering the queries.\\nThis key matrix is also full of tunable parameters, and just like the query matrix it maps the embedding vectors to that same smaller dimensional space.\\nYou think of the keys as matching the queries whenever they closely align with each other.',\n",
       " 'paragraph18': 'In our example, you would imagine that the key matrix maps the adjectives, like fluffy and blue, to vectors that are closely aligned with the query produced by the word creature.\\nTo measure how well each key matches each query, you compute a dot product between each possible key-query pair.\\nI like to visualize a grid full of a bunch of dots, where the bigger dots correspond the larger dot products, the places where the keys and queries align.',\n",
       " 'paragraph19': 'For our adjective-noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers.\\nIn the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature.\\nBy contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that these are unrelated to each other.',\n",
       " 'paragraph20': \"So we have this grid of values that can be any real number from negative infinity to infinity giving us a score for how relevant each word is to updating the meaning of every other word.\\nThe way we're about to use these scores is to take a certain weighted sum along each column weighted by the relevance.\\nSo instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution.\",\n",
       " 'paragraph21': \"If you're coming in from the last chapter, you know what we need to do then.\\nWe compute a softmax along each one of these columns to normalize the values.\\nIn our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values.\",\n",
       " 'paragraph22': \"At this point, you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top.\\nWe call this grid an attention pattern.\\nNow, if you look at the original Transformer paper, there's a really compact way that they write this all down.\",\n",
       " 'paragraph23': \"Here, the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices.\\nThis expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries.\\nA small technical detail that I didn't mention is that for numerical stability it happens to be helpful to divide all of these values by the square root of the dimension in that key query space.\",\n",
       " 'paragraph24': \"Then this softmax that's wrapped around the full expression, is meant to be understood to apply column by column.\\nAs to that V term, we'll talk about it in just a second.\\nBefore that, there's one other technical detail that so far I've skipped.\",\n",
       " 'paragraph25': \"During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial sub-sequence of tokens in this passage.\\nFor example, with the phrase that we've been focusing on, it might also be predicting what words follow creature, and what words follow the.\",\n",
       " 'paragraph26': 'This is really nice, because it means what would otherwise be a single training example effectively acts as many.\\nFor the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next.\\nWhat this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero.',\n",
       " 'paragraph27': \"The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized.\\nSo instead a common way to do this is that before applying softmax you set all of those entries to be negative infinity.\\nIf you do that then after applying softmax, all of those get turned into zero, but the columns stay normalized.\",\n",
       " 'paragraph28': \"This process is called masking.\\nThere are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones.\\nAnother fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size.\",\n",
       " 'paragraph29': 'So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial.\\nAs you might imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable.\\nBut right here, you and I are staying focused on the basics.',\n",
       " 'paragraph30': \"Okay, great, computing this pattern lets the model deduce which words are relevant to which other words.\\nNow you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to.\\nFor example, you want the embedding of fluffy to somehow cause a change to creature that moves it to a different part of this 12,000 dimensional embedding space that more specifically encodes a fluffy creature.\",\n",
       " 'paragraph31': \"What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention.\\nThis most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example fluffy.\\nThe result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of creature.\",\n",
       " 'paragraph32': \"So, this value vector lives in the same very high dimensional space as the embeddings.\\nWhen you multiply this value matrix by the embedding of a word, you might think of it as saying if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this?\\nLooking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors.\",\n",
       " 'paragraph33': 'You might think of these value vectors as being kind of associated with the corresponding keys.\\nFor each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column.\\nFor example, here, under the embedding of creature, you would be adding large proportions of the value vectors for fluffy and blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out.',\n",
       " 'paragraph34': \"And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of creature, you add together all of these rescaled values in the column, producing a change that you want to add that I'll label delta E, and then you add that to the original embedding.\\nHopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature.\\nAnd of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes.\",\n",
       " 'paragraph35': \"Adding all of those changes to the corresponding embeddings produces a full sequence of more refined embeddings popping out of the attention block.\\nZooming out, this whole process is what you would describe as a single head of attention.\\nAs I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value.\",\n",
       " 'paragraph36': 'I want to take a moment to continue what we started in the last chapter with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3.\\nThese key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space.\\nThis gives us an additional 1.5 million or so parameters for each one.',\n",
       " 'paragraph37': \"If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and its outputs live in this very large embedding space.\\nIf true, that would mean about 150 million added parameters.\\nAnd to be clear, you could do that, you could devote orders of magnitude more parameters to the value map than to the key and query.\",\n",
       " 'paragraph38': 'But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key in the query.\\nThis is especially relevant in the setting of running multiple attention heads in parallel.\\nThe way this looks is that the value map is factored as a product of two smaller matrices.',\n",
       " 'paragraph39': \"Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns.\\nIt's just that it's broken up into two separate steps.\\nThe first matrix on the right here has a smaller number of rows, typically the same size as the key query space.\",\n",
       " 'paragraph40': \"What this means is you can think of it as mapping the large embedding vectors down to a much smaller space.\\nThis is not the conventional naming, but I'm going to call this the value down matrix.\\nThe second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates.\",\n",
       " 'paragraph41': \"I'm going to call this one the value-up matrix, which, again, is not conventional.\\nThe way that you would see this written in most papers looks a little different.\\nI'll talk about it in a minute.\",\n",
       " 'paragraph42': \"In my opinion, it tends to make things a little more conceptually confusing.\\nTo throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low-rank transformation.\\nTurning back to the parameter count, all four of these matrices have the same size, and then adding them all up, we get about 6.3 million parameters for one attention head.\",\n",
       " 'paragraph43': \"As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention.\\nThis isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation.\",\n",
       " 'paragraph44': 'Or maybe audio input of speech, and an ongoing transcription.\\nA cross-attention head looks almost identical.\\nThe only difference is that the key and query maps act on different datasets.',\n",
       " 'paragraph45': \"In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another.\\nAnd in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones.\\nStaying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is.\",\n",
       " 'paragraph46': \"All that's really left to us is to lay out the sense in which you do this many, many different times.\\nIn our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word.\\nIf the words they crashed the preceded the word car, it has implications for the shape and the structure of that car, and a lot of associations might be less grammatical.\",\n",
       " 'paragraph47': 'If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince.\\nFor every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings.',\n",
       " 'paragraph48': \"And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token.\\nAs I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention where you run a lot of these operations in parallel each with its own distinct key query and value maps.\\nGPT-3 for example uses 96 attention heads inside each block.\",\n",
       " 'paragraph49': \"Considering that each one is already a bit confusing it's certainly a lot to hold in your head.\\nJust to spell it all out very explicitly this means you have 96 distinct key and query matrices producing 96 distinct attention patterns.\\nThen each head has its own distinct value matrices used to produce 96 sequences of value vectors.\",\n",
       " 'paragraph50': 'These are all added together using the corresponding attention patterns as weights.\\nWhat this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position.\\nSo what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position.',\n",
       " 'paragraph51': \"This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it.\\nAgain, this is a lot to think about, so don't worry at all if it takes some time to sink in.\\nThe overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning.\",\n",
       " 'paragraph52': \"Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters.\\nThere's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers.\\nYou remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices.\",\n",
       " 'paragraph53': \"The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way.\\nThat would be a valid design.\\nBut the way that you see this written in papers and the way that it's implemented in practice looks a little different.\",\n",
       " 'paragraph54': \"All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block.\\nAnd when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space.\\nFor the curious among you, I've left an on-screen note about it.\",\n",
       " 'paragraph55': \"It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources.\\nSetting aside all the technical nuances, in the preview from the last chapter, we saw how data flowing through a transformer doesn't just flow through a single attention block.\\nFor one thing, it also goes through these other operations called multi-layer perceptrons.\",\n",
       " 'paragraph56': \"We'll talk more about those in the next chapter.\\nAnd then it repeatedly goes through many, many copies of both of these operations.\\nWhat this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings.\",\n",
       " 'paragraph57': \"The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure.\\nThings like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece, and things like that.\",\n",
       " 'paragraph58': \"Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key, query, and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads.\\nThat is a lot, to be sure, but it's only about a third of the 175 billion that are in the network in total.\\nSo even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps.\",\n",
       " 'paragraph59': \"In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process.\\nA big part of the story for the success of the attention mechanism is not so much any specific kind of behavior that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs.\\nThat one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance.\",\n",
       " 'paragraph60': \"There's a huge advantage to parallelizable architectures that let you do this.\\nIf you want to learn more about this stuff, I've left lots of links in the description.\\nIn particular, anything produced by Andre Karpathy or Chris Ola tend to be pure gold.\",\n",
       " 'paragraph61': \"In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation.\\nAlso, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "text_data = paragraph_separate_result[paragraph_separate_result.find(\"{\"):paragraph_separate_result.find(\"}\")+1]\n",
    "parsed_data = ast.literal_eval(text_data)\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "tl = GoogleTranslator(source='en', target='ko')\n",
    "translated_data = {}\n",
    "for k,v in parsed_data.items():\n",
    "    translated_data[k] = tl.translate(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난 장에서 당신과 저는 트랜스포머의 내부 작동을 살펴보았습니다.\n",
      "이것은 대규모 언어 모델 내부의 핵심 기술 중 하나이며, 현대 AI의 많은 다른 도구입니다.\n",
      "이것은 현재 유명해진 2017년 논문인 Attention is All You Need에서 처음 등장했으며, 이 장에서 당신과 저는 이 어텐션 메커니즘이 무엇인지, 그리고 그것이 데이터를 처리하는 방식을 시각화하는 것을 파헤쳐 볼 것입니다.\n",
      "간단히 요약하자면, 제가 당신이 염두에 두어야 할 중요한 맥락은 다음과 같습니다.\n"
     ]
    }
   ],
   "source": [
    "print(translated_data['paragraph1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in summary_result.strip().split(\"\\n\"):\n",
    "    # 줄이 비어 있지 않다면 Document로 생성하고, 요약 내용임을 metadata로 추가\n",
    "    if line.strip():\n",
    "        docs[0].page_content += line+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "split_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_retriever = vec_store.as_retriever( search_kwargs={\"k\": 10} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        bm25_retriever,\n",
    "        vec_retriever,\n",
    "    ],\n",
    "    weights=[0.7, 0.3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "chat_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"당신은 유튜브 스크립트 기반의 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. \n",
    "당신의 임무는 주어진 영상의 텍스트 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\n",
    "검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. \n",
    "만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\n",
    "한글로 답변해 주세요. 단, 기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요.\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = PromptTemplate.from_template(\n",
    "\"\"\"당신은 유튜브 스크립트 기반의 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. \n",
    "당신의 임무는 주어진 영상의 텍스트 문맥(context)과 내부 지식을 활용하여 주어진 질문(question)에 답하는 것입니다.\n",
    "\n",
    "1. 검색된 다음 문맥(context)을 사용하여 질문(question)에 답하세요. \n",
    "2. 영상과 관련 없는 질문일 경우 \"영상과 관계 없는 질문 입니다.\" 라고 답하세요.\n",
    "3. 만약, 주어진 문맥(context)에서 답을 찾을 수 없다면, 내부 지식(internal knowledge)을 사용하여 답변을 생성하세요.\n",
    "4. 만약 문맥에서 답을 찾을 수 없고 내부 지식으로도 답변할 수 없다면, `주어진 정보에서 질문에 대한 답변을 찾을 수 없습니다`라고 답하세요.\n",
    "\n",
    "한글로 답변해 주세요. 단, 기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요.\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 8: 체인(Chain) 생성\n",
    "chain = (\n",
    "    {\"context\": ensemble_retriever, \"question\": RunnablePassthrough()}\n",
    "    | chat_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이이 영상 영상은은 Transformer Transformer와와 Attention Attention 메 메커커니니즘즘에에 대한 대한 핵 핵심심 개 개념념을을 설명 설명하고하고 있습니다 있습니다.. Transformer Transformer는는 대 대형형 언 언어어 모델 모델의의 핵 핵심심 기술 기술이며이며,, Attention Attention 메 메커커니니즘즘은은 단 단어어의의 의미 의미를를 문 문맥맥에에 맞 맞게게 업데이트 업데이트하는하는 데 데 도움 도움을을 줍 줍니다니다.. 영상 영상에서는에서는 이러한 이러한 기술 기술들이들이 어떻게 어떻게 작 작동동하는하는지를지를 다양한 다양한 예 예시시와와 함께 함께 흥 흥미미롭게롭게 설명 설명하고하고 있습니다 있습니다.."
     ]
    }
   ],
   "source": [
    "answer = chain.stream(\"어떤얘기인가요??\")\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Input'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[RunnableConfig]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Any]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'AsyncIterator[Output]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Default implementation of astream, which calls ainvoke.\n",
      "Subclasses should override this method if they support streaming output.\n",
      "\n",
      "Args:\n",
      "    input: The input to the Runnable.\n",
      "    config: The config to use for the Runnable. Defaults to None.\n",
      "    kwargs: Additional keyword arguments to pass to the Runnable.\n",
      "\n",
      "Yields:\n",
      "    The output of the Runnable.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/youtubee/lib/python3.11/site-packages/langchain_core/runnables/base.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "chain.astream?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 요약은 Transformer의 기본 작동 원리와 Attention 메커니즘의 중요성을 설명합니다.\n",
      "the larger dot products, the places where the keys and queries align. For our adjective-noun\n",
      "that would look a little more like this, where if the keys produced by fluffy and blue really do\n",
      "align closely with the query produced by creature, then the dot products in these two spots would\n",
      "would be\n",
      "some large positive numbers. In the lingo, machine learning people would say that this means the\n",
      "embeddings of fluffy and blue attend to the embedding of creature. By contrast to the dot\n",
      "product between the key for some other word like the and the query for creature would be some small\n",
      "or negative value that reflects that these are unrelated to each other. So we have this grid of\n",
      "values that can be any real number from negative infinity to infinity giving us a score for how\n",
      "- 🎲 **소프트맥스**: 점수 분포를 정규화하여 각 단어의 관련성을 계산함.  \n",
      "- 🎭 **마스킹**: 후속 단어가 이전 단어에 영향을 미치지 않도록 마스킹을 적용함.\n",
      "- ⚖️ **의미의 방향**: 고차원 공간에서 방향이 의미와 대응할 수 있음.\n",
      "- 🔑 **쿼리 및 키 매트릭스**: 쿼리와 키 매트릭스는 서로의 관련성을 측정하기 위해 사용됨.\n",
      "- 🌌 **임베딩**: 각 토큰은 고차원 벡터와 연관되어 있으며, 이 임베딩은 의미를 반영함.\n",
      "- 📄 **첫 등장**: 이 기술은 2017년의 논문 \"Attention is All You Need\"에서 처음 소개됨.\n",
      "- 🔍 **모델의 목표**: 모델은 주어진 텍스트를 기반으로 다음에 올 단어를 예측하는 것임.\n",
      "- ✨ **예시**: \"mole\"이라는 단어는 다양한 맥락에서 다르게 해석되며, Attention은 이를 반영함.\n",
      "you\n",
      "### 주요 주제: Transformer의 내부 작동 원리\n",
      "- 🏰 **단어 임베딩 업데이트**: 예를 들어, \"tower\"라는 단어는 문맥에 따라 업데이트됨.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "소프트맥스 함수는 점수 분포를 정규화하여 각 단어의 관련성을 계산하는 데 사용됩니다. 이는 머신러닝에서 주어진 입력에 대해 확률적인 출력을 얻기 위해 일반적으로 사용되는 방법입니다."
     ]
    }
   ],
   "source": [
    "query = \"소프트맥스 함수가 뭔가요?\"\n",
    "ret_docs = ensemble_retriever.invoke(\n",
    "    query)\n",
    "for doc in ret_docs:\n",
    "    print(doc.page_content)\n",
    "print(\"-\"*100)\n",
    "response = chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
