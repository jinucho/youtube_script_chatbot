{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_qa_with_sources_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pytubefix import YouTube\n",
    "import asyncio\n",
    "import torch\n",
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # .env 파일에서 환경 변수 로드\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(url):\n",
    "    yt = YouTube(url)\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    return {\n",
    "        \"title\": yt.title,\n",
    "        \"audio_url\": audio_stream.url if audio_stream else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Title: 가장 쉬운 까르보나라\n"
     ]
    }
   ],
   "source": [
    "video_url = \"https://www.youtube.com/shorts/a--NSC19MXM\"\n",
    "video_info = get_video_info(video_url)\n",
    "print(f\"Video Title: {video_info['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = WhisperModel(\"large-v3\", device=device, compute_type=compute_type)\n",
    "\n",
    "def transcribe_audio(audio_url):\n",
    "    segments, info = whisper_model.transcribe(audio_url)\n",
    "    transcript = [{\"text\": segment.text, \"start\": segment.start, \"end\": segment.end} for segment in segments]\n",
    "    return {\"script\": transcript, \"language\": info.language}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "transcribe.py       :324  2024-10-12 11:30:14,287 Processing audio with duration 00:59.118\n",
      "transcribe.py       :425  2024-10-12 11:30:19,512 Detected language 'ko' with probability 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript Language: ko\n",
      "First few lines of transcript: [{'text': ' 한 남자가 베이컨을 가져오는데요', 'start': 0.0, 'end': 10.46}, {'text': ' 갑자기 계란을 가져와 깨부수기 시작합니다', 'start': 10.46, 'end': 12.74}, {'text': ' 노른자를 건져내 따로 담아줍니다', 'start': 12.74, 'end': 14.74}]\n"
     ]
    }
   ],
   "source": [
    "transcript = transcribe_audio(video_info['audio_url'])\n",
    "print(f\"Transcript Language: {transcript['language']}\")\n",
    "print(f\"First few lines of transcript: {transcript['script'][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/0f7nfvt16ln8630csjtkk_1w0000gn/T/ipykernel_41602/2953908480.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "summary_prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            streaming=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader, TextLoader\n",
    "docs = TextLoader(\"script.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'script.txt'}, page_content=\"In the last chapter, you and I started to step through the internal workings of a transformer.\\nThis is one of the key pieces of technology inside large language models, and a lot of\\nother tools in the modern wave of AI.\\nIt first hit the scene in a now-famous 2017 paper called Attention is All You Need, and\\nin this chapter, you and I will dig into what this attention mechanism is, visualizing how\\nit processes data.\\nAs a quick recap, here's the important context I want you to have in mind.\\nThe goal of the model that you and I are studying is to take in a piece of text and predict\\nwhat word comes next.\\nThe input text is broken up into little pieces that we call tokens, and these are very often\\nwords or pieces of words, but just to make the examples in this video easier for you\\nand me to think about, let's simplify by pretending that tokens are always just words.\\nThe first step in a transformer is to associate each token with a high-dimensional vector,\\nwhat we call its embedding.\\nNow the most important idea I want you to have in mind is how directions in this high-dimensional\\nspace of all possible embeddings can correspond with semantic meaning.\\nIn the last chapter we saw an example for how direction can correspond to gender, in\\nthe sense that adding a certain step in this space can take you from the embedding of a\\nmasculine noun to the embedding of the corresponding feminine noun.\\njust one example, you could imagine how many other directions in this high-dimensional space\\ncould correspond to numerous other aspects of a word's meaning. The aim of a transformer is to\\nprogressively adjust these embeddings so that they don't merely encode an individual word,\\nbut instead they bake in some much, much richer contextual meaning. I should say up front that a\\nlot of people find the attention mechanism, this key piece in a transformer, very confusing, so\\ndon't worry if it takes some time for things to sink in. I think that before we dive into the\\ncomputational details and all the matrix multiplications, it's worth thinking about a\\ncouple examples for the kind of behavior that we want attention to enable. Consider the phrases\\nAmerican true mole, one mole of carbon dioxide, and take a biopsy of the mole. You and I know\\nthat the word mole has different meanings in each one of these, based on the context.\\nBut after the first step of a transformer, the one that breaks up the text and associates each\\ntoken with a vector, the vector that's associated with mole would be the same in all three of these\\ncases, because this initial token embedding is effectively a lookup table with no reference to\\nthe context. It's only in the next step of the transformer that the surrounding embeddings have\\nthe chance to pass information into this one. The picture you might have in mind is that there\\nare multiple distinct directions in this embedding space encoding the multiple distinct meanings of\\nthe word mole, and that a well-trained attention block calculates what you need to add to the\\ngeneric embedding to move it to one of these more specific directions, as a function of the context.\\nTo take another example, consider the embedding of the word tower. This is presumably some very\\ngeneric, non-specific direction in the space, associated with lots of other large, tall nouns.\\nIf this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update\\nthis vector so that it points in a direction that more specifically encodes the Eiffel Tower,\\nmaybe correlated with vectors associated with Paris and France and things made of steel.\\nIf it was also preceded by the word miniature, then the vector should be updated even further\\nso that it no longer correlates with large tall things. More generally than just refining the\\nmeaning of a word, the attention block allows the model to move information encoded in one\\nembedding to that of another, potentially ones that are quite far away, and potentially\\nwith information that's much richer than just a single word.\\nWhat we saw in the last chapter was how after all of the vectors flow through the network,\\nincluding many different attention blocks, the computation that you perform to produce\\na prediction of the next token is entirely a function of the last vector in the sequence.\\nSo imagine, for example, that the text you input is most of an entire mystery novel,\\nway up to a point near the end which reads, therefore the murderer was, if the model is\\ngoing to accurately predict the next word, that final vector in the sequence which began its life\\nsimply embedding the word was will have to have been updated by all of the attention blocks\\nto represent much much more than any individual word, somehow encoding all of the information\\nfrom the full context window that's relevant to predicting the next word. To step through the\\nthe computations though let's take a much simpler example. Imagine that the input includes the\\nphrase a fluffy blue creature roamed the verdant forest and for the moment suppose that the only\\ntype of update that we care about is having the adjectives adjust the meanings of their\\ncorresponding nouns. What I'm about to describe is what we would call a single head of attention\\nand later we will see how the attention block consists of many different heads run in parallel.\\nAgain, the initial embedding for each word is some high-dimensional vector\\nthat only encodes the meaning of that particular word with no context.\\nActually, that's not quite true. They also encode the position of the word.\\nThere's a lot more to say about the specific way that positions are encoded,\\nbut right now all you need to know is that the entries of this vector are enough to tell you\\nboth what the word is and where it exists in the context. Let's go ahead and denote these\\nembeddings with the letter E, the goal is to have a series of computations produce a\\nnew refined set of embeddings where, for example, those corresponding to the nouns have ingested\\nthe meaning from their corresponding adjectives.\\nAnd playing the deep learning game, we want most of the computations involved to look\\nlike matrix-vector products where the matrices are full of tunable weights, things that the\\nmodel will learn based on data.\\nTo be clear, I'm making up this example of adjectives updating nouns just to illustrate\\nthe type of behavior that you could imagine an intention had doing.\\nAs with so much deep learning, the true behavior is much harder to parse, because it's based\\non tweaking and tuning a huge number of parameters to minimize some cost function.\\nIt's just that as we step through all of the different matrices filled with parameters\\nthat are involved in this process, I think it's really helpful to have an imagined example\\nof something that it could be doing to help keep it all more concrete.\\nFor the first step of this process, you might imagine each noun, like creature, asking the\\nquestion, hey, are there any adjectives sitting in front of me, and for the words fluffy and\\nblue to each be able to answer, yeah, I'm an adjective and I'm in that position.\\nThat question is somehow encoded as yet another vector, another list of numbers, which we\\ncall the query for this word.\\nThis query vector, though, has a much smaller dimension than the embedding vector, say 128.\\nComputing this query looks like taking a certain matrix, which I'll label wq, and multiplying\\nit by the embedding.\\nCompressing things a bit, let's write that query vector as q, and then anytime you see\\nme put a matrix next to an arrow like this one, it's meant to represent that multiplying\\nthis matrix by the vector at the arrow's start gives you the vector at the arrow's end.\\nIn this case, you multiply this matrix by all of the embeddings in the context, producing\\none query vector for each token.\\nThe entries of this matrix are parameters of the model, which means the true behavior\\nis learned from data, and in practice what this matrix does in a particular attention\\nhead is challenging to parse.\\nBut for our sake, imagining an example that we might hope it would learn, we'll suppose\\nthat this query matrix maps the embeddings of nouns to certain directions in this smaller\\nquery space that somehow encodes the notion of looking for adjectives in preceding positions.\\nAs to what it does to other embeddings, who knows, maybe it simultaneously tries to accomplish\\nsome other goal with those, right now we're laser focused on the nouns.\\nAt the same time, associated with this is a second matrix called the key matrix, which\\nyou also multiply by every one of the embeddings.\\nThis produces a second sequence of vectors that we call the keys.\\nConceptually you want to think of the keys as potentially answering the queries.\\nThis key matrix is also full of tunable parameters, and just like the query matrix it maps the\\nembedding vectors to that same smaller dimensional space.\\nYou think of the keys as matching the queries whenever they closely align with each other.\\nIn our example, you would imagine that the key matrix maps the adjectives, like fluffy\\nand blue, to vectors that are closely aligned with the query produced by the word creature.\\nTo measure how well each key matches each query, you compute a dot product between each\\npossible key-query pair.\\nI like to visualize a grid full of a bunch of dots, where the bigger dots correspond\\nthe larger dot products, the places where the keys and queries align. For our adjective-noun example,\\nthat would look a little more like this, where if the keys produced by fluffy and blue really do\\nalign closely with the query produced by creature, then the dot products in these two spots would be\\nsome large positive numbers. In the lingo, machine learning people would say that this means the\\nembeddings of fluffy and blue attend to the embedding of creature. By contrast to the dot\\nproduct between the key for some other word like the and the query for creature would be some small\\nor negative value that reflects that these are unrelated to each other. So we have this grid of\\nvalues that can be any real number from negative infinity to infinity giving us a score for how\\nrelevant each word is to updating the meaning of every other word. The way we're about to use these\\nscores is to take a certain weighted sum along each column weighted by the relevance. So instead\\nInstead of having values range from negative infinity to infinity, what we want is for\\nthe numbers in these columns to be between 0 and 1, and for each column to add up to\\n1, as if they were a probability distribution.\\nIf you're coming in from the last chapter, you know what we need to do then.\\nWe compute a softmax along each one of these columns to normalize the values.\\nIn our picture, after you apply softmax to all of the columns, we'll fill in the grid\\nwith these normalized values.\\nAt this point, you're safe to think about each column as giving weights\\naccording to how relevant the word on the left is to the corresponding value at the top.\\nWe call this grid an attention pattern.\\nNow, if you look at the original Transformer paper,\\nthere's a really compact way that they write this all down.\\nHere, the variables q and k represent the full arrays of query and key vectors respectively,\\nthose little vectors you get by multiplying the embeddings by the query and the key matrices.\\nThis expression up in the numerator is a really compact way to represent the grid of all possible\\ndot products between pairs of keys and queries. A small technical detail that I didn't mention\\nis that for numerical stability it happens to be helpful to divide all of these values by the\\nsquare root of the dimension in that key query space. Then this softmax that's wrapped around\\nthe full expression, is meant to be understood to apply column by column.\\nAs to that V term, we'll talk about it in just a second.\\nBefore that, there's one other technical detail that so far I've skipped.\\nDuring the training process, when you run this model on a given text example, and all\\nof the weights are slightly adjusted and tuned to either reward or punish it based on how\\nhigh a probability it assigns to the true next word in the passage, it turns out to\\nmake the whole training process a lot more efficient if you simultaneously have it predict\\nevery possible next token following each initial sub-sequence of tokens in this passage.\\nFor example, with the phrase that we've been focusing on, it might also be predicting what\\nwords follow creature, and what words follow the.\\nThis is really nice, because it means what would otherwise be a single training example\\neffectively acts as many.\\nFor the purposes of our attention pattern, it means that you never want to allow later\\nwords to influence earlier words, since otherwise they could kind of give away the answer for\\nwhat comes next. What this means is that we want all of these spots here, the ones representing\\nlater tokens influencing earlier ones, to somehow be forced to be zero. The simplest thing you might\\nthink to do is to set them equal to zero, but if you did that the columns wouldn't add up to one\\nanymore, they wouldn't be normalized. So instead a common way to do this is that before applying\\nsoftmax you set all of those entries to be negative infinity. If you do that then after\\nAfter applying softmax, all of those get turned into zero, but the columns stay normalized.\\nThis process is called masking.\\nThere are versions of attention where you don't apply it, but in our GPT example, even\\nthough this is more relevant during the training phase than it would be, say, running it as\\na chatbot or something like that, you do always apply this masking to prevent later tokens\\nfrom influencing earlier ones.\\nAnother fact that's worth reflecting on about this attention pattern is how its size is\\nequal to the square of the context size.\\nSo this is why context size can be a really huge bottleneck for large language models,\\nand scaling it up is non-trivial.\\nAs you might imagine, motivated by a desire for bigger and bigger context windows, recent\\nyears have seen some variations to the attention mechanism aimed at making context more scalable.\\nBut right here, you and I are staying focused on the basics.\\nOkay, great, computing this pattern lets the model deduce which words are relevant to which\\nother words.\\nNow you need to actually update the embeddings, allowing words to pass information to whichever\\nother words they're relevant to.\\nFor example, you want the embedding of fluffy to somehow cause a change to creature that\\nmoves it to a different part of this 12,000 dimensional embedding space that more specifically\\nencodes a fluffy creature.\\nWhat I'm going to do here is first show you the most straightforward way that you could\\ndo this, though there's a slight way that this gets modified in the context of multi-headed\\nattention.\\nThis most straightforward way would be to use a third matrix, what we call the value\\nmatrix, which you multiply by the embedding of that first word, for example fluffy.\\nThe result of this is what you would call a value vector, and this is something that\\nyou add to the embedding of the second word, in this case something you add to the embedding\\nof creature.\\nSo, this value vector lives in the same very high dimensional space as the embeddings.\\nWhen you multiply this value matrix by the embedding of a word, you might think of it\\nas saying if this word is relevant to adjusting the meaning of something else, what exactly should\\nbe added to the embedding of that something else in order to reflect this? Looking back in our\\ndiagram, let's set aside all of the keys and the queries, since after you compute the attention\\npattern you're done with those, then you're going to take this value matrix and multiply it by every\\none of those embeddings to produce a sequence of value vectors. You might think of these value\\nvectors as being kind of associated with the corresponding keys.\\nFor each column in this diagram, you multiply each of the value vectors by the corresponding\\nweight in that column.\\nFor example, here, under the embedding of creature, you would be adding large proportions\\nof the value vectors for fluffy and blue, while all of the other value vectors get zeroed\\nout, or at least nearly zeroed out.\\nAnd then finally, the way to actually update the embedding associated with this column,\\npreviously encoding some context-free meaning of creature, you add together all of these\\nrescaled values in the column, producing a change that you want to add that I'll label\\ndelta E, and then you add that to the original embedding.\\nHopefully what results is a more refined vector encoding the more contextually rich meaning,\\nlike that of a fluffy blue creature.\\nAnd of course you don't just do this to one embedding, you apply the same weighted sum\\nacross all of the columns in this picture, producing a sequence of changes.\\nAdding all of those changes to the corresponding embeddings produces a full sequence of more\\nrefined embeddings popping out of the attention block.\\nZooming out, this whole process is what you would describe as a single head of attention.\\nAs I've described things so far, this process is parameterized by three distinct matrices,\\nall filled with tunable parameters, the key, the query, and the value.\\nI want to take a moment to continue what we started in the last chapter with the scorekeeping\\nwhere we count up the total number of model parameters using the numbers from GPT-3.\\nThese key and query matrices each have 12,288 columns, matching the embedding dimension,\\nand 128 rows, matching the dimension of that smaller key query space.\\nThis gives us an additional 1.5 million or so parameters for each one.\\nIf you look at that value matrix by contrast, the way I've described things so far would\\nsuggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both\\nits inputs and its outputs live in this very large embedding space.\\nIf true, that would mean about 150 million added parameters.\\nAnd to be clear, you could do that, you could devote orders of magnitude more parameters\\nto the value map than to the key and query.\\nBut in practice, it is much more efficient if instead you make it so that the number\\nof parameters devoted to this value map is the same as the number devoted to the key\\nin the query.\\nThis is especially relevant in the setting of running multiple attention heads in parallel.\\nThe way this looks is that the value map is factored as a product of two smaller matrices.\\nConceptually, I would still encourage you to think about the overall linear map, one\\nwith inputs and outputs both in this larger embedding space, for example taking the embedding\\nof blue to this blueness direction that you would add to nouns.\\nIt's just that it's broken up into two separate steps.\\nThe first matrix on the right here has a smaller number of rows, typically the same size as\\nthe key query space.\\nWhat this means is you can think of it as mapping the large embedding vectors down to\\na much smaller space.\\nThis is not the conventional naming, but I'm going to call this the value down matrix.\\nThe second matrix maps from this smaller space back up to the embedding space, producing\\nthe vectors that you use to make the actual updates.\\nI'm going to call this one the value-up matrix, which, again, is not conventional.\\nThe way that you would see this written in most papers looks a little different.\\nI'll talk about it in a minute.\\nIn my opinion, it tends to make things a little more conceptually confusing.\\nTo throw in linear algebra jargon here, what we're basically doing is constraining the\\noverall value map to be a low-rank transformation.\\nTurning back to the parameter count, all four of these matrices have the same size, and\\nThen adding them all up, we get about 6.3 million parameters for one attention head.\\nAs a quick side note, to be a little more accurate, everything described so far is what\\npeople would call a self-attention head, to distinguish it from a variation that comes\\nup in other models that's called cross-attention.\\nThis isn't relevant to our GPT example, but if you're curious, cross-attention involves\\nmodels that process two distinct types of data, like text in one language and text in\\nanother language that's part of an ongoing generation of a translation.\\nOr maybe audio input of speech, and an ongoing transcription.\\nA cross-attention head looks almost identical.\\nThe only difference is that the key and query maps act on different datasets.\\nIn a model doing translation, for example, the keys might come from one language, while\\nthe queries come from another, and the attention pattern could describe which words from one\\nlanguage correspond to which words in another.\\nAnd in this setting there would typically be no masking, since there's not really any\\nnotion of later tokens affecting earlier ones.\\nStaying focused on self-attention though, if you understood everything so far, and if\\nyou were to stop here, you would come away with the essence of what attention really\\nis.\\nAll that's really left to us is to lay out the sense in which you do this many, many\\ndifferent times.\\nIn our central example we focused on adjectives updating nouns, but of course there are lots\\nof different ways that context can influence the meaning of a word.\\nIf the words they crashed the preceded the word car, it has implications for the shape\\nand the structure of that car, and a lot of associations might be less grammatical.\\nIf the word wizard is anywhere in the same passage as Harry, it suggests that this might\\nbe referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were\\nin that passage, then perhaps the embedding of Harry should instead be updated to refer\\nto the prince.\\nFor every different type of contextual updating that you might imagine, the parameters of\\nthese key and query matrices would be different to capture the different attention patterns,\\nand the parameters of our value map would be different based on what should be added to the\\nembeddings. And again, in practice the true behavior of these maps is much more difficult\\nto interpret, where the weights are set to do whatever the model needs them to do to best\\naccomplish its goal of predicting the next token. As I said before, everything we described is a\\nsingle head of attention, and a full attention block inside a transformer consists of what's\\ncalled multi-headed attention where you run a lot of these operations in parallel each with its own\\ndistinct key query and value maps. GPT-3 for example uses 96 attention heads inside each block.\\nConsidering that each one is already a bit confusing it's certainly a lot to hold in your\\nhead. Just to spell it all out very explicitly this means you have 96 distinct key and query\\nmatrices producing 96 distinct attention patterns. Then each head has its own distinct value matrices\\nused to produce 96 sequences of value vectors. These are all added together using the\\ncorresponding attention patterns as weights. What this means is that for each position in the\\ncontext, each token, every one of these heads produces a proposed change to be added to the\\nembedding in that position. So what you do is you sum together all of those proposed changes,\\none for each head, and you add the result to the original embedding of that position.\\nThis entire sum here would be one slice of what's outputted from this multi-headed attention block,\\na single one of those refined embeddings that pops out the other end of it.\\nAgain, this is a lot to think about, so don't worry at all if it takes some time to sink in.\\nThe overall idea is that by running many distinct heads in parallel,\\nyou're giving the model the capacity to learn many distinct ways that context changes meaning.\\nPulling up our running tally for parameter count with 96 heads, each including its own variation\\nof these four matrices, each block of multi-headed attention ends up with around 600 million\\nparameters. There's one added slightly annoying thing that I should really mention for any of you\\nwho go on to read more about transformers. You remember how I said that the value map is factored\\nout into these two distinct matrices, which I labeled as the value down and the value up\\nmatrices. The way that I framed things would suggest that you see this pair of matrices\\ninside each attention head, and you could absolutely implement it this way. That would\\nbe a valid design. But the way that you see this written in papers and the way that it's\\nimplemented in practice looks a little different. All of these value up matrices for each head\\nappear stapled together in one giant matrix that we call the output matrix, associated with\\nthe entire multi-headed attention block. And when you see people refer to the value matrix for a\\ngiven attention head, they're typically only referring to this first step, the one that I\\nwas labeling as the value down projection into the smaller space. For the curious among you,\\nI've left an on-screen note about it. It's one of those details that runs the risk of distracting\\nfrom the main conceptual points, but I do want to call it out just so that you know if you read\\nabout this in other sources. Setting aside all the technical nuances, in the preview from the\\nlast chapter, we saw how data flowing through a transformer doesn't just flow through a single\\nattention block. For one thing, it also goes through these other operations called multi-layer\\nperceptrons. We'll talk more about those in the next chapter. And then it repeatedly goes through\\nmany, many copies of both of these operations. What this means is that after a given word imbibes\\nsome of its context, there are many more chances for this more nuanced embedding to be influenced\\nby its more nuanced surroundings. The further down the network you go, with each embedding\\ntaking in more and more meaning from all the other embeddings, which themselves are getting\\nmore and more nuanced, the hope is that there's the capacity to encode higher level and more\\nabstract ideas about a given input beyond just descriptors and grammatical structure.\\nThings like sentiment and tone and whether it's a poem and what underlying scientific truths are\\nare relevant to the piece, and things like that.\\nTurning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the\\ntotal number of key, query, and value parameters is multiplied by another 96, which brings\\nthe total sum to just under 58 billion distinct parameters devoted to all of the attention\\nheads.\\nThat is a lot, to be sure, but it's only about a third of the 175 billion that are\\nin the network in total.\\nSo even though attention gets all of the attention, the majority of parameters come from the blocks\\nsitting in between these steps.\\nIn the next chapter, you and I will talk more about those other blocks and also a lot more\\nabout the training process.\\nA big part of the story for the success of the attention mechanism is not so much any\\nspecific kind of behavior that it enables, but the fact that it's extremely parallelizable,\\nmeaning that you can run a huge number of computations in a short time using GPUs.\\nthat one of the big lessons about deep learning in the last decade or two has been that scale\\nalone seems to give huge qualitative improvements in model performance. There's a huge advantage to\\nparallelizable architectures that let you do this. If you want to learn more about this stuff, I've\\nleft lots of links in the description. In particular, anything produced by Andre Karpathy or Chris Ola\\ntend to be pure gold. In this video, I wanted to just jump into attention in its current form,\\nbut if you're curious about more of the history for how we got here and how you might reinvent\\nthis idea for yourself, my friend Vivek just put up a couple videos giving a lot more of\\nthat motivation. Also, Britt Cruz from the channel The Art of the Problem\\nhas a really nice video about the history of large language models.\\nyou\\n\")]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "document = [Document(page_content=\"\\n\".join([t[\"text\"] for t in transcript[\"script\"]]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client.py          :1786 2024-10-12 11:31:22,301 HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "summary_chain = create_stuff_documents_chain(llm,summary_prompt)\n",
    "result = await summary_chain.ainvoke({\"context\": document})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- 🥓 한 남자가 베이컨을 가져옴.  \\n- 🥚 계란을 깨고 노른자를 따로 담음.  \\n- 🥄 흰자는 생으로 먹고, 소금을 넣음.  \\n- 🌳 나무젓가락을 사용함.  \\n- 🍝 파스타를 넣고 재료를 섞음.  \\n- 🧂 후추와 그라라빠다노를 뿌림.  \\n- 🔥 베이컨을 구워 계란치즈 소스를 섞음.  \\n- 🍽️ 까르보나라를 예쁘게 담아 완성함.  \\n- 🤤 저도 한번 꼭 먹어보고 싶네요.  '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/0f7nfvt16ln8630csjtkk_1w0000gn/T/ipykernel_41602/2497576997.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- 🥓 한 남자가 베이컨을 가져옴.  ',\n",
       " '- 🥚 계란을 깨고 노른자를 따로 담음.  ',\n",
       " '- 🥄 흰자는 생으로 먹고, 소금을 넣음.  ',\n",
       " '- 🌳 나무젓가락을 사용함.  ',\n",
       " '- 🍝 파스타를 넣고 재료를 섞음.  ',\n",
       " '- 🧂 후추와 그라라빠다노를 뿌림.  ',\n",
       " '- 🔥 베이컨을 구워 계란치즈 소스를 섞음.  ',\n",
       " '- 🍽️ 까르보나라를 예쁘게 담아 완성함.  ',\n",
       " '- 🤤 저도 한번 꼭 먹어보고 싶네요.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m documents \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39mcreate_documents([t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transcript[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m----> 3\u001b[0m     doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43msummary\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary' is not defined"
     ]
    }
   ],
   "source": [
    "documents = text_splitter.create_documents([t[\"text\"] for t in transcript[\"script\"]])\n",
    "for doc in documents:\n",
    "    doc.page_content += \"\\n\" + summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "you_url = \"https://youtube.com/shorts/a--NSC19MXM?si=yiun-HK_7wX1sNvL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'id': 'sync-26147017-ff6e-408d-9622-80c484868c42-e1', 'status': 'IN_QUEUE'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = \"https://api.runpod.ai/v2/uq96boxkzy99ev/runsync\"\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터 (내부적으로 사용할 파라미터 설정)\n",
    "body = {\"input\":{\n",
    "    \"api\":{\n",
    "        \"method\":\"POST\",\n",
    "        \"endpoint\":\"/ping\",\n",
    "    },\n",
    "    \"payload\":{},\n",
    "}}\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=body, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    print(\"Response:\", response.json())\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: IN_QUEUE\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 작업 ID (작업 완료된 job ID)\n",
    "job_id = response.json()['id']\n",
    "\n",
    "# RunPod API STATUS 엔드포인트 URL\n",
    "status_url = f\"https://api.runpod.ai/v2/wm1xrz07all039/status/{job_id}\"\n",
    "\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# 작업 상태 및 결과 확인 요청 보내기\n",
    "response = requests.get(status_url, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    job_result = response.json()\n",
    "    if job_result.get(\"status\") == \"COMPLETED\":\n",
    "        print(\"Job Completed! Result:\", job_result.get(\"output\"))\n",
    "    else:\n",
    "        print(f\"Job Status: {job_result.get('status')}\")\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'delayTime': 6071, 'executionTime': 3825, 'id': 'sync-c9bea7a9-08ea-447d-bd62-e481515985b4-e1', 'output': {'hashtags': '#파뿌리 #생일 #친구 #예능 #노랭이', 'title': '수제 김밥 30줄로 생일 파티합니다!! 역대급 선물 언박싱까지!!!!!!!'}, 'status': 'COMPLETED', 'workerId': 'vto3bvdf9z7v0a'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "you_url = \"https://youtu.be/omEk2BNDt1I?si=xjtbYANtlux5CTfB\"\n",
    "\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/get_title_hash\",\n",
    "        \"method\": \"GET\",\n",
    "        # \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"url\": you_url},\n",
    "    }\n",
    "}\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    print(\"Response:\", response.json())\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "# endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# # RunPod RUNSYNC 엔드포인트 URL\n",
    "# url = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "\n",
    "# # FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "# payload = {\n",
    "#     \"input\": {\n",
    "#         \"endpoint\": \"/get_script_summary\",\n",
    "#         \"method\": \"GET\",\n",
    "#         \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "#         \"params\": {\"url\": you_url},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # 요청 헤더에 API 키 추가\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {api_key}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # RUNSYNC 요청 보내기\n",
    "# response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# # 응답 확인\n",
    "# if response.status_code == 200:\n",
    "#     print(\"Response:\", response.json())\n",
    "# else:\n",
    "#     print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Response: {'id': 'c2cb7372-07b6-4146-b2a4-7c8ad4e0eb34-e1', 'status': 'IN_QUEUE'}\n",
      "Current status: IN_QUEUE\n",
      "Current status: IN_QUEUE\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: COMPLETED\n",
      "결과값:{'delayTime': 6033, 'executionTime': 191359, 'id': 'c2cb7372-07b6-4146-b2a4-7c8ad4e0eb34-e1', 'output': {'language': 'ko', 'script': [{'end': 1.92, 'start': 0, 'text': ' 오늘 우리 노랭이 생일입니다!'}, {'end': 8.32, 'start': 4.38, 'text': ' 팝콘이가 멤버들 생일을 맞은 소원을 좀 들어주고 있어요.'}, {'end': 10.08, 'start': 8.32, 'text': ' 오늘 노랭이의 소원은 바로'}, {'end': 12.36, 'start': 10.08, 'text': ' 노랭이가 가장 좋아하는 음식 뭐죠?'}, {'end': 13.8, 'start': 12.36, 'text': ' 김밥 파티입니다.'}, {'end': 16.06, 'start': 13.8, 'text': ' 우와 김밥 맛있겠다.'}, {'end': 17.36, 'start': 16.06, 'text': ' 김밥이 약간 좀 생소하지 않나?'}, {'end': 21.04, 'start': 17.36, 'text': ' 노랭이가 김밥을 좋아한다는 것을 모르는 분들도 좀 있을 텐데'}, {'end': 23.76, 'start': 21.04, 'text': ' 사실 저희 어머니는 아십니다.'}, {'end': 25.72, 'start': 23.76, 'text': ' 제가 김밥을 좋아하기 때문에'}, {'end': 28.04, 'start': 25.72, 'text': ' 기절 때 김밥 해달라고 제가 조르거든요.'}, {'end': 31.4, 'start': 28.04, 'text': ' 명절에 먹을 정도면 이건 인정입니까?'}, {'end': 33.12, 'start': 31.4, 'text': ' 솔직히 사먹는 김밥 중에'}, {'end': 34.96, 'start': 33.12, 'text': ' 우리 엄마 김밥이 없더라고.'}, {'end': 39.5, 'start': 35.98, 'text': ' 그래서 오늘 우리가 노랭이 생일 기념으로'}, {'end': 41.72, 'start': 39.5, 'text': ' 김밥을 한번 만들어보려고 이렇게 재료들을'}, {'end': 43.76, 'start': 41.72, 'text': ' 야오미!'}, {'end': 47.72, 'start': 43.76, 'text': ' 과연 이 재료로 노랭이의 향수를 저희가 느끼게 해줄 수 있을지'}, {'end': 49.98, 'start': 47.72, 'text': ' 어머니만큼은 안 돼.'}, {'end': 53.24, 'start': 49.98, 'text': ' 힘들겠지만 아버지만큼은 할 수 있지.'}, {'end': 56.72, 'start': 53.24, 'text': ' 우리 다 같이 이렇게 김밥 오순노소 만들면서 재밌게 놀아보자고.'}, {'end': 57.64, 'start': 56.72, 'text': ' 좋습니다.'}, {'end': 59.4, 'start': 57.64, 'text': ' 결국엔 김밥 맛이 중요한 게 아니야.'}, {'end': 60.52, 'start': 59.4, 'text': ' 오순도순이 중요한 거야.'}, {'end': 61.64, 'start': 60.52, 'text': ' 아니야. 근데 맛도 중요하잖아.'}, {'end': 62.88, 'start': 61.64, 'text': ' 아 맞나.'}, {'end': 63.88, 'start': 62.88, 'text': ' 맛있게 먹어야지.'}, {'end': 65.32, 'start': 63.88, 'text': ' 아니야.'}, {'end': 66.14, 'start': 65.32, 'text': ' 아니야. 아니야.'}, {'end': 66.72, 'start': 66.14, 'text': ' 아니야.'}, {'end': 68.82, 'start': 66.72, 'text': ' 거기서 준비되면 안 돼.'}, {'end': 71.52, 'start': 68.82, 'text': ' 무슨 말만 하면 요즘 딱 후산이 뭐.'}, {'end': 72.76, 'start': 71.52, 'text': ' 오늘의 룰 있습니까?'}, {'end': 73.52, 'start': 72.76, 'text': ' 생일 룰.'}, {'end': 74.88, 'start': 73.52, 'text': ' 아 생일 룰?'}, {'end': 75.72, 'start': 74.88, 'text': ' 욕 금지.'}, {'end': 76.32, 'start': 75.72, 'text': ' 아 욕.'}, {'end': 77.16, 'start': 76.32, 'text': ' 아 욕 금지.'}, {'end': 78.72, 'start': 77.16, 'text': ' 저번에 말했던 대로 한번'}, {'end': 80.1, 'start': 78.72, 'text': ' 외래어 금지 한번 해볼까요?'}, {'end': 81.2, 'start': 80.1, 'text': ' 아 진짜로 외래어를 금지.'}, {'end': 82.4, 'start': 81.2, 'text': ' 외래어 금지.'}, {'end': 83.8, 'start': 82.4, 'text': ' 막 이렇게 쓰면?'}, {'end': 85.24, 'start': 83.8, 'text': ' 쓰면 짜오가'}, {'end': 85.6, 'start': 85.24, 'text': ' 어머.'}, {'end': 86.6, 'start': 85.6, 'text': ' 움직여줘.'}, {'end': 87.56, 'start': 86.6, 'text': ' 아 그러면 자기가 쓰면'}, {'end': 88.8, 'start': 87.56, 'text': ' 자기가 자기를 응징하나?'}, {'end': 90.96, 'start': 89.8, 'text': ' 아 저는 깍두기.'}, {'end': 92, 'start': 90.96, 'text': ' 네가 왜.'}, {'end': 93.7, 'start': 92, 'text': ' 저는 진행을 해야 되니깐요.'}, {'end': 97.14, 'start': 95.7, 'text': ' 약간 이 둘이가 불리해.'}, {'end': 98.04, 'start': 97.14, 'text': ' 영문학과.'}, {'end': 100.08, 'start': 98.04, 'text': ' 또 외고 출신.'}, {'end': 101.38, 'start': 100.08, 'text': ' 큰일 났네.'}, {'end': 104.84, 'start': 101.38, 'text': ' 외래어가 금지된 노잉이의 김밥 생일 잔치.'}, {'end': 106.48, 'start': 104.84, 'text': ' 시작해 보겠습니다.'}, {'end': 107.34, 'start': 106.48, 'text': ' 쉽지 않네.'}, {'end': 108.78, 'start': 107.34, 'text': ' 발 없나? 발.'}, {'end': 109.34, 'start': 108.78, 'text': ' 발?'}, {'end': 109.68, 'start': 109.34, 'text': ' 어.'}, {'end': 111.08, 'start': 109.68, 'text': ' 발.'}, {'end': 114.76, 'start': 113.78, 'text': ' 어 욕 금지.'}, {'end': 115.68, 'start': 114.76, 'text': ' 욕 금지.'}, {'end': 116.42, 'start': 115.68, 'text': ' 욕하면 안 돼.'}, {'end': 117.56, 'start': 116.42, 'text': ' 표정으로 욕하고'}, {'end': 118.32, 'start': 117.56, 'text': ' 알아서 욕 금지.'}, {'end': 119.9, 'start': 118.32, 'text': ' 발이 시라이.'}, {'end': 120.86, 'start': 119.9, 'text': ' 이거 뭐야?'}, {'end': 121.96, 'start': 120.86, 'text': ' 이거 누구 거야?'}, {'end': 122.9, 'start': 121.96, 'text': ' 삼겹살이네.'}, {'end': 124.7, 'start': 122.9, 'text': ' 아 요즘 삼겹살에요.'}, {'end': 125.6, 'start': 124.7, 'text': ' 깻잎 넣어가지고'}, {'end': 126.5, 'start': 125.6, 'text': ' 볼도 말고.'}, {'end': 128.04, 'start': 126.5, 'text': ' 이거 형이 만든 거야.'}, {'end': 128.74, 'start': 128.04, 'text': ' 너가 써도 돼.'}, {'end': 130.08, 'start': 128.74, 'text': ' 나는 진행을 해야 되기 때문에.'}, {'end': 131.54, 'start': 130.08, 'text': ' 싫어.'}, {'end': 132.58, 'start': 131.54, 'text': ' 작정하고 막.'}, {'end': 133.68, 'start': 132.58, 'text': ' 작정하고 골팡 나고.'}, {'end': 134.92, 'start': 133.68, 'text': ' 아니.'}, {'end': 136.48, 'start': 134.92, 'text': ' 야 우리 밥 몇 개나 쓰려나.'}, {'end': 137.98, 'start': 136.48, 'text': ' 역시 안박서 12개인데.'}, {'end': 138.58, 'start': 137.98, 'text': ' 어? 어?'}, {'end': 140.48, 'start': 138.58, 'text': ' 당신 그 말 조심해야 됩니다.'}, {'end': 141.52, 'start': 140.48, 'text': ' 상자.'}, {'end': 143.66, 'start': 141.52, 'text': ' 자 여러분들 이제 진짜 시작합니다.'}, {'end': 145.16, 'start': 143.66, 'text': ' 생일자가 서른 원입니다. 서른 원.'}, {'end': 146.68, 'start': 145.16, 'text': ' 외래어 금지 시작이라고.'}, {'end': 147.4, 'start': 146.68, 'text': ' 외래어 금지.'}, {'end': 148.8, 'start': 147.56, 'text': ' 쌈도.'}, {'end': 150.36, 'start': 148.8, 'text': ' 오케이 마지막입니다.'}, {'end': 151.36, 'start': 150.36, 'text': ' 시작입니다.'}, {'end': 153.8, 'start': 151.36, 'text': ' 자 여기 수많은 재료들이 있는데 짱배는?'}, {'end': 155.76, 'start': 153.8, 'text': ' 아 저 같은 경우에는 바로 이'}, {'end': 157.4, 'start': 155.76, 'text': ' 새우를 이용할 겁니다.'}, {'end': 157.9, 'start': 157.4, 'text': ' 새우?'}, {'end': 159.34, 'start': 157.9, 'text': ' 이 노래가 새우 좋아하는 거 아니야?'}, {'end': 161.78, 'start': 159.34, 'text': ' 근데 생새우를 사용하면은'}, {'end': 163.14, 'start': 161.78, 'text': ' 이 형들이'}, {'end': 164.04, 'start': 163.14, 'text': ' 아'}, {'end': 167.22, 'start': 166.04, 'text': ' 어우 상하게 얘기해.'}, {'end': 168.24, 'start': 167.22, 'text': ' 왜왜왜.'}, {'end': 170.78, 'start': 168.24, 'text': ' 거부 반응이 일어나가지고'}, {'end': 172.16, 'start': 170.78, 'text': ' 거부 반응 때문에 좀'}, {'end': 172.96, 'start': 172.16, 'text': ' 데칠 겁니다.'}, {'end': 177.02, 'start': 175.56, 'text': ' 아 만들어서 하나씩 쪼서 먹는구나.'}, {'end': 177.5, 'start': 177.02, 'text': ' 김밥을'}, {'end': 177.52, 'start': 177.5, 'text': ' 김밥을'}, {'end': 177.56, 'start': 177.52, 'text': ' 김밥을'}, {'end': 179, 'start': 177.56, 'text': ' 김밥을 이렇게 쪼서 먹는 맛이 있어.'}, {'end': 181.64, 'start': 179, 'text': ' 이거 햄은 뭐 구워서 넣어? 아니면 그냥 넣어?'}, {'end': 182.86, 'start': 181.64, 'text': ' 그냥 넣어 그냥 넣어 그냥 넣어 그냥 넣어.'}, {'end': 184.04, 'start': 182.86, 'text': ' 하자 볶아야지.'}, {'end': 185.6, 'start': 184.04, 'text': ' 그럼 짱배가 좀 볶아줄래?'}, {'end': 187.8, 'start': 185.6, 'text': ' 그래 내가 내가 볶아야 돼.'}, {'end': 189.24, 'start': 187.8, 'text': ' 혹시'}, {'end': 190.7, 'start': 189.24, 'text': ' 아냐아냐 그거 말고 좀 좀 좀.'}, {'end': 192.9, 'start': 190.7, 'text': ' 이거 왜 이렇게 새까매?'}, {'end': 194.88, 'start': 192.9, 'text': ' 이거 행성 폭발이잖아 이거.'}, {'end': 196.92, 'start': 194.88, 'text': ' 이거 스모키 향 내.'}, {'end': 199.54, 'start': 196.92, 'text': ' 스모키 용도로 쓰는 거 아니야.'}, {'end': 200.52, 'start': 199.54, 'text': ' 잠시만요.'}, {'end': 201.92, 'start': 200.52, 'text': ' 잠시만요.'}, {'end': 202.78, 'start': 201.92, 'text': ' 안녕.'}, {'end': 204.28, 'start': 202.78, 'text': ' 빵.'}, {'end': 206.56, 'start': 204.28, 'text': ' 생각보다 좀 심해야겠네.'}, {'end': 207.52, 'start': 206.56, 'text': ' 모래이가 생각하는.'}, {'end': 209.16, 'start': 207.52, 'text': ' 김밥의 생명은 뭐죠?'}, {'end': 210.46, 'start': 209.16, 'text': ' 단무지.'}, {'end': 212.8, 'start': 210.46, 'text': ' 단무지 안 들어가면 솔직히 김밥 맛이 안 나.'}, {'end': 215.3, 'start': 212.8, 'text': ' 단무지가 약간 김밥의 정체성이다.'}, {'end': 216.16, 'start': 215.3, 'text': ' 네.'}, {'end': 217.3, 'start': 216.16, 'text': ' 그 좀 특별합니까?'}, {'end': 219.76, 'start': 217.3, 'text': ' 그 어머니의 김밥에 들어가는 재료들이.'}, {'end': 220.9, 'start': 219.76, 'text': ' 아 특별하지 않아.'}, {'end': 223.24, 'start': 220.9, 'text': ' 햄, 계란, 단무지, 시금치.'}, {'end': 225, 'start': 223.24, 'text': ' 한 요 정도만 들어가는 거 같아.'}, {'end': 226.24, 'start': 225, 'text': ' 그렇게만 했는데도'}, {'end': 227.88, 'start': 226.24, 'text': ' 밖에서 사 먹으면 그 맛이 안 나.'}, {'end': 231.04, 'start': 227.88, 'text': ' 약간 어머니 고유의 밑간이나 이런 게 또 있나 봐.'}, {'end': 231.64, 'start': 231.04, 'text': ' 아 그럼요 그럼요.'}, {'end': 234.04, 'start': 231.64, 'text': ' 노랭이 어머니가 진짜로 진짜 요리를 잘하세요.'}, {'end': 236.88, 'start': 234.04, 'text': ' 아 진짜 거의 뭐 한식 전문가급이라서.'}, {'end': 237.48, 'start': 236.88, 'text': ' 맛도 제법.'}, {'end': 238.34, 'start': 237.48, 'text': ' 어 진짜.'}, {'end': 239.98, 'start': 238.34, 'text': ' 어?'}, {'end': 242.46, 'start': 239.98, 'text': ' 하하하하.'}, {'end': 244.18, 'start': 242.46, 'text': ' 김밥.'}, {'end': 246.52, 'start': 244.18, 'text': ' 아 얼마나 그 부분을.'}, {'end': 247.42, 'start': 246.52, 'text': ' 어 너무 재밌다.'}, {'end': 249.6, 'start': 247.42, 'text': ' 잠깐만.'}, {'end': 250.62, 'start': 249.6, 'text': ' 진행자예요.'}, {'end': 252.66, 'start': 250.62, 'text': ' 아 카메라 잡고 있어.'}, {'end': 253.52, 'start': 252.66, 'text': ' 하하하하.'}, {'end': 254.36, 'start': 253.52, 'text': ' 내가 형님.'}, {'end': 255.56, 'start': 254.36, 'text': ' 김밥.'}, {'end': 256.6, 'start': 255.56, 'text': ' 하하하하.'}, {'end': 257.36, 'start': 256.6, 'text': ' 어 야 이거.'}, {'end': 258.34, 'start': 257.36, 'text': ' 방심하면 안 되겠다.'}, {'end': 260.36, 'start': 258.34, 'text': ' 어 근데 인디업 한번 봐주는 겁니까?'}, {'end': 261.08, 'start': 260.36, 'text': ' 어?'}, {'end': 261.64, 'start': 261.08, 'text': ' 어?'}, {'end': 262.04, 'start': 261.64, 'text': ' 어?'}, {'end': 262.58, 'start': 262.04, 'text': ' 어?'}, {'end': 263.1, 'start': 262.58, 'text': ' 어?'}, {'end': 263.64, 'start': 263.1, 'text': ' 어?'}, {'end': 265.28, 'start': 263.64, 'text': ' 진행자가.'}, {'end': 266.84, 'start': 265.28, 'text': ' 아 근데 우리 다 했잖아 근데.'}, {'end': 269.08, 'start': 266.84, 'text': ' 아니 오늘 제가 입 닫고 있었어.'}, {'end': 271.12, 'start': 269.08, 'text': ' 자 그러면 우리 밥 끓여야겠네.'}, {'end': 272.34, 'start': 271.12, 'text': ' 원숭이.'}, {'end': 274.42, 'start': 272.34, 'text': ' 하하하하.'}, {'end': 276.52, 'start': 274.42, 'text': ' 맛살 요정도 쟤 쓰면 뭐 어때?'}, {'end': 277.22, 'start': 276.52, 'text': ' 하하하하.'}, {'end': 278.26, 'start': 277.22, 'text': ' 맛살 무덤이다.'}, {'end': 279.52, 'start': 278.26, 'text': ' 산산산 쌓이겠다.'}, {'end': 280.46, 'start': 279.52, 'text': ' 아 이건 그냥 먹어야지.'}, {'end': 280.96, 'start': 280.46, 'text': ' 어.'}, {'end': 283.72, 'start': 280.96, 'text': ' 햄을 굽겠습니다.'}, {'end': 284.82, 'start': 283.72, 'text': ' 어.'}, {'end': 285.96, 'start': 284.82, 'text': ' 기름 부이는 거 조심하고.'}, {'end': 286.8, 'start': 285.96, 'text': ' 아잉.'}, {'end': 287.3, 'start': 286.8, 'text': ' 잘 됐다.'}, {'end': 289.1, 'start': 287.3, 'text': ' 김밥도 만들어보자.'}, {'end': 291.1, 'start': 289.1, 'text': ' 이게 진짜 김밥 아이가?'}, {'end': 292.4, 'start': 291.1, 'text': ' 하하하하.'}, {'end': 294.46, 'start': 292.4, 'text': ' 김이 펄펄 나는 밥이 무슨.'}, {'end': 295.6, 'start': 294.46, 'text': ' 와.'}, {'end': 296.2, 'start': 295.6, 'text': ' 야.'}, {'end': 297.7, 'start': 296.84, 'text': ' 지단.'}, {'end': 298.74, 'start': 297.7, 'text': ' 좋아.'}, {'end': 300.04, 'start': 298.74, 'text': ' 축구선수 아닙니다.'}, {'end': 304.48, 'start': 300.04, 'text': ' 지단도 그렇게 많이 필요하면 집에서 일일이 만들 필요 없이 이렇게 팔아.'}, {'end': 308.12, 'start': 304.48, 'text': ' 제가 예전에 GS 납품하는 도시락 공장에서 일했었거든요.'}, {'end': 308.96, 'start': 308.12, 'text': ' 요런 거 다 써.'}, {'end': 309.72, 'start': 308.96, 'text': ' 음.'}, {'end': 310.32, 'start': 309.72, 'text': ' GS.'}, {'end': 311.02, 'start': 310.32, 'text': ' GS.'}, {'end': 311.62, 'start': 311.02, 'text': ' 하하하하.'}, {'end': 312.12, 'start': 311.62, 'text': ' 어?'}, {'end': 313.22, 'start': 312.12, 'text': ' 하하하하.'}, {'end': 314.02, 'start': 313.22, 'text': ' 원숭이.'}, {'end': 316.46, 'start': 314.02, 'text': ' 하하하하.'}, {'end': 317.62, 'start': 316.46, 'text': ' 자 생일자도.'}, {'end': 318.36, 'start': 317.62, 'text': ' 자 생일자도.'}, {'end': 320.2, 'start': 318.36, 'text': ' 원주민밥 맛있네.'}, {'end': 321.9, 'start': 320.2, 'text': ' 형님 그거 혹시 꼽는 김에.'}, {'end': 323.1, 'start': 321.9, 'text': ' 요것도.'}, {'end': 323.86, 'start': 323.1, 'text': ' 하하하하.'}, {'end': 325.56, 'start': 323.86, 'text': ' 오 잘 꼽고 있네 거의.'}, {'end': 326.16, 'start': 325.56, 'text': ' 하하하하.'}, {'end': 326.7, 'start': 326.16, 'text': ' 하하하하.'}, {'end': 327.38, 'start': 326.84, 'text': ' 국물.'}, {'end': 328.8, 'start': 327.38, 'text': ' 국물.'}, {'end': 329.58, 'start': 328.8, 'text': ' 국물.'}, {'end': 330.28, 'start': 329.58, 'text': ' 국기 잘 했나?'}, {'end': 331.34, 'start': 330.28, 'text': ' 야 국기 잘 했다.'}, {'end': 331.78, 'start': 331.34, 'text': ' 예.'}, {'end': 332.68, 'start': 331.78, 'text': ' 고였다니까.'}, {'end': 335.62, 'start': 332.68, 'text': ' 국물 같이 넣어야 더 실한 있지 않을까.'}, {'end': 337.56, 'start': 335.62, 'text': ' 여기 살짝 마요.'}, {'end': 338.18, 'start': 337.56, 'text': ' 어?'}, {'end': 338.52, 'start': 338.18, 'text': ' 어?'}, {'end': 339.96, 'start': 338.52, 'text': ' 그러지 마요.'}, {'end': 341.58, 'start': 339.96, 'text': ' 하하하하.'}, {'end': 341.96, 'start': 341.58, 'text': ' 국물.'}, {'end': 342.56, 'start': 341.96, 'text': ' 국물.'}, {'end': 343.8, 'start': 342.56, 'text': ' 국물 넣는 거 그러지 마요.'}, {'end': 344.72, 'start': 343.8, 'text': ' 오오.'}, {'end': 346.46, 'start': 344.72, 'text': ' 야 오늘.'}, {'end': 347.36, 'start': 346.46, 'text': ' 가면 카메라.'}, {'end': 348.02, 'start': 347.36, 'text': ' 하하하하.'}, {'end': 349.5, 'start': 348.02, 'text': ' 하하하하.'}, {'end': 351.56, 'start': 349.5, 'text': ' 어어어.'}, {'end': 353.44, 'start': 351.56, 'text': ' 어둡다.'}, {'end': 354, 'start': 353.44, 'text': ' 밥.'}, {'end': 354.4, 'start': 354, 'text': ' 어?'}, {'end': 355.84, 'start': 354.4, 'text': ' 노랭이 많이 수다.'}, {'end': 356.16, 'start': 355.84, 'text': ' 예.'}, {'end': 357.76, 'start': 356.16, 'text': ' 간단하게 하나 말아보죠.'}, {'end': 362.98, 'start': 361.92, 'text': ' 짜잔!'}, {'end': 364.26, 'start': 363.4, 'text': ' 간단하게.'}, {'end': 366.48, 'start': 364.92, 'text': ' 자 오늘의 1호 김밥.'}, {'end': 367.44, 'start': 366.48, 'text': ' 첫 입입니다.'}, {'end': 368.98, 'start': 367.86, 'text': ' 역사적인 한 입.'}, {'end': 373.42, 'start': 372.4, 'text': ' 맛있어, 사나.'}, {'end': 377.2, 'start': 375.42, 'text': ' 2호 김밥 완성.'}, {'end': 378.1, 'start': 377.2, 'text': ' 어 2호 김밥.'}, {'end': 382.76, 'start': 381.22, 'text': ' 왜 이렇게 한쪽으로 쏠려?'}, {'end': 383.8, 'start': 382.76, 'text': ' 반반이 되네.'}, {'end': 385.48, 'start': 383.8, 'text': ' 야 너무 갈라치기 된 거 아니야?'}, {'end': 386.54, 'start': 385.48, 'text': ' 그래, 그래도 맛있잖아.'}, {'end': 389.08, 'start': 387.7, 'text': ' 밥이 좀 질 나?'}, {'end': 392.72, 'start': 390.3, 'text': ' 원래 밥에도 뭘 이렇게 하지 않습니까?'}, {'end': 398.62, 'start': 395.52, 'text': ' 와 근데 이 밥 자체로도 이미 맛있는 하나가 됐어.'}, {'end': 403.78, 'start': 402.62, 'text': ' 아 발로 먹었네.'}, {'end': 404.66, 'start': 403.78, 'text': ' 오늘 진짜..'}, {'end': 408.5, 'start': 407.66, 'text': ' 발로 먹어.'}, {'end': 410.2, 'start': 408.5, 'text': ' 어 발로..'}, {'end': 411.08, 'start': 410.2, 'text': ' 와 됐다.'}, {'end': 412.46, 'start': 411.6, 'text': ' 화목이!'}, {'end': 414.5, 'start': 412.7, 'text': ' 아직 여전히 좀 쏠려있어.'}, {'end': 417.48, 'start': 415.48, 'text': ' 화목이의 첫 입은 짱베에게.'}, {'end': 418.48, 'start': 417.48, 'text': ' 이럴 수가.'}, {'end': 424.16, 'start': 423.66, 'text': ' 어때?'}, {'end': 424.92, 'start': 424.16, 'text': ' 맛있다.'}, {'end': 426.16, 'start': 424.92, 'text': ' 오 맛있어요?'}, {'end': 429.12, 'start': 426.52, 'text': ' 감정이 더 묻어나니까 더 맛있는 것 같아.'}, {'end': 430.42, 'start': 429.12, 'text': ' 짱베 입장에서는.'}, {'end': 431.08, 'start': 430.42, 'text': ' 여러분들.'}, {'end': 433.2, 'start': 431.7, 'text': ' 감격사자 꺼졌습니다.'}, {'end': 434.96, 'start': 433.2, 'text': ' 아 좋아요.'}, {'end': 435.96, 'start': 434.96, 'text': ' 맑아 이제.'}, {'end': 439.8, 'start': 438.7, 'text': ' 오 이거 기대됩니다.'}, {'end': 440.44, 'start': 439.8, 'text': ' 참치.'}, {'end': 441.2, 'start': 440.44, 'text': ' 자 나오라.'}, {'end': 443.04, 'start': 442.2, 'text': ' 이거 봐요.'}, {'end': 444.48, 'start': 443.04, 'text': ' 짜잔.'}, {'end': 445.48, 'start': 444.48, 'text': ' 오.'}, {'end': 447.08, 'start': 446, 'text': ' 너도 쏠렸네.'}, {'end': 450.38, 'start': 448.08, 'text': ' 어 나도 한쪽으로 쏠렸지만.'}, {'end': 451.24, 'start': 450.38, 'text': ' 음 맛있어!'}, {'end': 451.96, 'start': 451.24, 'text': ' 참치김밥.'}, {'end': 456.08, 'start': 454.72, 'text': ' 혹시 참치김밥 별로 안 좋아?'}, {'end': 459.72, 'start': 458.6, 'text': ' 지퍼여쏘.'}, {'end': 462.34, 'start': 460.72, 'text': ' 요런 거 준비가 되었습니다.'}, {'end': 465.32, 'start': 463.72, 'text': ' 자 짱베 너 말해도 돼. 말해도 돼.'}, {'end': 465.82, 'start': 465.32, 'text': ' 그래?'}, {'end': 468.18, 'start': 465.82, 'text': ' 저는 저희 둘째 재료.'}, {'end': 469.6, 'start': 468.18, 'text': ' 분수 들어갑니다.'}, {'end': 470.88, 'start': 469.6, 'text': ' 어 분수?'}, {'end': 474.26, 'start': 471.48, 'text': ' 여기 제가 만든 참치김밥도 있습니다.'}, {'end': 475.36, 'start': 474.6, 'text': ' 어라.'}, {'end': 476.92, 'start': 475.48, 'text': ' 후차님 먹어보고 싶은 사람?'}, {'end': 477.58, 'start': 476.92, 'text': ' 야!'}, {'end': 478.34, 'start': 477.58, 'text': ' 안 좋아한다며?'}, {'end': 478.84, 'start': 478.34, 'text': ' 맞아.'}, {'end': 485.72, 'start': 482.44, 'text': ' 전 새우를 듬뿍 넣은 새우 기도를 만들어보겠습니다.'}, {'end': 486.98, 'start': 485.72, 'text': ' 야 나 처음 봐.'}, {'end': 488.38, 'start': 486.98, 'text': ' 나도 처음 본다.'}, {'end': 490.08, 'start': 488.38, 'text': ' 야 이거 어떻게 말하자나요?'}, {'end': 491.18, 'start': 490.08, 'text': ' 너의 느낌대로.'}, {'end': 493.14, 'start': 491.18, 'text': ' 우리 엄마는 발을 안 쓴다는데.'}, {'end': 494.12, 'start': 493.14, 'text': ' 옳지 옳지 옳지 옳지.'}, {'end': 495.42, 'start': 494.12, 'text': ' 오 니 잘한다!'}, {'end': 496.02, 'start': 495.42, 'text': ' 옳지!'}, {'end': 497.18, 'start': 496.02, 'text': ' 와 그냥 말아버린다.'}, {'end': 498.52, 'start': 497.18, 'text': ' 참야 니 재능 있다!'}, {'end': 499.98, 'start': 498.52, 'text': ' 어? 감각이 달라요?'}, {'end': 501.08, 'start': 499.98, 'text': ' 제가 먼저 먹어보겠습니다 그러면.'}, {'end': 501.58, 'start': 501.08, 'text': ' 그래.'}, {'end': 509.42, 'start': 505.48, 'text': ' 별맛이 안 났네 생각보다 생새우 살이.'}, {'end': 511.54, 'start': 509.42, 'text': ' 안에 뭐 양념거나 아니면 초장이나 이런 거.'}, {'end': 513.78, 'start': 511.54, 'text': ' 초장 느낌 있네.'}, {'end': 515.52, 'start': 513.78, 'text': ' 제가 말씀드리는 건 진멸이죠.'}, {'end': 518.64, 'start': 515.52, 'text': ' 오! 가장 얄쌍한 김밥이라고 하셨습니다.'}, {'end': 520.54, 'start': 518.64, 'text': ' 진멸이 김밥.'}, {'end': 524.58, 'start': 520.54, 'text': ' 이따가 내가 김밥 잔치를 더 화끈하게 만들어줄'}, {'end': 527.48, 'start': 524.58, 'text': ' 떡볶이를 준비해보겠습니다.'}, {'end': 528.48, 'start': 527.48, 'text': ' 찍어먹으면 맛있잖아요?'}, {'end': 530.38, 'start': 528.48, 'text': ' 내가 또 옛날에 상어...'}, {'end': 531.58, 'start': 530.38, 'text': ' 상어 먹었지 않습니까?'}, {'end': 533.22, 'start': 531.58, 'text': ' 고 경험을 바탕으로'}, {'end': 535.28, 'start': 533.22, 'text': ' 활용 점점을 찍어먹겠습니다.'}, {'end': 535.98, 'start': 535.28, 'text': ' 좋아요.'}, {'end': 537.68, 'start': 535.98, 'text': ' 이번에는 익은 거.'}, {'end': 539.58, 'start': 537.68, 'text': ' 살짝만.'}, {'end': 540.88, 'start': 539.58, 'text': ' 따란!'}, {'end': 542.32, 'start': 540.88, 'text': ' 오장대의 초장 김밥.'}, {'end': 546.18, 'start': 542.32, 'text': ' 어! 진이야.'}, {'end': 547.52, 'start': 546.18, 'text': ' 음!'}, {'end': 549.82, 'start': 547.52, 'text': ' 이건가 봐!'}, {'end': 552.42, 'start': 549.82, 'text': ' 조용히 하세요. 더 하니까 더.'}, {'end': 553.82, 'start': 552.42, 'text': ' 음!'}, {'end': 556.18, 'start': 553.82, 'text': ' 아! 소스였습니다 여러분들!'}, {'end': 558.22, 'start': 556.18, 'text': ' 소스였어요.'}, {'end': 560.52, 'start': 558.22, 'text': ' 하하하하하하'}, {'end': 563.84, 'start': 560.52, 'text': ' 너무 좋아서.'}, {'end': 565.02, 'start': 563.84, 'text': ' 오징어!'}, {'end': 565.52, 'start': 565.02, 'text': ' 김밥!'}, {'end': 567.02, 'start': 565.52, 'text': ' 오징어 김밥!'}, {'end': 569.12, 'start': 567.02, 'text': ' 김밥!'}, {'end': 572.92, 'start': 569.12, 'text': ' 하하하하하하'}, {'end': 573.52, 'start': 572.92, 'text': ' 와우!'}, {'end': 574.36, 'start': 573.52, 'text': ' 아직 안 끝났네.'}, {'end': 575.26, 'start': 574.36, 'text': ' 와우!'}, {'end': 578.02, 'start': 575.26, 'text': ' 하하하하하하'}, {'end': 579.98, 'start': 578.02, 'text': ' 야! 야! 외국인 아니야?'}, {'end': 583.68, 'start': 579.98, 'text': ' 자, 우리는 이제 공장처럼 김밥을 생산 중입니다.'}, {'end': 584.72, 'start': 583.68, 'text': ' 진짜 김밥 공장이야.'}, {'end': 586.86, 'start': 584.72, 'text': ' 자, 이제 내가 상감사를 활용한 방법을 알려줄게.'}, {'end': 587.72, 'start': 586.86, 'text': ' 네가 말해봐 봐.'}, {'end': 588.68, 'start': 587.72, 'text': ' 이거 잘 맞는 것 같아.'}, {'end': 589.38, 'start': 588.68, 'text': ' 응?'}, {'end': 590.22, 'start': 589.38, 'text': ' 꿀 팔라 그러네.'}, {'end': 591.56, 'start': 590.22, 'text': ' 궁금하다. 궁금한 사람 나와봐.'}, {'end': 593.92, 'start': 591.56, 'text': ' 갈고, 거기다가 이제 깻잎 한 두 장 정도.'}, {'end': 594.92, 'start': 593.92, 'text': ' 꼭다리 딴 뒤에.'}, {'end': 595.52, 'start': 595.02, 'text': ' 어.'}, {'end': 598.32, 'start': 595.52, 'text': ' 삼겹살 한 줄.'}, {'end': 599.68, 'start': 598.32, 'text': ' 단무지랑 우엉만.'}, {'end': 600.82, 'start': 599.68, 'text': ' 단무지랑 우엉만.'}, {'end': 601.62, 'start': 600.82, 'text': ' 야, 이걸 말 수 있나?'}, {'end': 603.18, 'start': 601.62, 'text': ' 자, 그리고 여기서 핵심인데.'}, {'end': 605.82, 'start': 603.18, 'text': ' 자, 저기 냉장고 안에 쌈장이 있어.'}, {'end': 606.72, 'start': 605.82, 'text': ' 아!'}, {'end': 607.72, 'start': 606.72, 'text': ' 쌈장 한 번 가져와.'}, {'end': 608.68, 'start': 607.72, 'text': ' 좋아, 좋아, 좋아.'}, {'end': 610.92, 'start': 608.68, 'text': ' 아주 좋아!'}, {'end': 612.48, 'start': 610.92, 'text': ' 아, 꿀팁인데?'}, {'end': 613.42, 'start': 612.48, 'text': ' 아, 잘해.'}, {'end': 615.02, 'start': 613.42, 'text': ' 아, 잘해.'}, {'end': 616.72, 'start': 615.02, 'text': ' 나 계속 일 시키려고 자도한 거 아니야?'}, {'end': 617.96, 'start': 616.72, 'text': ' 아니야, 진짜 잘해.'}, {'end': 620.08, 'start': 617.96, 'text': ' 이 역사적인 한 입은 생일자가 사야 된다.'}, {'end': 621.92, 'start': 620.08, 'text': ' 아, 생일!'}, {'end': 623.92, 'start': 621.92, 'text': ' 아, 생일!'}, {'end': 624.72, 'start': 623.92, 'text': ' 맛있어.'}, {'end': 626.22, 'start': 625.02, 'text': ' 오!'}, {'end': 627.02, 'start': 626.22, 'text': ' 그냥 깔끔하게 맛있어.'}, {'end': 628.02, 'start': 627.02, 'text': ' 지금 카메라만 틀어봐.'}, {'end': 629.72, 'start': 628.02, 'text': ' 이건 개발자가 먹어야 돼.'}, {'end': 630.72, 'start': 629.72, 'text': ' 그래, 개발자 한 입.'}, {'end': 637.52, 'start': 636.72, 'text': ' 너무 맛있어.'}, {'end': 638.32, 'start': 637.52, 'text': ' 고기 좋아.'}, {'end': 639.42, 'start': 638.32, 'text': ' 참치까지.'}, {'end': 640.92, 'start': 639.42, 'text': ' 아, 밥이 별로 없네요.'}, {'end': 642.42, 'start': 640.92, 'text': ' 그럼 맛살만 한 번 해볼까?'}, {'end': 644.12, 'start': 642.42, 'text': ' 밥 대신 맛살.'}, {'end': 645.02, 'start': 644.12, 'text': ' 저건 좀...'}, {'end': 645.72, 'start': 645.02, 'text': ' 아, 이럴 때 해보지.'}, {'end': 646.32, 'start': 645.72, 'text': ' 어찌하겠어.'}, {'end': 647.02, 'start': 646.32, 'text': ' 그래.'}, {'end': 648.72, 'start': 647.02, 'text': ' 난 밥 대신 부추 해볼래, 그럼.'}, {'end': 650.62, 'start': 648.72, 'text': ' 밥 좀 더 데워줘.'}, {'end': 652.62, 'start': 650.62, 'text': ' 두 개만 더 하면 딱 한 상자야.'}, {'end': 654.42, 'start': 652.62, 'text': ' 너 그거 돌려, 돌려, 돌려!'}, {'end': 656.22, 'start': 655.02, 'text': ' 일단 새우만 넣어보겠습니다.'}, {'end': 657.32, 'start': 656.22, 'text': ' 우와.'}, {'end': 660.02, 'start': 657.32, 'text': ' 짬배는 삼겹살이랑 새우.'}, {'end': 661.52, 'start': 660.02, 'text': ' 짬배는 부추.'}, {'end': 663.52, 'start': 661.52, 'text': ' 부추.'}, {'end': 664.72, 'start': 663.52, 'text': ' 벌칙인가요?'}, {'end': 666.02, 'start': 664.72, 'text': ' 아니, 일단 짬만 들어가면 괜찮아요.'}, {'end': 666.72, 'start': 666.02, 'text': ' 짬만 들어가면.'}, {'end': 669.02, 'start': 666.72, 'text': ' 달기에 또 의미가 있다.'}, {'end': 671.72, 'start': 669.02, 'text': ' 와, 기관총.'}, {'end': 673.22, 'start': 671.72, 'text': ' 야, 이게 뭐야.'}, {'end': 675.02, 'start': 673.22, 'text': ' 하하하하하하.'}, {'end': 676.82, 'start': 675.02, 'text': ' 슉창.'}, {'end': 677.42, 'start': 676.82, 'text': ' 슉창.'}, {'end': 679.02, 'start': 677.42, 'text': ' 오오오, 깊게 들어갔어요.'}, {'end': 683.02, 'start': 682.22, 'text': ' 아, 괜찮네?'}, {'end': 683.72, 'start': 683.02, 'text': ' 어, 괜찮네?'}, {'end': 684.02, 'start': 683.72, 'text': ' 어.'}, {'end': 685.52, 'start': 684.02, 'text': ' 노랭이 한번 줘, 노랭이 한번 줘.'}, {'end': 687.02, 'start': 685.52, 'text': ' 으음.'}, {'end': 688.02, 'start': 687.02, 'text': ' 저거 반탕인데.'}, {'end': 689.52, 'start': 688.02, 'text': ' 하하하하하하.'}, {'end': 690.52, 'start': 689.52, 'text': ' 어, 김이 없네?'}, {'end': 691.52, 'start': 690.52, 'text': ' 아, 맞네.'}, {'end': 692.52, 'start': 691.52, 'text': ' 더 안 걸려도 되겠다.'}, {'end': 694.02, 'start': 692.52, 'text': ' 야, 그럼 참치랑 비벼.'}, {'end': 695.02, 'start': 694.02, 'text': ' 좋은 밥에.'}, {'end': 696.52, 'start': 695.02, 'text': ' 오, 노랭이 비빔밥?'}, {'end': 698.52, 'start': 696.52, 'text': ' 와, 진짜 알차게.'}, {'end': 699.02, 'start': 698.52, 'text': ' 아, 여러분.'}, {'end': 700.52, 'start': 699.02, 'text': ' 떡볶이가 다 됐습니다.'}, {'end': 701.52, 'start': 700.52, 'text': ' 그랬어.'}, {'end': 702.52, 'start': 701.52, 'text': ' 할, 할.'}, {'end': 705.02, 'start': 702.52, 'text': ' 아까 강우이가 양감장도 직접 만들더라고.'}, {'end': 705.52, 'start': 705.02, 'text': ' 네.'}, {'end': 707.02, 'start': 705.52, 'text': ' 와, 이거 진짜 맛있겠다.'}, {'end': 707.52, 'start': 707.02, 'text': ' 진짜 맛있겠다.'}, {'end': 710.52, 'start': 707.52, 'text': ' 시중에 있는 레시피를 조금 참고해서'}, {'end': 712.02, 'start': 710.52, 'text': ' 제 식대로 한번 바꿔봤습니다.'}, {'end': 712.52, 'start': 712.02, 'text': ' 와.'}, {'end': 714.52, 'start': 714.02, 'text': ' 네.'}, {'end': 715.02, 'start': 714.52, 'text': ' 한...'}, {'end': 743.52, 'start': 715.02, 'text': ' 한...'}, {'end': 746.58, 'start': 744.02, 'text': ' 김밥이랑 같이 찍어 먹는 걸 좀 생각했습니다.'}, {'end': 749.36, 'start': 746.58, 'text': ' 오묘한 육향이 느껴지는데 어디서 낸 겁니까, 이거?'}, {'end': 750.78, 'start': 749.36, 'text': ' 아주 정확하구만.'}, {'end': 753.56, 'start': 750.78, 'text': ' 짱배가 구운 삼겹살에 기름을 좀 달아주신 것 같습니다.'}, {'end': 754.64, 'start': 753.56, 'text': ' 아, 그래.'}, {'end': 755.94, 'start': 754.64, 'text': ' 버섯 캐치한다고?'}, {'end': 756.9, 'start': 755.94, 'text': ' 야, 오늘 아들하고.'}, {'end': 758.1, 'start': 756.9, 'text': ' 가래다 물을 부쳐야 돼.'}, {'end': 758.9, 'start': 758.1, 'text': ' 이렇게.'}, {'end': 760.14, 'start': 758.9, 'text': ' 참치김밥.'}, {'end': 762.9, 'start': 760.14, 'text': ' 이거 고기 송수 찍어가지고.'}, {'end': 763.9, 'start': 762.9, 'text': ' 뭐야, 뭐야.'}, {'end': 770.44, 'start': 768.9, 'text': ' 야, 가훈이 너무 맛있다, 이거.'}, {'end': 771.88, 'start': 770.44, 'text': ' 찍어 먹으니까 좀 괜찮다.'}, {'end': 773.28, 'start': 771.88, 'text': ' 자, 잘라 놓은 것도 좀 드시고.'}, {'end': 774.28, 'start': 773.28, 'text': ' 감사해요.'}, {'end': 775.28, 'start': 774.28, 'text': ' 우와.'}, {'end': 777.28, 'start': 775.28, 'text': ' 평소에 강훈이가 이런 거 잘 안 해주는데.'}, {'end': 777.98, 'start': 777.28, 'text': ' 생일인 게 뭐야?'}, {'end': 779.28, 'start': 777.98, 'text': ' 생일이잖아, 친구가.'}, {'end': 781.28, 'start': 779.28, 'text': ' 그러니까, 감동받을 것 같다.'}, {'end': 782.78, 'start': 781.28, 'text': ' 친구 생일에만 있는 특별 서비스.'}, {'end': 784.28, 'start': 782.78, 'text': ' 내일부터 이런 거 없어.'}, {'end': 786.28, 'start': 785.28, 'text': ' 특별.'}, {'end': 789.78, 'start': 788.28, 'text': ' 먹고 얘기해, 먹고 얘기해.'}, {'end': 790.78, 'start': 789.78, 'text': ' 강훈이가 특별.'}, {'end': 792.28, 'start': 790.78, 'text': ' 땡땡땡이라고 했어.'}, {'end': 793.28, 'start': 792.28, 'text': ' 아, 특별.'}, {'end': 794.78, 'start': 793.28, 'text': ' 아, 이벤트라고 했다고.'}, {'end': 799.28, 'start': 797.78, 'text': ' 조직은 규칙이잖아.'}, {'end': 801.28, 'start': 800.28, 'text': ' 다 굳었어.'}, {'end': 805.28, 'start': 803.28, 'text': ' 오, 맛있다.'}, {'end': 807.28, 'start': 805.28, 'text': ' 여러분들, 잊지 말고 여기 주먹밥도 있어요.'}, {'end': 808.28, 'start': 807.28, 'text': ' 아, 맞아.'}, {'end': 809.28, 'start': 808.28, 'text': ' 근데 왜 그게 달라?'}, {'end': 810.28, 'start': 809.28, 'text': ' 점점 갈수록.'}, {'end': 813.28, 'start': 812.28, 'text': ' 너무 가늘어.'}, {'end': 814.28, 'start': 813.28, 'text': ' 싱거울 수도 있어.'}, {'end': 815.28, 'start': 814.28, 'text': ' 응.'}, {'end': 817.28, 'start': 815.28, 'text': ' 이럴 때 떡볶이를 같이 먹으면.'}, {'end': 818.28, 'start': 817.28, 'text': ' 완벽해.'}, {'end': 819.28, 'start': 818.28, 'text': ' 중간 크기.'}, {'end': 822.28, 'start': 820.28, 'text': ' 주먹밥은 주먹만 해야지.'}, {'end': 825.28, 'start': 824.28, 'text': ' 잘한다.'}, {'end': 826.28, 'start': 825.28, 'text': ' 음.'}, {'end': 827.28, 'start': 826.28, 'text': ' 거의 맨밥.'}, {'end': 832.28, 'start': 829.28, 'text': ' 먹으면서 우리 노랭이 미담 한 개씩만 하자.'}, {'end': 833.28, 'start': 832.28, 'text': ' 아.'}, {'end': 834.28, 'start': 833.28, 'text': ' 미담?'}, {'end': 835.28, 'start': 834.28, 'text': ' 응.'}, {'end': 836.28, 'start': 835.28, 'text': ' 그럼 선물로 가자.'}, {'end': 840.28, 'start': 839.28, 'text': ' 선물 하나씩 주면서.'}, {'end': 841.28, 'start': 840.28, 'text': ' 응.'}, {'end': 842.28, 'start': 841.28, 'text': ' 어, 미담 하나씩.'}, {'end': 843.28, 'start': 842.28, 'text': ' 난 그런 거 좋아.'}, {'end': 846.28, 'start': 844.28, 'text': ' 자, 누구부터 선물 증정할까요?'}, {'end': 848.28, 'start': 846.28, 'text': ' 어, 제가 먼저 하겠습니다.'}, {'end': 851.28, 'start': 848.28, 'text': ' 저는 3초 두바이 초콜릿 준비했습니다.'}, {'end': 853.28, 'start': 851.28, 'text': ' 그냥 두바이 초콜릿도 아니고.'}, {'end': 854.28, 'start': 853.28, 'text': ' 그게 뭔데?'}, {'end': 856.28, 'start': 854.28, 'text': ' 뭔가 3초 동안 두바이 초콜릿을 보입니다.'}, {'end': 857.28, 'start': 856.28, 'text': ' 짜잔.'}, {'end': 858.28, 'start': 857.28, 'text': ' 어? 우와.'}, {'end': 860.28, 'start': 858.28, 'text': ' 게임의 왓츠 젤다.'}, {'end': 861.28, 'start': 860.28, 'text': ' 원래 시계인데.'}, {'end': 862.28, 'start': 861.28, 'text': ' 어.'}, {'end': 864.28, 'start': 862.28, 'text': ' 이 안에 게임이 내장되어 있어.'}, {'end': 867.28, 'start': 864.28, 'text': ' 제가 이거 마리오 버전이 있는데 살까 말까 고민 많이 했던 건데.'}, {'end': 868.28, 'start': 867.28, 'text': ' 우와.'}, {'end': 869.28, 'start': 868.28, 'text': ' 대박.'}, {'end': 873.28, 'start': 869.28, 'text': ' 이 안에 젤다 1, 젤다 2, 젤다 어웨이킹까지 있습니다.'}, {'end': 874.28, 'start': 873.28, 'text': ' 아.'}, {'end': 875.28, 'start': 874.28, 'text': ' 바로 뜯어봐야겠다.'}, {'end': 876.28, 'start': 875.28, 'text': ' 어머.'}, {'end': 877.28, 'start': 876.28, 'text': ' 이 외래어 쓰는 건 인정하는 부분이죠?'}, {'end': 878.28, 'start': 877.28, 'text': ' 아.'}, {'end': 879.28, 'start': 878.28, 'text': ' 이제 끝났어.'}, {'end': 880.28, 'start': 879.28, 'text': ' 끝났어.'}, {'end': 881.28, 'start': 880.28, 'text': ' 아.'}, {'end': 882.28, 'start': 881.28, 'text': ' 끝났어.'}, {'end': 883.28, 'start': 882.28, 'text': ' 오케이.'}, {'end': 884.28, 'start': 883.28, 'text': ' 오케이.'}, {'end': 885.28, 'start': 884.28, 'text': ' 오케이.'}, {'end': 886.28, 'start': 885.28, 'text': ' 오케이.'}, {'end': 887.28, 'start': 886.28, 'text': ' 오케이.'}, {'end': 888.28, 'start': 887.28, 'text': ' 오케이.'}, {'end': 889.28, 'start': 888.28, 'text': ' 오케이.'}, {'end': 890.28, 'start': 889.28, 'text': ' 오케이.'}, {'end': 891.28, 'start': 890.28, 'text': ' 오케이.'}, {'end': 892.28, 'start': 891.28, 'text': ' 오케이.'}, {'end': 893.28, 'start': 892.28, 'text': ' 오케이.'}, {'end': 894.28, 'start': 893.28, 'text': ' 오케이.'}, {'end': 895.28, 'start': 894.28, 'text': ' 오케이.'}, {'end': 896.28, 'start': 895.28, 'text': ' 오케이.'}, {'end': 897.28, 'start': 896.28, 'text': ' 오케이.'}, {'end': 898.28, 'start': 897.28, 'text': ' 오케이.'}, {'end': 899.28, 'start': 898.28, 'text': ' 오케이.'}, {'end': 900.28, 'start': 899.28, 'text': ' 오케이.'}, {'end': 901.28, 'start': 900.28, 'text': ' 오케이.'}, {'end': 902.28, 'start': 901.28, 'text': ' 오케이.'}, {'end': 903.28, 'start': 902.28, 'text': ' 오케이.'}, {'end': 904.28, 'start': 903.28, 'text': ' 옢.'}, {'end': 905.28, 'start': 904.28, 'text': '东 경우 집에 मrafly 포장되어 있어요.'}, {'end': 906.28, 'start': 905.28, 'text': ' 오.'}, {'end': 907.28, 'start': 906.28, 'text': ' 오.'}, {'end': 908.28, 'start': 907.28, 'text': ' 오!'}, {'end': 909.28, 'start': 908.28, 'text': ' 오!'}, {'end': 910.28, 'start': 909.28, 'text': ' 오!'}, {'end': 911.28, 'start': 910.28, 'text': ' 오!'}, {'end': 912.28, 'start': 911.28, 'text': 'fed....'}, {'end': 913.28, 'start': 912.28, 'text': ' 드디어.'}, {'end': 914.28, 'start': 913.28, 'text': ' 오!'}, {'end': 915.28, 'start': 914.28, 'text': ' 오네� 04.'}, {'end': 916.28, 'start': 915.28, 'text': ' pussy naj.'}, {'end': 917.28, 'start': 916.28, 'text': ' 오!'}, {'end': 918.28, 'start': 917.28, 'text': ' 오!'}, {'end': 919.28, 'start': 918.28, 'text': ' 오.'}, {'end': 920.28, 'start': 919.28, 'text': ' 하!'}, {'end': 921.42, 'start': 920.28, 'text': ' 아, 감사합니다.'}, {'end': 924.1, 'start': 921.96, 'text': ' 저는 뭐 근데 귀담이라고..'}, {'end': 929.08, 'start': 924.1, 'text': ' 노랭이가 약간 좀 은근히 알게 모르게 꿀팁이나 이런 거 각종 정보들 알려주거든.'}, {'end': 931.04, 'start': 929.08, 'text': ' 원래 그런 거 넘어갈 수도 있잖아.'}, {'end': 932.74, 'start': 931.04, 'text': ' 제로 비락시켜 세일한다 이거.'}, {'end': 935.72, 'start': 932.74, 'text': ' 사이즈 사게 살 수 있다 링크 막 올려주고.'}, {'end': 939.72, 'start': 935.72, 'text': ' 그리고 특히나 저는 노랭이한테 약간 이런 각종 물건들 막 이렇게 막'}, {'end': 942.32, 'start': 939.72, 'text': ' 거의 흙값에 막 이렇게 구매한 적도 많습니다.'}, {'end': 943.68, 'start': 942.32, 'text': ' 아, 맞아. 막 저렴하게.'}, {'end': 944.88, 'start': 943.68, 'text': ' 노랭이가 쓰던 에어팟 프로.'}, {'end': 948.68, 'start': 944.88, 'text': ' 그리고 또 막 그 팝플 영상에도 좀 자주 막 나왔었던 온클레어 패딩.'}, {'end': 951.36, 'start': 948.68, 'text': ' 일상에서 그런 소소한 뭐랄까 도움들을 진짜 많이 줘요.'}, {'end': 954.44, 'start': 951.36, 'text': ' 맞아요. 저희 중에서도 가장 정보검색왕이거든요.'}, {'end': 957.06, 'start': 954.44, 'text': ' 단순히 그냥 정보가 많다는 게 아니야.'}, {'end': 960.98, 'start': 957.06, 'text': ' 학창시 시절에 정보검색능력대회에서 장교사가 나왔어.'}, {'end': 961.86, 'start': 960.98, 'text': ' 맞아요, 맞아요.'}, {'end': 963.92, 'start': 961.86, 'text': ' 아, 장교사는 뭔가 좀 고민되는..'}, {'end': 965.26, 'start': 963.92, 'text': ' 아니야, 상반되는 거야.'}, {'end': 966.22, 'start': 965.26, 'text': ' 고민되는..'}, {'end': 967.12, 'start': 966.22, 'text': ' 자, 그런 의미에서..'}, {'end': 970.76, 'start': 967.12, 'text': ' 지갑을 지켜줄 수 있지는 않지만 그래도'}, {'end': 973.36, 'start': 970.76, 'text': ' 양으로 승부하겠습니다.'}, {'end': 974.58, 'start': 973.36, 'text': ' 일단 이거..'}, {'end': 977.78, 'start': 976.68, 'text': ' 가면라이더!'}, {'end': 978.66, 'start': 977.78, 'text': ' 가면라이더!'}, {'end': 982.46, 'start': 978.66, 'text': ' 이건 최근에 팝플이 굉장히 재밌게 봤었던 뽑기로 뽑았는데'}, {'end': 983.74, 'start': 982.46, 'text': ' 이렇게 노랭이 생각이 나서'}, {'end': 984.58, 'start': 983.74, 'text': ' 축하해.'}, {'end': 986.14, 'start': 984.58, 'text': ' 아, 진짜?'}, {'end': 988.46, 'start': 986.14, 'text': ' A상이면 엄청 뽑기 힘든 거거든.'}, {'end': 990.02, 'start': 988.46, 'text': ' 한 번 뽑았는데 이렇게 나왔어.'}, {'end': 990.62, 'start': 990.02, 'text': ' 우와!'}, {'end': 993, 'start': 990.62, 'text': ' 근데 뽑기 한 번에 만 원, 이만 원 하잖아.'}, {'end': 996.26, 'start': 993, 'text': ' 그리고 중요한 의미는 저 물건이 뽑기가 아니면 구할 수가 없는 물건이니까.'}, {'end': 997.62, 'start': 996.26, 'text': ' 맞아, 맞아.'}, {'end': 1000.32, 'start': 997.62, 'text': ' 난 뽑기로 뽑은 걸 주기에는 또 그럴 것 같아서'}, {'end': 1001.42, 'start': 1000.32, 'text': ' 또 따로 추가로..'}, {'end': 1002.54, 'start': 1001.42, 'text': ' 아니, 와..'}, {'end': 1003.56, 'start': 1002.54, 'text': ' 네..'}, {'end': 1004.82, 'start': 1003.56, 'text': ' 이건 뭐야?'}, {'end': 1008.66, 'start': 1004.82, 'text': ' 이거는 이제 노랭이가 굉장히 좋아하는 게임 중에 하나죠.'}, {'end': 1010.42, 'start': 1008.66, 'text': ' 승리의 여신.'}, {'end': 1011.06, 'start': 1010.42, 'text': ' 오!'}, {'end': 1013.82, 'start': 1011.06, 'text': ' 제가 얼마 전에 또 팝업스토어까지 다녀왔는데'}, {'end': 1017.66, 'start': 1013.82, 'text': ' 이거는 평범한 팝업스토어의 굿즈가 있는데'}, {'end': 1020.04, 'start': 1017.66, 'text': ' 잠깐만, 방송에 나갈 수 있는 거야?'}, {'end': 1022.7, 'start': 1020.04, 'text': ' 야, 일단 봐봐, 이거 검열 될까, 이거?'}, {'end': 1024.7, 'start': 1022.7, 'text': ' 괜찮을까?'}, {'end': 1025.9, 'start': 1024.7, 'text': ' 나는 괜찮을 것 같은데?'}, {'end': 1026.82, 'start': 1025.9, 'text': ' 오, 잠깐만.'}, {'end': 1028.5, 'start': 1026.82, 'text': ' 자세히 보지 말고.'}, {'end': 1029.82, 'start': 1028.5, 'text': ' 아, 잠깐만.'}, {'end': 1031.9, 'start': 1029.82, 'text': ' 좀 더 검열을 많이 해야 될 것 같아요.'}, {'end': 1033.82, 'start': 1031.9, 'text': ' 이 캐릭터로 가려가지고.'}, {'end': 1036.66, 'start': 1033.82, 'text': ' 아, 이 캐릭터!'}, {'end': 1037.66, 'start': 1036.66, 'text': ' 아, 캐릭터!'}, {'end': 1038.5, 'start': 1037.66, 'text': ' 앨리스!'}, {'end': 1039.7, 'start': 1038.5, 'text': ' 이게 제가 좋아하는 캐릭터입니다.'}, {'end': 1041, 'start': 1039.7, 'text': ' 아, 맞아요. 아, 노랭이가 또 봤어.'}, {'end': 1044.2, 'start': 1041, 'text': ' 위키의 출시 1주년 기념 후쿠오카.'}, {'end': 1044.8, 'start': 1044.2, 'text': ' 가정입니까?'}, {'end': 1045.46, 'start': 1044.8, 'text': ' 네, 가정입니다.'}, {'end': 1046.8, 'start': 1045.46, 'text': ' 우와!'}, {'end': 1050.2, 'start': 1046.8, 'text': ' 아, 근데 안 뜯고 이렇게 보관하고 싶다.'}, {'end': 1051.2, 'start': 1050.2, 'text': ' 아, 이거 궁금하네요, 개인적으로.'}, {'end': 1052.7, 'start': 1051.2, 'text': ' 와, 이거 꽤 크더라고요.'}, {'end': 1053.7, 'start': 1052.7, 'text': ' 이게 모공품입니다.'}, {'end': 1055.9, 'start': 1053.7, 'text': ' 우와!'}, {'end': 1056.7, 'start': 1055.9, 'text': ' 대박!'}, {'end': 1057.5, 'start': 1056.7, 'text': ' 크네요?'}, {'end': 1060, 'start': 1057.5, 'text': ' 후독이들이 우리가 나이에 안 맞다고 생각하실 수도 있는데'}, {'end': 1061.5, 'start': 1060, 'text': ' 저희 특철물 되게 좋아합니다.'}, {'end': 1064.2, 'start': 1061.5, 'text': ' 새로운 변신 나올 때마다 다 같이'}, {'end': 1065, 'start': 1064.2, 'text': ' 우와!'}, {'end': 1065.8, 'start': 1065, 'text': ' 이러고 서거든요.'}, {'end': 1066.5, 'start': 1065.8, 'text': ' 새로운 슈트.'}, {'end': 1070.2, 'start': 1068.5, 'text': ' 예!'}, {'end': 1071.2, 'start': 1070.2, 'text': ' 퀄리티 뭐야?'}, {'end': 1072.2, 'start': 1071.2, 'text': ' 우와!'}, {'end': 1074, 'start': 1072.2, 'text': ' 오, 내가 한 가지 느낀 점.'}, {'end': 1076.3, 'start': 1074, 'text': ' 먼저 하길 잘했다.'}, {'end': 1078, 'start': 1076.3, 'text': ' 쿠오카의 그다음은 누굽니까?'}, {'end': 1079.3, 'start': 1078, 'text': ' 짜오, 그럼 하나.'}, {'end': 1080.3, 'start': 1079.3, 'text': ' 어, 일단 미담.'}, {'end': 1081.8, 'start': 1080.3, 'text': ' 어, 짜오의 미담, 짜오.'}, {'end': 1084.3, 'start': 1081.8, 'text': ' 일단 노랭이는 영화광입니다, 영화광.'}, {'end': 1086.2, 'start': 1084.3, 'text': ' 포스터도 모으고 하다 보니까'}, {'end': 1088.5, 'start': 1086.2, 'text': ' 포스터를 받기 위해 보는 영화도 있어.'}, {'end': 1091.7, 'start': 1088.5, 'text': ' 아, 근데 그런 와중에 또 작품성 있는 영화들도 되게 많이 보기 때문에'}, {'end': 1094.3, 'start': 1091.7, 'text': ' 그런 거 있으면 저한테 꼭 한번 물어봅니다, 거의.'}, {'end': 1095.5, 'start': 1094.3, 'text': ' 추천해 줍니다, 제가.'}, {'end': 1097.5, 'start': 1095.5, 'text': ' 최근에 위대한 쇼맨을 재고용을 했는데.'}, {'end': 1098.5, 'start': 1097.5, 'text': ' 최근에 위대한 쇼맨을 재고용을 했는데.'}, {'end': 1101.8, 'start': 1098.5, 'text': ' 돌비 시네마에서 한다고 이렇게 알려줘가지고'}, {'end': 1103.8, 'start': 1101.8, 'text': ' 여자친구가 진짜 재밌게 보고.'}, {'end': 1106.8, 'start': 1103.8, 'text': ' 여자친구가 진짜 노랭이한테 감사했어, 그거.'}, {'end': 1110.9, 'start': 1106.8, 'text': ' 음악 영화들이 돌비 시네마에서 보면 진짜 그 울림이 달라.'}, {'end': 1112.5, 'start': 1110.9, 'text': ' 전 꼭 추천해 주고 싶었거든.'}, {'end': 1114.1, 'start': 1112.5, 'text': ' 이런 거를 저도 좋아하는데'}, {'end': 1116.3, 'start': 1114.1, 'text': ' 항상 그런 정보 찾는 거에 대해서 미숙했는데'}, {'end': 1118, 'start': 1116.3, 'text': ' 노랭이가 있으면 알려주고'}, {'end': 1120.5, 'start': 1118, 'text': ' 가서 보면 또 좋아해서 난 또 감사해고.'}, {'end': 1125.8, 'start': 1120.5, 'text': ' 근데 다들 여기는 티켓팅, 여기는 물건 구매할 때, 여기 영화 볼 때.'}, {'end': 1128.3, 'start': 1125.8, 'text': ' 이거는 저희 노랭이 하는 거잖아.'}, {'end': 1131.1, 'start': 1128.3, 'text': ' 회의는 상품을 받아라.'}, {'end': 1134.4, 'start': 1131.1, 'text': ' 이런 것 좀 이제 같이 나누면 나도 좋겠네.'}, {'end': 1137.6, 'start': 1134.4, 'text': ' 선물은 뭐 그렇게 큰 의미가 있는 건 아닙니다만'}, {'end': 1139, 'start': 1137.6, 'text': ' 하지만 만드는 거 좋아하는데.'}, {'end': 1141.4, 'start': 1139, 'text': ' 오, 커.'}, {'end': 1144.5, 'start': 1141.4, 'text': ' 이런 거는 본 적이 없을 거예요.'}, {'end': 1146.6, 'start': 1144.5, 'text': ' 야, 이게 뭐야?'}, {'end': 1149.9, 'start': 1146.6, 'text': ' 퍼즐인데 그 종이로 돼 있는 거.'}, {'end': 1151.5, 'start': 1149.9, 'text': ' 근데 이게 중요한 거는 저'}, {'end': 1153.8, 'start': 1151.5, 'text': ' 이제 귀엽 파츠를 쓰면 자동으로 돌아가.'}, {'end': 1155.4, 'start': 1153.8, 'text': ' 아, 진짜?'}, {'end': 1156.9, 'start': 1155.4, 'text': ' 이런 거 좋아해가지고'}, {'end': 1158.3, 'start': 1156.9, 'text': ' 그 옛날에 초코파이였나?'}, {'end': 1159.6, 'start': 1158.3, 'text': ' 그 상품 들어있었어요.'}, {'end': 1161.2, 'start': 1159.6, 'text': ' 그렇지, 그렇지.'}, {'end': 1162, 'start': 1161.2, 'text': ' 코빌, 코빌.'}, {'end': 1163.2, 'start': 1162, 'text': ' 코빌인가?'}, {'end': 1166.2, 'start': 1163.2, 'text': ' 야, 근데 첫 프라이 치고는 난이도가 좀 높다.'}, {'end': 1167.4, 'start': 1166.2, 'text': ' 아, 얘 있어, 얘 있어.'}, {'end': 1169, 'start': 1167.4, 'text': ' 난이도가 있잖아.'}, {'end': 1170.3, 'start': 1169, 'text': ' 난이도 일단 별 4개.'}, {'end': 1171.5, 'start': 1170.3, 'text': ' 이거 주는 의미는'}, {'end': 1174.9, 'start': 1171.5, 'text': ' 이 밑에 보면 이게 세계적인 약간 명소들이 있어.'}, {'end': 1176.5, 'start': 1174.9, 'text': ' 아, 핸드마크 같은 거.'}, {'end': 1178.2, 'start': 1176.5, 'text': ' 그래서 혹시나 만약에 여기 들릴 일 있으면'}, {'end': 1180.3, 'start': 1178.2, 'text': ' 거기 물품 사와서 여기 보관하고 뭐 그런.'}, {'end': 1181.6, 'start': 1180.3, 'text': ' 아, 그런 거.'}, {'end': 1183.6, 'start': 1181.6, 'text': ' 진짜 저거 채우는 재미가 있겠다.'}, {'end': 1184.6, 'start': 1183.6, 'text': ' 아, 감사합니다.'}, {'end': 1187.1, 'start': 1184.6, 'text': ' 우와.'}, {'end': 1188.1, 'start': 1187.1, 'text': ' 야, 이거.'}, {'end': 1189.6, 'start': 1188.3, 'text': ' 마지막 선물 기쁘신 줄 알았는데.'}, {'end': 1190.5, 'start': 1189.6, 'text': ' 야, 제일.'}, {'end': 1192.8, 'start': 1190.5, 'text': ' 아니, 뭐 그런 건 아니지, 아니지.'}, {'end': 1193.8, 'start': 1192.8, 'text': ' 얼굴 빨개진다.'}, {'end': 1195.9, 'start': 1193.8, 'text': ' 내가 선물을 잘 할 줄은 몰라.'}, {'end': 1197, 'start': 1195.9, 'text': ' 고민을 좀 했죠.'}, {'end': 1198, 'start': 1197, 'text': ' 뭘 해야 되나.'}, {'end': 1199.2, 'start': 1198, 'text': ' 고민을 하다가'}, {'end': 1201.7, 'start': 1199.2, 'text': ' 더불어 약간 노랭이라는 인간에 대해서'}, {'end': 1203.4, 'start': 1201.7, 'text': ' 생각을 해봤습니다.'}, {'end': 1204.8, 'start': 1203.4, 'text': ' 이 친구가 없었더라면'}, {'end': 1207.3, 'start': 1204.8, 'text': ' 제 인생이 좀 많이 달라지지 않았을까.'}, {'end': 1208.2, 'start': 1207.3, 'text': ' 그런 생각을 해서'}, {'end': 1210.1, 'start': 1208.2, 'text': ' 사실 노랭이와의 미담'}, {'end': 1211.8, 'start': 1210.1, 'text': ' 없어요.'}, {'end': 1214.4, 'start': 1211.8, 'text': ' 그냥 노랭이와 함께 살아온 인생이'}, {'end': 1216.3, 'start': 1214.4, 'text': ' 네 생이었다.'}, {'end': 1217.7, 'start': 1216.3, 'text': ' 아름다운 인생.'}, {'end': 1219.5, 'start': 1217.7, 'text': ' 노랭이를 처음 만났던 그날'}, {'end': 1223, 'start': 1219.5, 'text': ' 방송부에서 웬 도실도실한 아이가 한 명 있어가지고'}, {'end': 1225.1, 'start': 1223, 'text': ' 나는 1학년 5반의 박강훈이다.'}, {'end': 1226.9, 'start': 1225.1, 'text': ' 시작.'}, {'end': 1228.7, 'start': 1226.9, 'text': ' 저 멘트가 특이했어.'}, {'end': 1231.1, 'start': 1228.7, 'text': ' 보통 고등학생 때 친구 만나면'}, {'end': 1233.1, 'start': 1231.1, 'text': ' 어, 반갑다 이렇게만 하지.'}, {'end': 1234, 'start': 1233.1, 'text': ' 갑자기 와가지고'}, {'end': 1236.4, 'start': 1234, 'text': ' 어, 나는 1학년 5반의 박강훈이다.'}, {'end': 1238.5, 'start': 1236.4, 'text': ' 그냥 외국식 인사인데.'}, {'end': 1240.7, 'start': 1238.5, 'text': ' 이 역사가 아주 힙합이다.'}, {'end': 1241.9, 'start': 1240.7, 'text': ' 전설의 시작이었네요.'}, {'end': 1243.5, 'start': 1241.9, 'text': ' 전설의 시작.'}, {'end': 1245.3, 'start': 1243.5, 'text': ' 지가 지 입으로.'}, {'end': 1247.5, 'start': 1245.3, 'text': ' 전설의 시작이었잖아.'}, {'end': 1250.7, 'start': 1247.7, 'text': ' 옛날에 책 인터뷰에서 쓴 거 있잖아.'}, {'end': 1253.4, 'start': 1250.7, 'text': ' 그 내용을 내가 한번 봤거든. 얘 파트를.'}, {'end': 1255.5, 'start': 1253.4, 'text': ' 얘가 팥뿌리를 너무 좋아합니다.'}, {'end': 1260.1, 'start': 1255.5, 'text': ' 우리를 만난 거를 자기 인생에서 복이라고 생각하는데'}, {'end': 1263.2, 'start': 1260.1, 'text': ' 얘가 원래 겉으로 그런 표현을 잘 안 한단 말이야.'}, {'end': 1266.5, 'start': 1263.2, 'text': ' 다 같이 하는 활동들이 좀 내가 좋아하는 스타일이었거든.'}, {'end': 1267.9, 'start': 1266.5, 'text': ' 게임하고 만들고'}, {'end': 1268.5, 'start': 1267.9, 'text': ' 맞아 맞아.'}, {'end': 1269.9, 'start': 1268.5, 'text': ' 그러는 게 좋더라고.'}, {'end': 1273.1, 'start': 1269.9, 'text': ' 결국에 그렇게 놀던 게 팥뿌리가 되었다.'}, {'end': 1276.9, 'start': 1273.1, 'text': ' 그래서 그냥 요즘에 노랭이가 집에 갈 때 좀 걸어가요.'}, {'end': 1281.7, 'start': 1277.7, 'text': ' 그러면서 평소보다 많이 쓰는 게 에어콘이에요.'}, {'end': 1284.9, 'start': 1281.7, 'text': ' 하루에 1시간 이상은 꼭 이걸 끼고 있더라고.'}, {'end': 1288.7, 'start': 1284.9, 'text': ' 그래서 설마 좋은 소리 들으라고?'}, {'end': 1291.7, 'start': 1288.7, 'text': ' 내가 어제 쓴 이 프로라는 거.'}, {'end': 1293.7, 'start': 1291.7, 'text': ' 와, 소리 너무 많이 나온 거 아이가.'}, {'end': 1294.7, 'start': 1293.7, 'text': ' 스노우보이터.'}, {'end': 1295.7, 'start': 1294.7, 'text': ' 스노우보이터.'}, {'end': 1296.7, 'start': 1295.7, 'text': ' 스노우보이터.'}, {'end': 1301.7, 'start': 1300.7, 'text': ' 고급스럽다.'}, {'end': 1306.7, 'start': 1305.7, 'text': ' 네 목소리만 들려.'}, {'end': 1312.7, 'start': 1307.7, 'text': ' 주변 소리 듣기 하니까 너희 목소리가 그냥 암기 것처럼 잘 들려.'}, {'end': 1313.7, 'start': 1312.7, 'text': ' 비교해봐.'}, {'end': 1315.7, 'start': 1313.7, 'text': ' 와 와 와 와 와.'}, {'end': 1317.7, 'start': 1315.7, 'text': ' 나만 켜.'}, {'end': 1318.7, 'start': 1317.7, 'text': ' 안 들려.'}, {'end': 1320.7, 'start': 1318.7, 'text': ' 우와.'}, {'end': 1322.7, 'start': 1320.7, 'text': ' 우와 하는 건 들려.'}, {'end': 1323.7, 'start': 1322.7, 'text': ' 우와 하는 건 들려.'}, {'end': 1324.7, 'start': 1323.7, 'text': ' 신기하다.'}, {'end': 1325.7, 'start': 1324.7, 'text': ' 신기하다.'}, {'end': 1327.7, 'start': 1325.7, 'text': ' 말소리가 아예 다 차단되는 건 아닌가 봐.'}, {'end': 1329.7, 'start': 1327.7, 'text': ' 적당히.'}, {'end': 1330.7, 'start': 1329.7, 'text': ' 와.'}, {'end': 1331.7, 'start': 1330.7, 'text': ' 와.'}, {'end': 1332.7, 'start': 1331.7, 'text': ' 와.'}, {'end': 1334.7, 'start': 1332.7, 'text': ' 이쪽에는 또 음악이 하는 단지에 나온다.'}, {'end': 1335.7, 'start': 1334.7, 'text': ' 말 들리나?'}, {'end': 1336.7, 'start': 1335.7, 'text': ' 말 들리나?'}, {'end': 1337.7, 'start': 1336.7, 'text': ' 우리말이 들리나?'}, {'end': 1339.7, 'start': 1337.7, 'text': ' 자기만의 세계로 맞춰서.'}, {'end': 1342.7, 'start': 1339.7, 'text': ' 와 이 에어팟이랑 베이스가 좀 다른데?'}, {'end': 1343.7, 'start': 1342.7, 'text': ' 아 그래요?'}, {'end': 1344.7, 'start': 1343.7, 'text': ' 진짜.'}, {'end': 1345.7, 'start': 1344.7, 'text': ' 아 들어봐봐.'}, {'end': 1352.7, 'start': 1351.7, 'text': ' 진짜 안 들리네?'}, {'end': 1353.7, 'start': 1352.7, 'text': ' 안 들리지?'}, {'end': 1359.7, 'start': 1358.7, 'text': ' 감사합니다.'}, {'end': 1360.7, 'start': 1359.7, 'text': ' 축하합니다.'}, {'end': 1363.7, 'start': 1362.7, 'text': ' 감사합니다.'}, {'end': 1365.7, 'start': 1363.7, 'text': ' 너네도 그냥 알차게 맞춤형 선물로.'}, {'end': 1366.7, 'start': 1365.7, 'text': ' 친구들이.'}, {'end': 1368.7, 'start': 1366.7, 'text': ' 오 진짜 제일 단다.'}, {'end': 1369.7, 'start': 1368.7, 'text': ' 제일 단다.'}, {'end': 1370.7, 'start': 1369.7, 'text': ' 제일 단다.'}, {'end': 1371.7, 'start': 1370.7, 'text': ' 자 충전하고 왔습니다.'}, {'end': 1372.7, 'start': 1371.7, 'text': ' 켜볼까나.'}, {'end': 1373.7, 'start': 1372.7, 'text': ' 오 궁금해.'}, {'end': 1374.7, 'start': 1373.7, 'text': ' 오 제일 단 소리.'}, {'end': 1375.7, 'start': 1374.7, 'text': ' 이게 이제 시계 화면인 거고.'}, {'end': 1376.7, 'start': 1375.7, 'text': ' 그리고 이제 게임 버튼을 누르면 게임을 할 수 있습니다.'}, {'end': 1377.7, 'start': 1376.7, 'text': ' 와우 와우.'}, {'end': 1378.7, 'start': 1377.7, 'text': ' 아우 게임에 빠져들었어.'}, {'end': 1379.7, 'start': 1378.7, 'text': ' 그만해.'}, {'end': 1380.7, 'start': 1379.7, 'text': ' 그만해.'}, {'end': 1381.7, 'start': 1380.7, 'text': ' 그만해.'}, {'end': 1382.7, 'start': 1381.7, 'text': ' 마무리할게.'}, {'end': 1383.7, 'start': 1382.7, 'text': ' 그만해.'}, {'end': 1384.7, 'start': 1383.7, 'text': ' 그만해.'}, {'end': 1385.7, 'start': 1384.7, 'text': ' 그만해.'}, {'end': 1386.7, 'start': 1385.7, 'text': ' 그만해.'}, {'end': 1387.7, 'start': 1386.7, 'text': ' 그만해.'}, {'end': 1388.7, 'start': 1387.7, 'text': ' 그만해.'}, {'end': 1389.7, 'start': 1388.7, 'text': ' 그만해.'}, {'end': 1390.7, 'start': 1389.7, 'text': ' 그만해.'}, {'end': 1391.7, 'start': 1390.7, 'text': ' 그만해.'}, {'end': 1392.7, 'start': 1391.7, 'text': ' 그만해.'}, {'end': 1393.7, 'start': 1392.7, 'text': ' 그만해.'}, {'end': 1394.7, 'start': 1393.7, 'text': ' 그만해.'}, {'end': 1395.7, 'start': 1394.7, 'text': ' 그만해.'}, {'end': 1396.7, 'start': 1395.7, 'text': ' 그냥 끝나고.'}, {'end': 1397.7, 'start': 1396.7, 'text': ' 끝나고 이거 네고 먹으면서.'}, {'end': 1398.7, 'start': 1397.7, 'text': ' 끝나네.'}, {'end': 1399.7, 'start': 1398.7, 'text': ' 아 너무 감사합니다.'}, {'end': 1400.7, 'start': 1399.7, 'text': ' 재밌었어.'}, {'end': 1403.62, 'start': 1400.7, 'text': ' 네 어쨌든 또 우리 노랭이의 생일.'}, {'end': 1405.92, 'start': 1403.62, 'text': ' 우리 뽀둥이분들 많이많이 축하해 주시고.'}, {'end': 1408.22, 'start': 1405.92, 'text': ' 다음 생일 진열이 생일에.'}, {'end': 1409.22, 'start': 1408.22, 'text': ' 와.'}, {'end': 1410.56, 'start': 1409.22, 'text': ' 좀 많이 남았어 아직.'}, {'end': 1411.56, 'start': 1410.56, 'text': ' 네.'}, {'end': 1414.14, 'start': 1411.56, 'text': ' 그때 한번 어떤 파티를 해볼지.'}, {'end': 1415.48, 'start': 1414.14, 'text': ' 진열이가 한번 생각해 보시죠.'}, {'end': 1416.16, 'start': 1415.48, 'text': ' 제가 이렇게 놀아보자.'}, {'end': 1455.68, 'start': 1425.7, 'text': ' 시청해주셔서 감사합니다.'}], 'summary_result': \"- 🎉 오늘은 노랭이의 생일입니다!\\n- 🍣 노랭이가 가장 좋아하는 음식은 김밥입니다.\\n- 👩\\u200d🍳 우리는 노랭이의 생일을 기념하여 김밥을 만들기로 했습니다.\\n- 🥳 김밥의 맛보다 함께하는 시간이 더 중요하다고 이야기했습니다.\\n- 📜 생일 룰로 욕과 외래어 사용 금지가 정해졌습니다.\\n- 🍤 노랭이의 어머니가 만든 김밥을 그리워하며 준비했습니다.\\n- 🎁 친구들이 선물과 미담을 나누며 생일을 축하했습니다.\\n- 🍲 떡볶이와 함께 김밥을 즐기며 다 같이 재미있게 놀았습니다.\\n\\n- 🎉 Today is Norangi's birthday!\\n- 🍣 Norangi's favorite food is gimbap.\\n- 👩\\u200d🍳 We decided to make gimbap to celebrate Norangi's birthday.\\n- 🥳 We talked about the importance of time together over the taste of gimbap.\\n- 📜 The birthday rules included a ban on swearing and foreign words.\\n- 🍤 We prepared gimbap while reminiscing about Norangi's mother's cooking.\\n- 🎁 Friends shared gifts and compliments to celebrate the birthday.\\n- 🍲 We enjoyed gimbap along with tteokbokki and had fun together.\"}, 'status': 'COMPLETED', 'workerId': 'jxd93hs8fbpe4z'}\n",
      "Error fetching results: 404\n",
      "Error message: 404 page not found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/run\"\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/get_script_summary\",\n",
    "        \"method\": \"GET\",\n",
    "        \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"url\": you_url},\n",
    "    }\n",
    "}\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Initial Response:\", result)\n",
    "    \n",
    "    if result.get('status') in ['IN_PROGRESS',\"IN_QUEUE\"]:\n",
    "        job_id = result.get('id')\n",
    "        status_url = f\"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}\"\n",
    "        \n",
    "        while True:\n",
    "            status_response = requests.get(status_url, headers=headers)\n",
    "            if status_response.status_code == 200:\n",
    "                status_data = status_response.json()\n",
    "                print(f\"Current status: {status_data.get('status')}\")\n",
    "                \n",
    "                if status_data.get('status') == 'COMPLETED':\n",
    "                    print(f\"결과값:{status_data}\")\n",
    "                    result_url = f\"https://api.runpod.ai/v2/{endpoint_id}/result/{job_id}\"\n",
    "                    result_response = requests.get(result_url, headers=headers)\n",
    "                    \n",
    "                    if result_response.status_code == 200:\n",
    "                        final_result = result_response.json()\n",
    "                        print(\"Final Result:\", final_result)\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Error fetching results: {result_response.status_code}\")\n",
    "                        print(f\"Error message: {result_response.text}\")\n",
    "                        break\n",
    "                elif status_data.get('status') == 'FAILED':\n",
    "                    print(\"Job failed\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Error checking status: {status_response.status_code}\")\n",
    "                print(f\"Error message: {status_response.text}\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)  # 5초 대기 후 다시 상태 확인\n",
    "    else:\n",
    "        print(\"Job completed immediately\")\n",
    "        print(\"Final Result:\", result)\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}\"\n",
    "url2 = f\"https://api.runpod.ai/v2/{endpoint_id}/result/{job_id}\"\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "while True:\n",
    "    # RUNSYNC 요청 보내기\n",
    "    response = requests.get(url,headers=headers)\n",
    "\n",
    "    # 응답 확인\n",
    "    if response.status_code == 200:\n",
    "        if response.json().get(\"status\") == \"COMPLETED\":\n",
    "            response = requests.get(url2,headers=headers)\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Job status: {response.json().get('status')}\")\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'404 page not found'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "RUNPOD_API_URL = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/rag_stream_chat\",\n",
    "        \"method\": \"POST\",\n",
    "        \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"prompt\": \"영상의 주제가 뭔가요?\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    RUNPOD_API_URL, headers=headers, json=payload, stream=True\n",
    ")\n",
    "\n",
    "# for chunk in response.iter_content(chunk_size=None):\n",
    "#     if chunk:\n",
    "#         chunk_data = chunk.decode(\"utf-8\").strip()\n",
    "#         if chunk_data.startswith(\"data: \"):\n",
    "#             chunk_content = chunk_data[6:]\n",
    "#             if chunk_content == \"[DONE]\":\n",
    "#                 break\n",
    "#             try:\n",
    "#                 content = json.loads(chunk_content)\n",
    "#                 print(\"Stream content:\", content)\n",
    "#             except json.JSONDecodeError:\n",
    "#                 print(\"Invalid JSON:\", chunk_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "영상\n",
      "영상의\n",
      "영상의 주\n",
      "영상의 주제\n",
      "영상의 주제는\n",
      "영상의 주제는 까\n",
      "영상의 주제는 까르\n",
      "영상의 주제는 까르보\n",
      "영상의 주제는 까르보나라\n",
      "영상의 주제는 까르보나라 요\n",
      "영상의 주제는 까르보나라 요리\n",
      "영상의 주제는 까르보나라 요리 과정\n",
      "영상의 주제는 까르보나라 요리 과정에\n",
      "영상의 주제는 까르보나라 요리 과정에 대한\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다.\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨,\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란,\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루고\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루고 있습니다\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루고 있습니다.\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루고 있습니다.\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루고 있습니다.[DONE]\n"
     ]
    }
   ],
   "source": [
    "answer = \"\"\n",
    "for chunk in response.json().get(\"output\"):\n",
    "    answer += chunk.get(\"content\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"script.txt\",\"r\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the last chapter, you and I started to step through the internal workings of a transformer.\n",
      "This is one of the key pieces of technology inside large language models, and a lot of\n",
      "other tools in the modern wave of AI.\n",
      "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and\n",
      "in this chapter, you and I will dig into what this attention mechanism is, visualizing how\n",
      "it processes data.\n",
      "As a quick recap, here's the important context I want you to have in mind.\n",
      "The goal of the model that you and I are studying is to take in a piece of text and predict\n",
      "what word comes next.\n",
      "The input text is broken up into little pieces that we call tokens, and these are very often\n",
      "words or pieces of words, but just to make the examples in this video easier for you\n",
      "and me to think about, let's simplify by pretending that tokens are always just words.\n",
      "The first step in a transformer is to associate each token with a high-dimensional vector,\n",
      "what we call its embedding.\n",
      "Now the most important idea I want you to have in mind is how directions in this high-dimensional\n",
      "space of all possible embeddings can correspond with semantic meaning.\n",
      "In the last chapter we saw an example for how direction can correspond to gender, in\n",
      "the sense that adding a certain step in this space can take you from the embedding of a\n",
      "masculine noun to the embedding of the corresponding feminine noun.\n",
      "just one example, you could imagine how many other directions in this high-dimensional space\n",
      "could correspond to numerous other aspects of a word's meaning. The aim of a transformer is to\n",
      "progressively adjust these embeddings so that they don't merely encode an individual word,\n",
      "but instead they bake in some much, much richer contextual meaning. I should say up front that a\n",
      "lot of people find the attention mechanism, this key piece in a transformer, very confusing, so\n",
      "don't worry if it takes some time for things to sink in. I think that before we dive into the\n",
      "computational details and all the matrix multiplications, it's worth thinking about a\n",
      "couple examples for the kind of behavior that we want attention to enable. Consider the phrases\n",
      "American true mole, one mole of carbon dioxide, and take a biopsy of the mole. You and I know\n",
      "that the word mole has different meanings in each one of these, based on the context.\n",
      "But after the first step of a transformer, the one that breaks up the text and associates each\n",
      "token with a vector, the vector that's associated with mole would be the same in all three of these\n",
      "cases, because this initial token embedding is effectively a lookup table with no reference to\n",
      "the context. It's only in the next step of the transformer that the surrounding embeddings have\n",
      "the chance to pass information into this one. The picture you might have in mind is that there\n",
      "are multiple distinct directions in this embedding space encoding the multiple distinct meanings of\n",
      "the word mole, and that a well-trained attention block calculates what you need to add to the\n",
      "generic embedding to move it to one of these more specific directions, as a function of the context.\n",
      "To take another example, consider the embedding of the word tower. This is presumably some very\n",
      "generic, non-specific direction in the space, associated with lots of other large, tall nouns.\n",
      "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update\n",
      "this vector so that it points in a direction that more specifically encodes the Eiffel Tower,\n",
      "maybe correlated with vectors associated with Paris and France and things made of steel.\n",
      "If it was also preceded by the word miniature, then the vector should be updated even further\n",
      "so that it no longer correlates with large tall things. More generally than just refining the\n",
      "meaning of a word, the attention block allows the model to move information encoded in one\n",
      "embedding to that of another, potentially ones that are quite far away, and potentially\n",
      "with information that's much richer than just a single word.\n",
      "What we saw in the last chapter was how after all of the vectors flow through the network,\n",
      "including many different attention blocks, the computation that you perform to produce\n",
      "a prediction of the next token is entirely a function of the last vector in the sequence.\n",
      "So imagine, for example, that the text you input is most of an entire mystery novel,\n",
      "way up to a point near the end which reads, therefore the murderer was, if the model is\n",
      "going to accurately predict the next word, that final vector in the sequence which began its life\n",
      "simply embedding the word was will have to have been updated by all of the attention blocks\n",
      "to represent much much more than any individual word, somehow encoding all of the information\n",
      "from the full context window that's relevant to predicting the next word. To step through the\n",
      "the computations though let's take a much simpler example. Imagine that the input includes the\n",
      "phrase a fluffy blue creature roamed the verdant forest and for the moment suppose that the only\n",
      "type of update that we care about is having the adjectives adjust the meanings of their\n",
      "corresponding nouns. What I'm about to describe is what we would call a single head of attention\n",
      "and later we will see how the attention block consists of many different heads run in parallel.\n",
      "Again, the initial embedding for each word is some high-dimensional vector\n",
      "that only encodes the meaning of that particular word with no context.\n",
      "Actually, that's not quite true. They also encode the position of the word.\n",
      "There's a lot more to say about the specific way that positions are encoded,\n",
      "but right now all you need to know is that the entries of this vector are enough to tell you\n",
      "both what the word is and where it exists in the context. Let's go ahead and denote these\n",
      "embeddings with the letter E, the goal is to have a series of computations produce a\n",
      "new refined set of embeddings where, for example, those corresponding to the nouns have ingested\n",
      "the meaning from their corresponding adjectives.\n",
      "And playing the deep learning game, we want most of the computations involved to look\n",
      "like matrix-vector products where the matrices are full of tunable weights, things that the\n",
      "model will learn based on data.\n",
      "To be clear, I'm making up this example of adjectives updating nouns just to illustrate\n",
      "the type of behavior that you could imagine an intention had doing.\n",
      "As with so much deep learning, the true behavior is much harder to parse, because it's based\n",
      "on tweaking and tuning a huge number of parameters to minimize some cost function.\n",
      "It's just that as we step through all of the different matrices filled with parameters\n",
      "that are involved in this process, I think it's really helpful to have an imagined example\n",
      "of something that it could be doing to help keep it all more concrete.\n",
      "For the first step of this process, you might imagine each noun, like creature, asking the\n",
      "question, hey, are there any adjectives sitting in front of me, and for the words fluffy and\n",
      "blue to each be able to answer, yeah, I'm an adjective and I'm in that position.\n",
      "That question is somehow encoded as yet another vector, another list of numbers, which we\n",
      "call the query for this word.\n",
      "This query vector, though, has a much smaller dimension than the embedding vector, say 128.\n",
      "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying\n",
      "it by the embedding.\n",
      "Compressing things a bit, let's write that query vector as q, and then anytime you see\n",
      "me put a matrix next to an arrow like this one, it's meant to represent that multiplying\n",
      "this matrix by the vector at the arrow's start gives you the vector at the arrow's end.\n",
      "In this case, you multiply this matrix by all of the embeddings in the context, producing\n",
      "one query vector for each token.\n",
      "The entries of this matrix are parameters of the model, which means the true behavior\n",
      "is learned from data, and in practice what this matrix does in a particular attention\n",
      "head is challenging to parse.\n",
      "But for our sake, imagining an example that we might hope it would learn, we'll suppose\n",
      "that this query matrix maps the embeddings of nouns to certain directions in this smaller\n",
      "query space that somehow encodes the notion of looking for adjectives in preceding positions.\n",
      "As to what it does to other embeddings, who knows, maybe it simultaneously tries to accomplish\n",
      "some other goal with those, right now we're laser focused on the nouns.\n",
      "At the same time, associated with this is a second matrix called the key matrix, which\n",
      "you also multiply by every one of the embeddings.\n",
      "This produces a second sequence of vectors that we call the keys.\n",
      "Conceptually you want to think of the keys as potentially answering the queries.\n",
      "This key matrix is also full of tunable parameters, and just like the query matrix it maps the\n",
      "embedding vectors to that same smaller dimensional space.\n",
      "You think of the keys as matching the queries whenever they closely align with each other.\n",
      "In our example, you would imagine that the key matrix maps the adjectives, like fluffy\n",
      "and blue, to vectors that are closely aligned with the query produced by the word creature.\n",
      "To measure how well each key matches each query, you compute a dot product between each\n",
      "possible key-query pair.\n",
      "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond\n",
      "the larger dot products, the places where the keys and queries align. For our adjective-noun example,\n",
      "that would look a little more like this, where if the keys produced by fluffy and blue really do\n",
      "align closely with the query produced by creature, then the dot products in these two spots would be\n",
      "some large positive numbers. In the lingo, machine learning people would say that this means the\n",
      "embeddings of fluffy and blue attend to the embedding of creature. By contrast to the dot\n",
      "product between the key for some other word like the and the query for creature would be some small\n",
      "or negative value that reflects that these are unrelated to each other. So we have this grid of\n",
      "values that can be any real number from negative infinity to infinity giving us a score for how\n",
      "relevant each word is to updating the meaning of every other word. The way we're about to use these\n",
      "scores is to take a certain weighted sum along each column weighted by the relevance. So instead\n",
      "Instead of having values range from negative infinity to infinity, what we want is for\n",
      "the numbers in these columns to be between 0 and 1, and for each column to add up to\n",
      "1, as if they were a probability distribution.\n",
      "If you're coming in from the last chapter, you know what we need to do then.\n",
      "We compute a softmax along each one of these columns to normalize the values.\n",
      "In our picture, after you apply softmax to all of the columns, we'll fill in the grid\n",
      "with these normalized values.\n",
      "At this point, you're safe to think about each column as giving weights\n",
      "according to how relevant the word on the left is to the corresponding value at the top.\n",
      "We call this grid an attention pattern.\n",
      "Now, if you look at the original Transformer paper,\n",
      "there's a really compact way that they write this all down.\n",
      "Here, the variables q and k represent the full arrays of query and key vectors respectively,\n",
      "those little vectors you get by multiplying the embeddings by the query and the key matrices.\n",
      "This expression up in the numerator is a really compact way to represent the grid of all possible\n",
      "dot products between pairs of keys and queries. A small technical detail that I didn't mention\n",
      "is that for numerical stability it happens to be helpful to divide all of these values by the\n",
      "square root of the dimension in that key query space. Then this softmax that's wrapped around\n",
      "the full expression, is meant to be understood to apply column by column.\n",
      "As to that V term, we'll talk about it in just a second.\n",
      "Before that, there's one other technical detail that so far I've skipped.\n",
      "During the training process, when you run this model on a given text example, and all\n",
      "of the weights are slightly adjusted and tuned to either reward or punish it based on how\n",
      "high a probability it assigns to the true next word in the passage, it turns out to\n",
      "make the whole training process a lot more efficient if you simultaneously have it predict\n",
      "every possible next token following each initial sub-sequence of tokens in this passage.\n",
      "For example, with the phrase that we've been focusing on, it might also be predicting what\n",
      "words follow creature, and what words follow the.\n",
      "This is really nice, because it means what would otherwise be a single training example\n",
      "effectively acts as many.\n",
      "For the purposes of our attention pattern, it means that you never want to allow later\n",
      "words to influence earlier words, since otherwise they could kind of give away the answer for\n",
      "what comes next. What this means is that we want all of these spots here, the ones representing\n",
      "later tokens influencing earlier ones, to somehow be forced to be zero. The simplest thing you might\n",
      "think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one\n",
      "anymore, they wouldn't be normalized. So instead a common way to do this is that before applying\n",
      "softmax you set all of those entries to be negative infinity. If you do that then after\n",
      "After applying softmax, all of those get turned into zero, but the columns stay normalized.\n",
      "This process is called masking.\n",
      "There are versions of attention where you don't apply it, but in our GPT example, even\n",
      "though this is more relevant during the training phase than it would be, say, running it as\n",
      "a chatbot or something like that, you do always apply this masking to prevent later tokens\n",
      "from influencing earlier ones.\n",
      "Another fact that's worth reflecting on about this attention pattern is how its size is\n",
      "equal to the square of the context size.\n",
      "So this is why context size can be a really huge bottleneck for large language models,\n",
      "and scaling it up is non-trivial.\n",
      "As you might imagine, motivated by a desire for bigger and bigger context windows, recent\n",
      "years have seen some variations to the attention mechanism aimed at making context more scalable.\n",
      "But right here, you and I are staying focused on the basics.\n",
      "Okay, great, computing this pattern lets the model deduce which words are relevant to which\n",
      "other words.\n",
      "Now you need to actually update the embeddings, allowing words to pass information to whichever\n",
      "other words they're relevant to.\n",
      "For example, you want the embedding of fluffy to somehow cause a change to creature that\n",
      "moves it to a different part of this 12,000 dimensional embedding space that more specifically\n",
      "encodes a fluffy creature.\n",
      "What I'm going to do here is first show you the most straightforward way that you could\n",
      "do this, though there's a slight way that this gets modified in the context of multi-headed\n",
      "attention.\n",
      "This most straightforward way would be to use a third matrix, what we call the value\n",
      "matrix, which you multiply by the embedding of that first word, for example fluffy.\n",
      "The result of this is what you would call a value vector, and this is something that\n",
      "you add to the embedding of the second word, in this case something you add to the embedding\n",
      "of creature.\n",
      "So, this value vector lives in the same very high dimensional space as the embeddings.\n",
      "When you multiply this value matrix by the embedding of a word, you might think of it\n",
      "as saying if this word is relevant to adjusting the meaning of something else, what exactly should\n",
      "be added to the embedding of that something else in order to reflect this? Looking back in our\n",
      "diagram, let's set aside all of the keys and the queries, since after you compute the attention\n",
      "pattern you're done with those, then you're going to take this value matrix and multiply it by every\n",
      "one of those embeddings to produce a sequence of value vectors. You might think of these value\n",
      "vectors as being kind of associated with the corresponding keys.\n",
      "For each column in this diagram, you multiply each of the value vectors by the corresponding\n",
      "weight in that column.\n",
      "For example, here, under the embedding of creature, you would be adding large proportions\n",
      "of the value vectors for fluffy and blue, while all of the other value vectors get zeroed\n",
      "out, or at least nearly zeroed out.\n",
      "And then finally, the way to actually update the embedding associated with this column,\n",
      "previously encoding some context-free meaning of creature, you add together all of these\n",
      "rescaled values in the column, producing a change that you want to add that I'll label\n",
      "delta E, and then you add that to the original embedding.\n",
      "Hopefully what results is a more refined vector encoding the more contextually rich meaning,\n",
      "like that of a fluffy blue creature.\n",
      "And of course you don't just do this to one embedding, you apply the same weighted sum\n",
      "across all of the columns in this picture, producing a sequence of changes.\n",
      "Adding all of those changes to the corresponding embeddings produces a full sequence of more\n",
      "refined embeddings popping out of the attention block.\n",
      "Zooming out, this whole process is what you would describe as a single head of attention.\n",
      "As I've described things so far, this process is parameterized by three distinct matrices,\n",
      "all filled with tunable parameters, the key, the query, and the value.\n",
      "I want to take a moment to continue what we started in the last chapter with the scorekeeping\n",
      "where we count up the total number of model parameters using the numbers from GPT-3.\n",
      "These key and query matrices each have 12,288 columns, matching the embedding dimension,\n",
      "and 128 rows, matching the dimension of that smaller key query space.\n",
      "This gives us an additional 1.5 million or so parameters for each one.\n",
      "If you look at that value matrix by contrast, the way I've described things so far would\n",
      "suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both\n",
      "its inputs and its outputs live in this very large embedding space.\n",
      "If true, that would mean about 150 million added parameters.\n",
      "And to be clear, you could do that, you could devote orders of magnitude more parameters\n",
      "to the value map than to the key and query.\n",
      "But in practice, it is much more efficient if instead you make it so that the number\n",
      "of parameters devoted to this value map is the same as the number devoted to the key\n",
      "in the query.\n",
      "This is especially relevant in the setting of running multiple attention heads in parallel.\n",
      "The way this looks is that the value map is factored as a product of two smaller matrices.\n",
      "Conceptually, I would still encourage you to think about the overall linear map, one\n",
      "with inputs and outputs both in this larger embedding space, for example taking the embedding\n",
      "of blue to this blueness direction that you would add to nouns.\n",
      "It's just that it's broken up into two separate steps.\n",
      "The first matrix on the right here has a smaller number of rows, typically the same size as\n",
      "the key query space.\n",
      "What this means is you can think of it as mapping the large embedding vectors down to\n",
      "a much smaller space.\n",
      "This is not the conventional naming, but I'm going to call this the value down matrix.\n",
      "The second matrix maps from this smaller space back up to the embedding space, producing\n",
      "the vectors that you use to make the actual updates.\n",
      "I'm going to call this one the value-up matrix, which, again, is not conventional.\n",
      "The way that you would see this written in most papers looks a little different.\n",
      "I'll talk about it in a minute.\n",
      "In my opinion, it tends to make things a little more conceptually confusing.\n",
      "To throw in linear algebra jargon here, what we're basically doing is constraining the\n",
      "overall value map to be a low-rank transformation.\n",
      "Turning back to the parameter count, all four of these matrices have the same size, and\n",
      "Then adding them all up, we get about 6.3 million parameters for one attention head.\n",
      "As a quick side note, to be a little more accurate, everything described so far is what\n",
      "people would call a self-attention head, to distinguish it from a variation that comes\n",
      "up in other models that's called cross-attention.\n",
      "This isn't relevant to our GPT example, but if you're curious, cross-attention involves\n",
      "models that process two distinct types of data, like text in one language and text in\n",
      "another language that's part of an ongoing generation of a translation.\n",
      "Or maybe audio input of speech, and an ongoing transcription.\n",
      "A cross-attention head looks almost identical.\n",
      "The only difference is that the key and query maps act on different datasets.\n",
      "In a model doing translation, for example, the keys might come from one language, while\n",
      "the queries come from another, and the attention pattern could describe which words from one\n",
      "language correspond to which words in another.\n",
      "And in this setting there would typically be no masking, since there's not really any\n",
      "notion of later tokens affecting earlier ones.\n",
      "Staying focused on self-attention though, if you understood everything so far, and if\n",
      "you were to stop here, you would come away with the essence of what attention really\n",
      "is.\n",
      "All that's really left to us is to lay out the sense in which you do this many, many\n",
      "different times.\n",
      "In our central example we focused on adjectives updating nouns, but of course there are lots\n",
      "of different ways that context can influence the meaning of a word.\n",
      "If the words they crashed the preceded the word car, it has implications for the shape\n",
      "and the structure of that car, and a lot of associations might be less grammatical.\n",
      "If the word wizard is anywhere in the same passage as Harry, it suggests that this might\n",
      "be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were\n",
      "in that passage, then perhaps the embedding of Harry should instead be updated to refer\n",
      "to the prince.\n",
      "For every different type of contextual updating that you might imagine, the parameters of\n",
      "these key and query matrices would be different to capture the different attention patterns,\n",
      "and the parameters of our value map would be different based on what should be added to the\n",
      "embeddings. And again, in practice the true behavior of these maps is much more difficult\n",
      "to interpret, where the weights are set to do whatever the model needs them to do to best\n",
      "accomplish its goal of predicting the next token. As I said before, everything we described is a\n",
      "single head of attention, and a full attention block inside a transformer consists of what's\n",
      "called multi-headed attention where you run a lot of these operations in parallel each with its own\n",
      "distinct key query and value maps. GPT-3 for example uses 96 attention heads inside each block.\n",
      "Considering that each one is already a bit confusing it's certainly a lot to hold in your\n",
      "head. Just to spell it all out very explicitly this means you have 96 distinct key and query\n",
      "matrices producing 96 distinct attention patterns. Then each head has its own distinct value matrices\n",
      "used to produce 96 sequences of value vectors. These are all added together using the\n",
      "corresponding attention patterns as weights. What this means is that for each position in the\n",
      "context, each token, every one of these heads produces a proposed change to be added to the\n",
      "embedding in that position. So what you do is you sum together all of those proposed changes,\n",
      "one for each head, and you add the result to the original embedding of that position.\n",
      "This entire sum here would be one slice of what's outputted from this multi-headed attention block,\n",
      "a single one of those refined embeddings that pops out the other end of it.\n",
      "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.\n",
      "The overall idea is that by running many distinct heads in parallel,\n",
      "you're giving the model the capacity to learn many distinct ways that context changes meaning.\n",
      "Pulling up our running tally for parameter count with 96 heads, each including its own variation\n",
      "of these four matrices, each block of multi-headed attention ends up with around 600 million\n",
      "parameters. There's one added slightly annoying thing that I should really mention for any of you\n",
      "who go on to read more about transformers. You remember how I said that the value map is factored\n",
      "out into these two distinct matrices, which I labeled as the value down and the value up\n",
      "matrices. The way that I framed things would suggest that you see this pair of matrices\n",
      "inside each attention head, and you could absolutely implement it this way. That would\n",
      "be a valid design. But the way that you see this written in papers and the way that it's\n",
      "implemented in practice looks a little different. All of these value up matrices for each head\n",
      "appear stapled together in one giant matrix that we call the output matrix, associated with\n",
      "the entire multi-headed attention block. And when you see people refer to the value matrix for a\n",
      "given attention head, they're typically only referring to this first step, the one that I\n",
      "was labeling as the value down projection into the smaller space. For the curious among you,\n",
      "I've left an on-screen note about it. It's one of those details that runs the risk of distracting\n",
      "from the main conceptual points, but I do want to call it out just so that you know if you read\n",
      "about this in other sources. Setting aside all the technical nuances, in the preview from the\n",
      "last chapter, we saw how data flowing through a transformer doesn't just flow through a single\n",
      "attention block. For one thing, it also goes through these other operations called multi-layer\n",
      "perceptrons. We'll talk more about those in the next chapter. And then it repeatedly goes through\n",
      "many, many copies of both of these operations. What this means is that after a given word imbibes\n",
      "some of its context, there are many more chances for this more nuanced embedding to be influenced\n",
      "by its more nuanced surroundings. The further down the network you go, with each embedding\n",
      "taking in more and more meaning from all the other embeddings, which themselves are getting\n",
      "more and more nuanced, the hope is that there's the capacity to encode higher level and more\n",
      "abstract ideas about a given input beyond just descriptors and grammatical structure.\n",
      "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are\n",
      "are relevant to the piece, and things like that.\n",
      "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the\n",
      "total number of key, query, and value parameters is multiplied by another 96, which brings\n",
      "the total sum to just under 58 billion distinct parameters devoted to all of the attention\n",
      "heads.\n",
      "That is a lot, to be sure, but it's only about a third of the 175 billion that are\n",
      "in the network in total.\n",
      "So even though attention gets all of the attention, the majority of parameters come from the blocks\n",
      "sitting in between these steps.\n",
      "In the next chapter, you and I will talk more about those other blocks and also a lot more\n",
      "about the training process.\n",
      "A big part of the story for the success of the attention mechanism is not so much any\n",
      "specific kind of behavior that it enables, but the fact that it's extremely parallelizable,\n",
      "meaning that you can run a huge number of computations in a short time using GPUs.\n",
      "that one of the big lessons about deep learning in the last decade or two has been that scale\n",
      "alone seems to give huge qualitative improvements in model performance. There's a huge advantage to\n",
      "parallelizable architectures that let you do this. If you want to learn more about this stuff, I've\n",
      "left lots of links in the description. In particular, anything produced by Andre Karpathy or Chris Ola\n",
      "tend to be pure gold. In this video, I wanted to just jump into attention in its current form,\n",
      "but if you're curious about more of the history for how we got here and how you might reinvent\n",
      "this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of\n",
      "that motivation. Also, Britt Cruz from the channel The Art of the Problem\n",
      "has a really nice video about the history of large language models.\n",
      "you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3.5-turbo 토큰 수: 5831\n",
      "GPT-4 토큰 수: 5831\n",
      "GPT-4o-mini 토큰 수: 5729\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# gpt-3.5-turbo 모델의 토큰 수 계산\n",
    "def calculate_tokens_gpt35(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# gpt-4 모델의 토큰 수 계산\n",
    "def calculate_tokens_gpt4(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# gpt-4 모델의 토큰 수 계산\n",
    "def calculate_tokens_gpt4o_mini(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "\n",
    "# gpt-3.5-turbo의 토큰 수 계산\n",
    "tokens_gpt35 = calculate_tokens_gpt35(context)\n",
    "print(f\"GPT-3.5-turbo 토큰 수: {tokens_gpt35}\")\n",
    "\n",
    "# gpt-4의 토큰 수 계산\n",
    "tokens_gpt4 = calculate_tokens_gpt4(context)\n",
    "print(f\"GPT-4 토큰 수: {tokens_gpt4}\")\n",
    "\n",
    "# gpt-4의 토큰 수 계산\n",
    "tokens_gpt4o = calculate_tokens_gpt4o_mini(context)\n",
    "print(f\"GPT-4o-mini 토큰 수: {tokens_gpt4o}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4 모델의 토큰 수 계산\n",
    "def calculate_tokens_gpt4(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
