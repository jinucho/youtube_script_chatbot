{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "import re\n",
    "\n",
    "\n",
    "class YouTubeService:\n",
    "    async def get_title_and_hashtags(self, url: str):\n",
    "        yt = await self._create_youtube_instance(url)\n",
    "        print(\"영상 정보 확인\")\n",
    "        title = yt.title\n",
    "        description = yt.description\n",
    "        hashtags = re.findall(r\"#\\w+\", description)\n",
    "        return {\"title\": title, \"hashtags\": \" \".join(hashtags)}\n",
    "\n",
    "    async def get_video_info(self, url: str):\n",
    "        yt = await self._create_youtube_instance(url)\n",
    "        audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "        print(\"음성 추출 완료\")\n",
    "        return {\n",
    "            \"title\": yt.title,\n",
    "            \"audio_url\": audio_stream.url if audio_stream else None,\n",
    "        }\n",
    "\n",
    "    async def _create_youtube_instance(self, url: str):\n",
    "        print(\"YouTube 인스턴스 생성 완료\")\n",
    "        return YouTube(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import ffmpeg\n",
    "import requests\n",
    "import soundfile as sf\n",
    "from faster_whisper import BatchedInferencePipeline, WhisperModel\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "class WhisperTranscriptionService:\n",
    "    def __init__(self):\n",
    "        model = WhisperModel(\n",
    "            \"large-v3\", device='cuda', compute_type=\"float16\"\n",
    "        )\n",
    "        self.model = BatchedInferencePipeline(model=model)\n",
    "        self.language = None\n",
    "        self.okt = Okt()\n",
    "        print(\"Whisper 모델 초기화 완료\")\n",
    "\n",
    "    def create_session(self):\n",
    "        session = requests.Session()\n",
    "        retry = Retry(\n",
    "            total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        return session\n",
    "\n",
    "    def download_chunk(self, args):\n",
    "        url, start, end, chunk_number, temp_dir = args\n",
    "\n",
    "        headers = {\"Range\": f\"bytes={start}-{end}\"}\n",
    "        session = self.create_session()\n",
    "\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, stream=True)\n",
    "            chunk_path = os.path.join(temp_dir, f\"chunk_{chunk_number:04d}\")\n",
    "\n",
    "            with open(chunk_path, \"wb\") as f:\n",
    "                for data in response.iter_content(chunk_size=8192):\n",
    "                    f.write(data)\n",
    "\n",
    "            return chunk_path, chunk_number\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading chunk {chunk_number}: {str(e)}\")\n",
    "            return None, chunk_number\n",
    "\n",
    "    def _single_stream_download(self, url: str, temp_dir: str) -> str:\n",
    "        \"\"\"단일 스트림으로 파일을 다운로드합니다.\"\"\"\n",
    "        print(\"Starting single stream download...\")\n",
    "        session = self.create_session()\n",
    "        output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "\n",
    "        try:\n",
    "            with session.get(url, stream=True) as response:\n",
    "                response.raise_for_status()\n",
    "                with open(output_path, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to download file: {str(e)}\")\n",
    "\n",
    "    def parallel_download(self, url: str, temp_dir: str, num_chunks: int = 10) -> str:\n",
    "        \"\"\"병렬 다운로드를 시도하고, 실패 시 단일 스트림으로 폴백\"\"\"\n",
    "        session = self.create_session()\n",
    "\n",
    "        try:\n",
    "            # HEAD 요청으로 파일 크기 확인 시도\n",
    "            response = session.head(url, allow_redirects=True)\n",
    "            total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "            # HEAD 요청이 실패하면 GET 요청으로 시도\n",
    "            if total_size == 0:\n",
    "                response = session.get(url, stream=True)\n",
    "                total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "            # 파일 크기를 여전히 확인할 수 없는 경우 단일 스트림으로 다운로드\n",
    "            if total_size == 0:\n",
    "                print(\n",
    "                    \"Warning: Could not determine file size. Falling back to single stream download.\"\n",
    "                )\n",
    "                return self._single_stream_download(url, temp_dir)\n",
    "            print(\"Starting parallel download...\")\n",
    "            chunk_size = total_size // num_chunks\n",
    "            chunks = []\n",
    "\n",
    "            for i in range(num_chunks):\n",
    "                start = i * chunk_size\n",
    "                end = start + chunk_size - 1 if i < num_chunks - 1 else total_size - 1\n",
    "                chunks.append((start, end))\n",
    "\n",
    "            download_args = [\n",
    "                (url, start, end, i, temp_dir) for i, (start, end) in enumerate(chunks)\n",
    "            ]\n",
    "\n",
    "            chunk_paths = []\n",
    "            with concurrent.futures.ThreadPoolExecutor(\n",
    "                max_workers=num_chunks\n",
    "            ) as executor:\n",
    "                futures = executor.map(self.download_chunk, download_args)\n",
    "                chunk_paths = [(path, num) for path, num in futures if path is not None]\n",
    "\n",
    "            if not chunk_paths:\n",
    "                raise Exception(\"No chunks were downloaded successfully\")\n",
    "\n",
    "            chunk_paths.sort(key=lambda x: x[1])\n",
    "            output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "\n",
    "            with open(output_path, \"wb\") as outfile:\n",
    "                for chunk_path, _ in chunk_paths:\n",
    "                    with open(chunk_path, \"rb\") as infile:\n",
    "                        outfile.write(infile.read())\n",
    "                    os.remove(chunk_path)\n",
    "\n",
    "            return output_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error in parallel download: {str(e)}. Falling back to single stream download.\"\n",
    "            )\n",
    "            return self._single_stream_download(url, temp_dir)\n",
    "\n",
    "    def convert_to_wav(self, input_path: str, output_path: str) -> bool:\n",
    "        try:\n",
    "            stream = ffmpeg.input(input_path)\n",
    "            stream = ffmpeg.output(\n",
    "                stream, output_path, acodec=\"pcm_s16le\", ar=\"16000\", ac=\"1\"\n",
    "            )\n",
    "            ffmpeg.run(stream, capture_stdout=True, capture_stderr=True)\n",
    "            return True\n",
    "        except ffmpeg.Error as e:\n",
    "            print(\"FFmpeg error:\", e.stderr.decode())\n",
    "            return False\n",
    "\n",
    "    def process_audio_chunk(self, chunk_data: tuple,promp:str = None,filtered_words:list = None) -> List[Dict[str, Any]]:\n",
    "        audio_path, start_time, duration = chunk_data\n",
    "        try:\n",
    "            segments, info = self.model.transcribe(\n",
    "                audio_path,\n",
    "                beam_size=5,\n",
    "                best_of=7,\n",
    "                batch_size=32,\n",
    "                temperature=0.7,\n",
    "                word_timestamps=True,\n",
    "                initial_prompt=f\"음성 제목: {promp}\",\n",
    "                repetition_penalty=2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=1.1,\n",
    "                log_prob_threshold=-0.5,\n",
    "                no_speech_threshold=0.7,\n",
    "                patience=1.2,\n",
    "                hotwords=filtered_words\n",
    "            )\n",
    "            if info and hasattr(info, \"language\"):\n",
    "                self.language = info.language\n",
    "            return self._process_segments(segments, start_time)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk at {start_time}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _process_segments(\n",
    "        self, segments, start_time: float = 0\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        transcript = []\n",
    "        for segment in segments:\n",
    "            transcript.append(\n",
    "                {\n",
    "                    \"start\": round(segment.start + start_time, 2),\n",
    "                    \"end\": round(segment.end + start_time, 2),\n",
    "                    \"text\": segment.text,\n",
    "                }\n",
    "            )\n",
    "        return transcript\n",
    "\n",
    "    async def process_with_progress(\n",
    "        self, url: str, prompt:str, filtered_words:str,chunk_duration: int = 30, num_download_chunks: int = 10\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            mp4_path = self.parallel_download(url, temp_dir, num_download_chunks)\n",
    "            print(\"Download complete!\")\n",
    "\n",
    "            wav_path = os.path.join(temp_dir, \"audio.wav\")\n",
    "            if not self.convert_to_wav(mp4_path, wav_path):\n",
    "                raise Exception(\"Failed to convert audio to WAV format\")\n",
    "\n",
    "            wav_info = sf.info(wav_path)\n",
    "            total_duration = wav_info.duration\n",
    "            total_chunks = math.ceil(total_duration / chunk_duration)\n",
    "\n",
    "            chunks_data = []\n",
    "            for i in range(total_chunks):\n",
    "                start_time = i * chunk_duration\n",
    "                chunk_wav_path = os.path.join(temp_dir, f\"chunk_{i}.wav\")\n",
    "\n",
    "                duration = min(chunk_duration, total_duration - start_time)\n",
    "                stream = ffmpeg.input(wav_path, ss=start_time, t=duration)\n",
    "                stream = ffmpeg.output(\n",
    "                    stream, chunk_wav_path, acodec=\"pcm_s16le\", ar=\"16000\", ac=\"1\"\n",
    "                )\n",
    "                ffmpeg.run(stream, quiet=True)\n",
    "\n",
    "                chunks_data.append((chunk_wav_path, start_time, duration))\n",
    "\n",
    "            all_segments = []\n",
    "            for chunk_data in chunks_data:\n",
    "                segments = self.process_audio_chunk(chunk_data,prompt,filtered_words)\n",
    "                all_segments.extend(segments)\n",
    "\n",
    "                if os.path.exists(chunk_data[0]):\n",
    "                    os.remove(chunk_data[0])\n",
    "\n",
    "        return all_segments\n",
    "\n",
    "    async def transcribe(self, audio_url: str,prompt: str = None) -> Dict[str, Any]:\n",
    "        try:\n",
    "            try:\n",
    "                tagged = self.okt.pos(prompt)\n",
    "                filtered_words = []\n",
    "                for word, tag in tagged:\n",
    "                    if tag == \"Noun\" or tag == \"Hashtag\":\n",
    "                        filtered_words.append(word)\n",
    "            except:\n",
    "                filtered_words = None\n",
    "            segments = await self.process_with_progress(\n",
    "                audio_url, prompt, filtered_words,chunk_duration=30, num_download_chunks=10\n",
    "            )\n",
    "\n",
    "            print(\"텍스트 추출 완료\")\n",
    "\n",
    "            return {\"script\": segments, \"language\": self.language}\n",
    "        except Exception as e:\n",
    "            print(f\"Error in transcribe: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = YouTubeService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube 인스턴스 생성 완료\n",
      "음성 추출 완료\n"
     ]
    }
   ],
   "source": [
    "video_info = await youtube.get_video_info(\"https://youtu.be/EMMC0ym0QOI?si=bx7raBo-QwR3MGy7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://rr3---sn-ab02a0nfpgxapox-bh2es.googlevideo.com/videoplayback?expire=1730288214&ei=9sUhZ-S4KqmS1d8P2fC86AI&ip=106.254.102.210&id=o-APMMqsFIdvorwX2g8mZSYwSI97kewVVffUbfm63s3jaW&itag=139&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1730266614%2C&mh=in&mm=31%2C26&mn=sn-ab02a0nfpgxapox-bh2es%2Csn-un57sne7&ms=au%2Conr&mv=m&mvi=3&pcm2cms=yes&pl=18&rms=au%2Cau&initcwndbps=631250&vprv=1&mime=audio%2Fmp4&rqh=1&gir=yes&clen=3098057&dur=507.866&lmt=1730247679854257&mt=1730266148&fvip=5&keepalive=yes&fexp=51312688%2C51326932&c=ANDROID_VR&txp=5532434&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cvprv%2Cmime%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIhAJ1iwrkBkHIGgBxoEOdXDVUki9JxnzCxlH4NwZNL-c7VAiAIuPvuo8iG1lVUslal7d0UgXVSbKRlNX40w6N0WY8ndg%3D%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpcm2cms%2Cpl%2Crms%2Cinitcwndbps&lsig=ACJ0pHgwRgIhALIR7EC_OxXJaEcXyq70r6pKBJzcOc-VBpVflruAyVIaAiEA1Xwp3212wT-5N5XJYpYahE_QW5RwDB_BtFuDeiiUkLA%3D'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_info[\"audio_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'시리가 AI 에이전트가 된다?! 애플의 AI, Apple Intelligence 알아보기  iOS 18.1, iOS 18.2 개발자 Beta 업데이트'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_info[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../miniconda3/envs/youtube/lib/python3.10/site-packages/faster_whisper/assets/pyannote_vad_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.4.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Whisper 모델 초기화 완료\n"
     ]
    }
   ],
   "source": [
    "whisper = WhisperTranscriptionService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel download...\n",
      "Download complete!\n",
      "텍스트 추출 완료\n"
     ]
    }
   ],
   "source": [
    "transcript = await whisper.transcribe(video_info[\"audio_url\"],video_info[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 올해 WWDC에서 공개된 애플의 AI, Apple Intelligence가 드디어 iOS 18.1 정식 버전으로 누구나 사용할 수 있게 공개가 되었습니다 또 추가로 더 많은 AI 기능들이 iOS 18.2 개발자 베타 버전을 통해 공개가 되었는데요 업데이트된 AI 기능들을 하나씩 함께 살펴보겠습니다 이번 iOS 18.1에 포함된 AI 기능들은 강력해진 시리부터 글쓰기 도구, 알림 관련 AI, 클린업 인데요 강력해진 시리는 디자인이 변경되었고 \n",
      "\n",
      " 애플 인텔리전스로 업데이트된 시리는 사용자가 화면에서 보고 있는 것을 파악하고 그에 맞춤 작업을 수행할 수 있는데요 뿐만 아니라 더 자연스러운 대화를 지원해서 사용자가 말을 더듬거나 멈추더라도 이해하고 반응할 수 있게 되었고 이전 대화를 기억해 대화 맥락도 이해합니다 다음으로 글쓰기 도구는 이렇게 버튼만 딸깍하면 텍스트를 교정하고 다시 쓰고 요약해주는 기능입니다 맞춤법을 고치거나 텍스트의 느낌을 좀 더 친근하게 또는 전문적으로 바꿀 수도 있습니다 \n",
      "\n",
      " 네이티브 앱은 물론 서드파티 앱에서도 활용이 가능합니다. 메일 앱이나 메시지 앱에서는 이메일 내용을 요약해서 보여주고 알림을 요약해서 보여주는 기능과 답변을 추천해주는 스마트 답장 기능도 추가되었습니다. Reduce Interruption 이라는 새로운 방해 금지 모드도 추가되었습니다. AI가 알림의 중요도를 판단해서 중요한 알림만 노출시켜줍니다. 애플 워치에서도 동일하게 이 방해 금지 모드를 적용할 수 있습니다. 마지막으로 사진의 특정 부분을 지우는 \n",
      "\n",
      " 기능도 사용 가능합니다. 이번 iOS 18.2 개발자 베타 업데이트에서는 애플 인텔리전스의 다른 기능들도 미리 체험해 볼 수 있었는데요. Genmoji, Image Playground, Visual Intelligence, ChatGPT와 통합된 Siri를 사용할 수 있습니다. 이번 업데이트에서는 다양한 이미지 생성 AI 기능이 포함되었는데요. 베타 버전에서는 웨이트 리스트를 신청하고 승인이 되어야 쓸 수 있습니다. Genmoji는 나만의 이모지를 생성할 수 있는 기능입니다. \n",
      "\n",
      " 원하는 이모지에 간단한 설명을 입력하면 제모지가 생성되는데요 생성된 이모지 중에 마음에 드는 것을 선택해 이모지를 사용하듯 사용이 가능합니다 다음으로 이미지 플레이그라운드는 애플의 이미지 생성 AI로 새로 생긴 앱 또는 메시지 앱과 메일 앱 등에서 사용할 수 있는데요 텍스트로 원하는 이미지를 입력하면 만화나 일러스트 스타일로 빠르게 이미지를 만들어주며 추가로 아이템이나 의상을 덧붙여 이미지를 수정할 수도 있고 내 사진을 기반으로도 이미지를 만들 수 있습니다 \n",
      "\n",
      " 추가로 아이패드에서는 Image Wand라는 기능도 사용 가능한데요. 메모에서 간단한 스케치를 하면 이를 정교한 이미지로 변환해줍니다. 이제 아이폰 16 시리즈의 카메라 버튼을 꾹 누르면 Visual Intelligence를 활용한 검색이 가능해집니다. 이 상태로 사물을 촬영하면 관련된 질문을 구글로 검색하거나 최츠피티에게 검색할 수 있게 되었는데요. 말이 많았던 이 버튼, 드디어 쓸모를 찾은 것 같습니다. 최츠피티 얘기가 나왔으니 바로 얘기를 하자면 \n",
      "\n",
      " 시리에 뭔가 물어보고 나서 최찌 PT를 사용해서 대답해달라고 하면 최찌 PT를 이용한 대답을 보여줍니다. 최찌 PT를 유료로 구독 중이시라면 계정 연동도 가능한데요. 계정을 연동하고 그림을 그려달라고 부탁하면 GPT가 그림도 바로 그려주는 것을 볼 수 있습니다. 하지만 WWDC에서 소개된 내용 중 아직 완전히 구형되지 않은 기능들이 있는데요. 바로 In-App Actions와 Personal Context입니다. 사실 이 두 가지 기능이 시리의 변화와 관련된 핵심 기능이라고 할 수 있겠는데요. \n",
      "\n",
      " 생각하는 진짜 시리는 무엇일지 개발자와 관련된 업데이트를 되짚어보면 자세히 알 수 있습니다. 일단 이번 WWDC에서 개발자들이 자신들의 앱을 시리와 통합할 수 있게 해주는 다양한 방법을 새롭게 공개했습니다. 이미 이전부터 시리킥과 앱 인텐츠 프레임워크를 통해서 개발자들이 시리와 본인의 앱을 통합할 수 있었지만 이번에 공개된 앱 인텐츠 도메인과 인덱스트 엔티티 API를 활용하면 시리와 여러분의 앱을 좀 더 긴밀하게 통합할 수 있게 되는데요. \n",
      "\n",
      " 앱 인텐츠는 앱이 제공하는 기능을 시리 및 애플 인텔리전스가 이해할 수 있는 형식으로 변환해주는 프레임워크인데요. 이번 업데이트를 통해 앱 인텐츠에 사진, 메일 등을 포함한 12개의 도메인이 추가되었습니다. 즉, 이렇게 새로운 도메인의 기능들을 시리 및 애플 인텔리전스와 통합할 수 있게 된 것이죠. 이를 통해 애플에서 직접 만든 앱이 아닌 서드 파티 앱에서도 애플 인텔리전스가 앱 기능을 직접 수행하는 것이 가능해지는 인앱 액션을 수행할 수 있게 됩니다. \n",
      "\n",
      " 또 다른 API인 Indexed Entity API를 활용하면 앱 내의 데이터 구조를 Spotlight와 시리가 검색할 수 있는 형태로 만들어주는데요. 이를 인덱싱한다고 합니다. 앱 내의 컨텐츠를 인덱싱하면 시리가 이걸 들여다볼 수 있게 되는 것이죠. 그런 인덱싱한 정보를 이용해 더 다양한 업무를 할 수도 있게 되는 것입니다. 뿐만 아니라 Indexed Entity를 통해 시멘틱 검색, 즉 의미 검색도 가능해지는데요. 기존에는 고양이 사진을 검색해달라고 하면 \n",
      "\n",
      " 이제는 반려동물을 검색해달라고 하면 시리가 자연어처리를 통해 고양이, 강아지 등등을 모두 찾아줄 수 있게 됩니다. 설명이 굉장히 길었는데요. 그럼 이렇게 시리와 앱이 잘 통합되면 어떤 것이 가능해졌냐? 만약에 사용자가 다음 주 뉴욕 출장 계획을 요약해서 PD에게 메일로 답장해줘 라고 요청을 한다고 가정을 해보겠습니다. 일단 시리가 메일로 주고받은 출장 관련 정보, 캘린더 앱의 일정, 여행 앱의 예약 정보, 메모 앱의 관련 노트 등을 \n",
      "\n",
      " 이때 여행 앱 개발자가 Indexed Entity API를 활용해 데이터를 인덱싱할 수 있게 해두면 시리가 예약 정보 데이터를 확인할 수 있게 됩니다. 이 부분이 WWDC에서 얘기했던 Personal Context인데요. 애플 인텔리전스가 다양한 앱에서 수집한 정보를 통해 개인적인 맥락을 이해할 수 있게 됩니다. 이렇게 모은 개인 정보를 시리가 요약해서 메일로 전할 수 있습니다. 이때 애플 메일이 아닌 다른 메일 앱을 사용하더라도 \n",
      "\n",
      " Intents와 연결되어 있다면 시리가 그 앱의 메일 답장 기능을 이해하고 알아서 답장을 보낼 수 있습니다 결국 애플이 생각하는 궁극적인 시리는 단순히 말을 알아듣고 답변만 하는 것이 아니라 사용자의 요청을 이해하고 여러 앱의 데이터와 기능을 통합하여 실제 필요한 작업을 사람을 대신해서 수행해주는 AI를 지향하고 있다는 것을 알 수 있는데요 이를 간단히 AI 에이전트라고 합니다 정리하면 올해 WWDC에서 업데이트된 내용들이 \n",
      "\n",
      " 인앱 액션스를 가능하게 하고 퍼스널 컨텍스트를 이해할 수 있게 해주는데요 즉, 시리를 AI 에이전트로 만들어줄 수 있도록 꼽는 중요한 역할을 하게 되는 것이죠 다만 아직은 이번 업데이트를 개발자들이 각각의 앱에 다 적용하기에는 시간도 많이 필요하고 시리 또한 아직 기능이 완벽하지 않은데요 앞으로 시리가 더 많은 앱의 데이터를 참조할 수 있고 앱 내의 동작 수행도 가능해질 테니 모든 작업을 알아서 수행하는 아이언맨의 자비스 같은 나만의 비성 \n",
      "\n",
      " AI 에이전트 시리를 곧 만나볼 수 있을 것 같습니다. 이렇게 다양한 기능에 추가된 애플 인텔리전스. 아직은 안타깝게도 미국 한정으로 영어로만 사용이 가능한데요. 몇 가지 과정을 거치면 한국에서도 사용이 가능합니다. 일단 국가를 미국으로 설정해줘야 하는데요. 설정으로 들어가서 일반, 언어 및 지역으로 이동한 후 영어를 기본 언어로 선택하고 지역을 미국으로 설정합니다. 만약 영어가 없다면 언어 추가를 통해 설치하고 \n",
      "\n",
      " 기본으로 설정해 주시면 됩니다 또 베타 버전을 다운받으려면 개발자 등록이 필요한데요 애플 디벨로퍼 홈페이지 또는 앱에서 자신의 애플 아이디로 로그인해 주시면 됩니다 마지막으로 미국 앱스토어를 같이 이용하려면 미국 주소가 필요한데요 미국 배송대행지를 검색해 나오는 서비스를 이용하면 간단하게 미국 주소를 생성하고 등록이 가능합니다 다만 지금 말씀드린 방법은 공식적인 사용법은 아니기 때문에 주로 사용하는 디바이스에는 적용하지 않는 것을 추천드립니다 \n",
      "\n",
      " 지금까지 공개된 애플 인텔리전스와 애플이 생각하는 AI의 미래를 소개해드렸는데요 애플이 내년까지 계속해서 AI 관련 업데이트를 예고하고 있는 만큼 새로운 기능이 나오면 추가로 더 소개해드리겠습니다 영상이 도움이 되셨다면 좋아요와 구독 부탁드리겠습니다 감사합니다 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for script in transcript['script']:\n",
    "    print(script['text'],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel download...\n",
      "Download complete!\n",
      "텍스트 추출 완료\n"
     ]
    }
   ],
   "source": [
    "transcript = await whisper.transcribe(video_info[\"audio_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 올해 WWDC에서 공개된 애플의 AI, 애플 인텔리전스가 드디어 iOS 18.1 정식 버전으로 누구나 사용할 수 있게 공개가 되었습니다 또 추가로 더 많은 AI 기능들이 iOS 18.2 개발자 베타 버전을 통해 공개가 되었는데요 업데이트된 AI 기능들을 하나씩 함께 살펴보겠습니다 이번 iOS 18.1에 포함된 AI 기능들은 강력해진 Siri부터 글쓰기 도구, 알림 관련 AI, 클린업인데요 강력해진 Siri는 디자인이 변경되었고 \n",
      "\n",
      " 애플 인텔리전스로 업데이트된 Siri는 사용자가 화면에서 보고 있는 것을 파악하고 그에 맞춤 작업을 수행할 수 있는데요 뿐만 아니라 더 자연스러운 대화를 지원해서 사용자가 말을 더듬거나 멈추더라도 이해하고 반응할 수 있게 되었고 이전 대화를 기억해 대화 맥락도 이해합니다 다음으로 글쓰기 도구는 이렇게 버튼만 딸깍하면 텍스트를 교정하고 다시 쓰고 요약해주는 기능입니다 맞춤법을 고치거나 텍스트의 느낌을 좀 더 친근하게 또는 전문적으로 바꿀 수도 있습니다 \n",
      "\n",
      " 네이티브 앱은 물론 서드파티 앱에서도 활용이 가능합니다. 메일 앱이나 메시지 앱에서는 이메일 내용을 요약해서 보여주고 알림을 요약해서 보여주는 기능과 답변을 추천해주는 스마트 답장 기능도 추가되었습니다. Reduce Interruption이라는 새로운 방해 금지 모드도 추가되었습니다. AI가 알림의 중요도를 판단해서 중요한 알림만 노출시켜줍니다. 애플 워치에서도 동일하게 이 방해 금지 모드를 적용할 수 있습니다. 마지막으로 사진의 특정 부분을 지우는 \n",
      "\n",
      " 이번 iOS 18.2 개발자 베타 업데이트에서는 애플 인텔리전스의 다른 기능들도 미리 체험해 볼 수 있었는데요 이번 업데이트에서는 다양한 이미지 생성 AI 기능이 포함되었는데요 베타 버전에서는 웨이트 리스트를 신청하고 승인이 되어야 쓸 수 있습니다 \n",
      "\n",
      " 그리고 원하는 이모지에 간단한 설명을 입력하면 제모지가 생성되는데요 생성된 이모지 중에 마음에 드는 것을 선택해 이모지를 사용하듯 사용이 가능합니다 다음으로 이미지 플레이그라운드는 애플의 이미지 생성 AI로 새로 생긴 앱 또는 메시지 앱과 메일 앱 등에서 사용할 수 있는데요 텍스트로 원하는 이미지를 입력하면 만화나 일러스트 스타일로 빠르게 이미지를 만들어주며 추가로 아이템이나 의상을 덧붙여 이미지를 수정할 수도 있고 내 사진을 기반으로도 이미지를 만들 수 있습니다 \n",
      "\n",
      " 추가로 아이패드에서는 Image Wand라는 기능도 사용 가능한데요. 메모에서 간단한 스케치를 하면 이를 정교한 이미지로 변환해줍니다. 이제 아이폰 16 시리즈의 카메라 버튼을 꾹 누르면 Visual Intelligence를 활용한 검색이 가능해집니다. 이 상태로 사물을 촬영하면 관련된 질문을 구글로 검색하거나 최츠피티에게 검색할 수 있게 되었는데요. 말이 많았던 이 버튼, 드디어 쓸모를 찾은 것 같습니다. 최츠피티 얘기가 나왔으니 바로 얘기를 하자면 \n",
      "\n",
      " 실의에 뭔가 물어보고 나서 최찌PT를 사용해서 대답해달라고 하면 최찌PT를 이용한 대답을 보여줍니다. 최찌PT를 유료로 구독 중이시라면 계정 연동도 가능한데요. 계정을 연동하고 그림을 그려달라고 부탁하면 GPT가 그림도 바로 그려주는 것을 볼 수 있습니다. 하지만 WWDC에서 소개된 내용 중 아직 완전히 구형되지 않은 기능들이 있는데요. 바로 In-App Actions와 Personal Context입니다. 사실 이 두 가지 기능이 실의의 변화와 관련된 핵심 기능이라고 할 수 있겠는데요. \n",
      "\n",
      " 제가 생각하는 진짜 시리는 무엇일지 개발자와 관련된 업데이트를 되짚어보면 자세히 알 수 있습니다 일단 이번 WWDC에서 개발자들이 자신들의 앱을 시리와 통합할 수 있게 해주는 다양한 방법을 새롭게 공개했습니다 이미 이전부터 시리킥과 앱 인텐츠 프레임워크를 통해서 개발자들이 시리와 본인의 앱을 통합할 수 있었지만 이번에 공개된 앱 인텐츠 도메인과 인덱스트 엔티티 API를 활용하면 시리와 여러분의 앱을 좀 더 긴밀하게 통합할 수 있게 되는데요 \n",
      "\n",
      " 앱 인텐츠는 앱이 제공하는 기능을 Siri 및 애플 인텔리전스가 이해할 수 있는 형식으로 변환해주는 프레임워크인데요 이번 업데이트를 통해 앱 인텐츠에 사진, 메일 등을 포함한 12개의 도메인이 추가되었습니다 즉, 이렇게 새로운 도메인의 기능들을 Siri 및 애플 인텔리전스와 통합할 수 있게 된 것이죠 이를 통해 애플에서 직접 만든 앱이 아닌 서드 파티 앱에서도 애플 인텔리전스가 앱 기능을 직접 수행하는 것이 가능해지는 인앱 액션을 수행할 수 있게 됩니다 \n",
      "\n",
      " 또 다른 API인 Indexed Entity API를 활용하면 앱 내의 데이터 구조를 Spotlight와 Siri가 검색할 수 있는 형태로 만들어주는데요. 이를 인덱싱한다고 합니다. 앱 내의 컨텐츠를 인덱싱하면 Siri가 이걸 들여다볼 수 있게 되는 것이죠. 그런 인덱싱한 정보를 이용해 더 다양한 업무를 할 수도 있게 되는 것입니다. 뿐만 아니라 Indexed Entity를 통해 시멘틱 검색, 즉 의미 검색도 가능해지는데요. 기존에는 고양이 사진을 검색해달라고 하면 \n",
      "\n",
      " 이제는 반려동물을 검색해달라고 하면 시리가 자연어처리를 통해 고양이, 강아지 등등을 모두 찾아줄 수 있게 됩니다. 설명이 굉장히 길었는데요. 그럼 이렇게 시리와 앱이 잘 통합되면 어떤 것이 가능해졌냐? 만약에 사용자가 다음 주 뉴욕 출장 계획을 요약해서 PD에게 메일로 답장해줘 라고 요청을 한다고 가정을 해보겠습니다. 일단 시리가 메일로 주고받은 출장 관련 정보, 캘린더 앱의 일정, 여행 앱의 예약 정보, 메모 앱의 관련 노트 등을 \n",
      "\n",
      " 이때 여행 앱 개발자가 인덱스트 엔티티 API를 활용해 데이터를 인덱싱할 수 있게 해두면 시리가 예약 정보 데이터를 확인할 수 있게 됩니다. 이 부분이 WWDC에서 얘기했던 Personal Context인데요. 애플 인텔리전스가 다양한 앱에서 수집한 정보를 통해 개인적인 맥락을 이해할 수 있게 됩니다. 이렇게 모은 개인 정보를 시리가 요약해서 메일로 전할 수 있습니다. 이때 애플 메일이 아닌 다른 메일 앱을 사용하더라도 \n",
      "\n",
      " Intents와 연결되어 있다면 Siri가 그 앱의 메일 답장 기능을 이해하고 알아서 답장을 보낼 수 있습니다 결국 애플이 생각하는 궁극적인 Siri는 단순히 말을 알아듣고 답변만 하는 것이 아니라 사용자의 요청을 이해하고 여러 앱의 데이터와 기능을 통합하여 실제 필요한 작업을 사람을 대신해서 수행해주는 AI를 지향하고 있다는 것을 알 수 있는데요 이를 간단히 AI Agent라고 합니다 정리하면 올해 WWDC에서 업데이트된 내용들이 \n",
      "\n",
      " 스냅 액션스를 가능하게 하고 퍼스널 컨텍스트를 이해할 수 있게 해주는데요 즉, 시리를 AI 에이전트로 만들어줄 수 있도록 꼽는 중요한 역할을 하게 되는 것이죠 다만 아직은 이번 업데이트를 개발자들이 각각의 앱에 다 적용하기에는 시간도 많이 필요하고 시리 또한 아직 기능이 완벽하지 않은데요 앞으로 시리가 더 많은 앱의 데이터를 참조할 수 있고 앱 내의 동작 수행도 가능해질 테니 모든 작업을 알아서 수행하는 아이언맨의 자비스 같은 나만의 비성 \n",
      "\n",
      " AI 에이전트 시리를 곧 만나볼 수 있을 것 같습니다 이렇게 다양한 기능에 추가된 애플 인텔리전스 아직은 안타깝게도 미국 한정으로 영어로만 사용이 가능한데요 몇 가지 과정을 거치면 한국에서도 사용이 가능합니다 일단 국가를 미국으로 설정해줘야 하는데요 설정으로 들어가서 일반, 언어 및 지역으로 이동한 후 영어를 기본 언어로 선택하고 지역을 미국으로 설정합니다 만약 영어가 없다면 언어 추가를 통해 설치하고 \n",
      "\n",
      " 기본으로 설정해주시면 됩니다 또 베타 버전을 다운받으려면 개발자 등록이 필요한데요 애플 디벨로퍼 홈페이지 또는 앱에서 자신의 애플 아이디로 로그인 해주시면 됩니다 마지막으로 미국 앱스토어를 같이 이용하려면 미국 주소가 필요한데요 미국 배송대행지를 검색해 나오는 서비스를 이용하면 간단하게 미국 주소를 생성하고 등록이 가능합니다 다만 지금 말씀드린 방법은 공식적인 사용법은 아니기 때문에 주로 사용하는 디바이스에는 적용하지 않는 것을 추천드립니다 \n",
      "\n",
      " 지금까지 공개된 애플 인텔리전스와 애플이 생각하는 AI의 미래를 소개해드렸는데요 애플이 내년까지 계속해서 AI 관련 업데이트를 예고하고 있는 만큼 새로운 기능이 나오면 추가로 더 소개해드리겠습니다 영상이 도움이 되셨다면 좋아요와 구독 부탁드리겠습니다 감사합니다 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for script in transcript['script']:\n",
    "    print(script['text'],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://youtu.be/AA621UofTUA?si=gn4XutRMWUDSYLFL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Please summarize the sentence according to the following FINAL REQUEST. \n",
    "FINAL REQUEST:\n",
    "1. The provided summary sections are partial summaries of one document. Please combine them into a single cohesive summary.\n",
    "2. Summarize the main points in bullet points in KOREAN.\n",
    "3. Each summarized sentence must start with a single emoji that fits the meaning of the sentence.\n",
    "4. Use various emojis to make the summary more interesting, but keep it concise and relevant.\n",
    "5. Focus on identifying and presenting only one main topic and one overall summary for the document.\n",
    "6. Avoid redundant or repeated points, and ensure that the summary covers all key ideas without introducing multiple conclusions or topics.\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\n",
    "FINAL SUMMARY:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please summarize the sentence according to the following REQUEST.\n",
      "REQUEST:\n",
      "1. Summarize the main points in bullet points in KOREAN.\n",
      "2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\n",
      "3. Use various emojis to make the summary more interesting.\n",
      "4. Translate the summary into KOREAN if it is written in ENGLISH.\n",
      "5. DO NOT translate any technical terms.\n",
      "6. DO NOT include any unnecessary information.\n",
      "\n",
      "CONTEXT:\n",
      "{context}\n",
      "\n",
      "SUMMARY:\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Please summarize the sentence according to the following REQUEST.\\nREQUEST:\\n1. Summarize the main points in bullet points in KOREAN.\\n2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\\n3. Use various emojis to make the summary more interesting.\\n4. Translate the summary into KOREAN if it is written in ENGLISH.\\n5. DO NOT translate any technical terms.\\n6. DO NOT include any unnecessary information.\\n\\nCONTEXT:\\n{context}\\n\\nSUMMARY:\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please summarize the sentence according to the following FINAL REQUEST. \n",
      "FINAL REQUEST:\n",
      "1. The provided summary sections are partial summaries of one document. Please combine them into a single cohesive summary.\n",
      "2. Summarize the main points in bullet points in KOREAN, but DO NOT translate any technical terms.\n",
      "3. Each summarized sentence must start with a single emoji that fits the meaning of the sentence.\n",
      "4. Use various emojis to make the summary more interesting, but keep it concise and relevant.\n",
      "5. Focus on identifying and presenting only one main topic and one overall summary for the document.\n",
      "6. Avoid redundant or repeated points, and ensure that the summary covers all key ideas without introducing multiple conclusions or topics.\n",
      "7. Please refer to each summary and indicate the key topic.\n",
      "8. If the original text is in English, we have already provided a summary translated into Korean, so please do not provide a separate translation.\n",
      "\n",
      "CONTEXT: \n",
      "{context}\n",
      "\n",
      "FINAL SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(\"Please summarize the sentence according to the following FINAL REQUEST. \\nFINAL REQUEST:\\n1. The provided summary sections are partial summaries of one document. Please combine them into a single cohesive summary.\\n2. Summarize the main points in bullet points in KOREAN, but DO NOT translate any technical terms.\\n3. Each summarized sentence must start with a single emoji that fits the meaning of the sentence.\\n4. Use various emojis to make the summary more interesting, but keep it concise and relevant.\\n5. Focus on identifying and presenting only one main topic and one overall summary for the document.\\n6. Avoid redundant or repeated points, and ensure that the summary covers all key ideas without introducing multiple conclusions or topics.\\n7. Please refer to each summary and indicate the key topic.\\n8. If the original text is in English, we have already provided a summary translated into Korean, so please do not provide a separate translation.\\n\\nCONTEXT: \\n{context}\\n\\nFINAL SUMMARY:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "\n",
    "yt = YouTube(url)\n",
    "audio_stream = yt.streams.filter(only_audio=True).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "# model = WhisperModel(\n",
    "#     \"large-v3\", device='cuda', compute_type=\"bfloat16\"\n",
    "# )\n",
    "# model = BatchedInferencePipeline(model=model)  # 배치 모델일 경우\n",
    "# print(\"Whisper 모델 초기화 완료\")\n",
    "\n",
    "\n",
    "# segments, info = model.transcribe(\n",
    "#     audio_stream.url,\n",
    "#     batch_size=64,  # 배치 모델인 경우\n",
    "#     repetition_penalty=1.5,\n",
    "#     beam_size=10,\n",
    "#     patience=2,\n",
    "#     no_repeat_ngram_size=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../miniconda3/envs/youtube/lib/python3.10/site-packages/faster_whisper/assets/pyannote_vad_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper 모델 초기화 완료\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.4.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Starting parallel download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parallel downloading: 100%|██████████| 32.3M/32.3M [00:02<00:00, 11.3MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio chunks:   0%|          | 0/177 [00:00<?, ?it/s]/home/jinu/miniconda3/envs/youtube/lib/python3.10/site-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/jinu/miniconda3/envs/youtube/lib/python3.10/site-packages/torch/nested/__init__.py:220: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  return _nested.nested_tensor(\n",
      "Processing audio chunks: 100%|██████████| 177/177 [03:07<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 -> 29.82:  꼼꼼한 딥러닝 논문 리뷰와 코드 실습. 이번 시간에 리뷰할 논문은 현대 딥러닝 기반의 자연어처리 기술의 핵심 아키텍처가 되고 있는 트랜스포머입니다. 트랜스포머 논문의 원래 제목은 Attention is all you need 입니다. 논문의 제목에서 알 수 있듯이 트랜스포머라는 아키텍처에는 이 Attention이라고 하는 것이 가장 메인 아이디어로서 사용이 된다는 걸 알 수 있습니다. 실제로 트랜스포머는 Attention이라는 메커니즘을 전적으로 활용하는 아키텍처입니다.\n",
      "30.00 -> 52.40:  트랜스포머가 나오게 된 계기를 이해하기 위해서 딥러닝 기반의 기계 번역 발전 과정에 대해 확인해 보겠습니다. 2021년 기준으로 최신 자연어처리 쪽 고성능 모델들은 이런 트랜스포머 아키텍처를 기반으로 하고 있습니다. 최근까지 화제가 되었던 GPT와 BERT는 모두 이러한 트랜스포머의 아키텍처를 적절히 활용하여 좋은 성능을 내고 있습니다.\n",
      "60.00 -> 89.66:  있다는 점이 특징입니다. 자연어처리 태스크 중에서 가장 대표적이면서 중요한 태스크 중 하나는 기계 번역입니다. 실제로 기계 번역 기술의 발전 과정을 확인해 보시면 1986년도 즈음에 RNN이 제한되었고 그로부터 약 10년 정도가 지난 뒤에 LSTM이 등장하였습니다. 이러한 LSTM을 활용하면 다양한 시퀀스 정보를 모델링할 수 있는데요. 대표적으로 주가 예측, 주기함수 예측 등이 가능합니다. 이러한 LSTM을 활용해서 2014년도에는 딥러닝 기반 기술로\n",
      "90.00 -> 118.84:  시퀀스트 시퀀스가 등장하였습니다. 시퀀스트 시퀀스는 현대의 딥러닝 기술들이 다시 빠르게 나오기 시작한 시점인 2014년도에 이러한 LSTM을 활용해서 고정된 크기의 컨텍스트 벡터를 사용하는 방식으로 번역을 수행하는 방법을 제안하였습니다. 다만 이러한 시퀀스트 시퀀스 모델이 나왔을 때의 시점만 하더라도 고정된 크기의 컨텍스트 벡터를 쓰고 있기 때문에 소스 문장을 전부 고정된 크기의 한 벡터에다가 압축을 할 필요가 있다는 점에서\n",
      "120.00 -> 141.36:  이후에 어텐션 메커니즘이 제한된 논문이 나오면서 이러한 시퀀스트 시퀀스 모델에 어텐션 기법을 적용하여 성능을 더 끌어올릴 수가 있었고요. 이제 그 이후에 트랜스포머 논문에서는 그냥 RNN 자체를 사용할 필요가 없다는 아이디어로 오직 어텐션 기법에 의존하는 아키텍처를 설계했더니 성능이 훨씬 좋아지는 것을 보여주었습니다.\n",
      "150.00 -> 176.50:  어텐션 메커니즘을 더욱더 많이 사용하게 되었습니다. 그래서 어텐션 메커니즘이 등장한 이후로부터는 입력 시퀀스 전체에서 정보를 추출하는 방향으로 연구 방향이 발전되어 왔다고 할 수 있습니다. 물론 이후에 나온 논문들 중에서도 RNN을 활용하는 아키텍쳐도 많이 존재하지만 전반적인 추세 자체는 어텐션 기법을 더욱더 활용하는 이런 트랜스포머의 아키텍쳐를 따르는 방식으로 다양한 고성능 모델들이 제한되고 있습니다.\n",
      "180.00 -> 208.14:  어떤 한계점이 존재할까요? 기존 시퀀스트 시퀀스 모델의 한계점이라고 한다면 이 컨텍스트 벡터 V에 소스 문장의 정보를 압축한다는 점입니다. 이때 병목 현상이 발생할 수 있기 때문에 성능 하락의 원인이 될 수 있는데요. 현재 예시를 확인해 보시면 대표적인 시퀀스트 시퀀스 모델을 활용한 기계 번역 예시라고 할 수 있습니다. 왼쪽에 있는 독일어 문장, 즉 각각의 단어들로 구성된 하나의 시퀀스가 들어왔을 때 이렇게 중간에서 하나의 고정된 크기의\n",
      "210.00 -> 236.02:  다시 이러한 컨텍스트 벡터로부터 출력 문장을 만들어내는 것을 확인할 수 있습니다. 즉 한쪽의 시퀀스에서부터 다른 한쪽의 시퀀스를 만든다는 의미에서 시퀀스 투 시퀀스 모델이라고 부를 수 있습니다. 결과적으로 이렇게 영어 출력 문장이 나오는 걸 확인할 수 있고요. 다만 이때 이러한 시퀀스 투 시퀀스 아키텍처를 확인해 보시면 매번 단어가 입력될 때마다 히든 스테이트 값을 갱신하는 걸 확인할 수 있습니다.\n",
      "240.00 -> 269.80:  단어들에 대한 정보를 포함하고 있는 히든스테이트 값을 받아서 매번 이런 식으로 히든스테이트 값을 새롭게 갱신합니다. 즉 이런 식으로 각각의 단어가 차례대로 순서에 맞게 입력될 때마다 히든스테이트 값이 갱신되어 이러한 히든스테이트 값은 이전까지 입력되었던 단어들에 대한 정보를 갖고 있기 때문에 이렇게 마지막 단어가 들어왔을 때 그때의 히든스테이트 값은 소스 문장 전체를 대표하는 하나의 컨텍스트 벡터로서 사용할 수가 있다는 것입니다. 그렇기 때문에\n",
      "270.00 -> 296.02:  이렇게 마지막 단어가 들어왔을 때의 히든스테이트 값을 하나의 컨텍스트 벡터로서 이 컨텍스트 벡터 안에는 앞에 등장했던 소스 문장에 대한 문맥적인 정보를 담고 있다고 가정하는 것입니다. 그렇기 때문에 이러한 컨텍스트 벡터로부터 출발해서 이렇게 출력을 수행하는 디코더 파트에서는 매번 출력 단어가 들어올 때마다 이러한 컨텍스트 벡터로부터 출발해서 마찬가지로 히든스테이트를 만들어서 매번 출력을 내보냅니다.\n",
      "300.00 -> 328.14:  입력으로 들어와서 반복적으로 이전까지 출력했던 단어에 대한 정보를 가지고 있는 히든스테이트와 같이 입력을 받아 새롭게 히든스테이트를 갱신하는 걸 확인할 수 있습니다. 이런 식으로 디코더 파트에서는 매번 히든스테이트 값을 갱신하면서 이렇게 히든스테이트 값으로부터 출력 값이 end of sequence가 나올 때까지 반복합니다. 그래서 end of sequence가 나왔을 때 출력 문장 생성을 마치게 되고요. 이렇게 출력된 정보인 good evening이 나오는 걸 확인할 수 있습니다.\n",
      "330.00 -> 356.80:  시퀀스트 시퀀스 모델의 동작 원리입니다. 다만 확인해 보시면 이렇게 소스 문장을 대표하는 하나의 컨텍스트 벡터를 만들어야 한다는 점에서 이렇게 고정된 크기의 컨텍스트 벡터의 정보를 압축하려고 하면 이러한 입력 문장은 어떨 때는 짧기도 하고 어떨 때는 길기도 하기 때문에 그러한 다양한 경우의 수에 대해서 항상 소스 문장의 정보를 고정된 크기로 가지고 있는 것은 전체 성능에서 병목현상의 원인이 될 수 있습니다.\n",
      "360.00 -> 388.72:  이 고정된 크기의 컨텍스트 벡터를 매번 이 디코더의 RNN 셀에서 참고하도록 만들어서 조금 더 성능을 개선할 수 있습니다. 이렇게 하게 되면 이 컨텍스트 벡터에 대한 정보가 이 디코더 파트의 RNN 셀을 거침에 따라서 정보가 손실되는 정도를 더 줄일 수 있기 때문에 출력되는 문장이 길어진다고 하더라도 각각의 출력되는 단어에 이러한 컨텍스트 벡터에 대한 정보를 다시 한번 넣어줄 수 있어서 성능이 기존보다 조금 더 향상될 수 있습니다.\n",
      "390.00 -> 419.52:  접근한다고 하더라도 여전히 이 소스 문장을 하나의 벡터에 압축해야 된다는 점은 동일하기 때문에 병목 현상은 여전히 발생합니다. 즉 현재의 문제 상황이라고 한다면 하나의 문맥 벡터 즉 컨텍스트 벡터가 소스 문장의 모든 정보를 가지고 있어야 하기 때문에 성능이 저하될 수 있다는 것입니다. 그렇다면 디코더 파트에서는 하나의 문맥 벡터에 대한 정보만 가지고 있는 게 아니라 출력 단어를 만들 때마다 매번 소스 문장에서의 출력 값들 전부를 입력으로 받으면 어떨까요?\n",
      "420.00 -> 448.72:  아이디어가 나올 수 있는 거죠. 최신 GPU는 많은 메모리와 그리고 빠른 병렬 처리를 지원하기 때문에 소스 문장의 시퀀스 길이가 길다고 하더라도 그러한 소스 문장을 구성하는 각각의 단어에 대한 출력값들 전부를 특정 행렬에다가 기록해 놓았다가 소스 문장에 대한 전반적인 내용들을 매번 출력할 때마다 반영할 수 있기 때문에 성능이 좋아질 것을 기대할 수 있습니다. 다시 말해 하나의 고정된 크기의 컨텍스트 벡터에 담지 말고 그냥 소스 문장에서 나왔던\n",
      "450.00 -> 479.76:  매번 입력으로 받아서 1년의 처리 과정을 거쳐서 출력 단어를 만들도록 하면 성능이 더 좋아질 수 있다는 겁니다. 지금 보이는 아키텍처가 바로 시퀀스 시퀀스에 어텐션 메커니즘을 적용한 아키텍처인데요. 이렇게 어텐션 메커니즘을 적용해서 인코더 파트의 모든 출력을 참고하도록 만들 수가 있습니다. 실제로 파이톨치와 같은 프레임워크에서는 단순히 RNN이나 LSTM 같은 걸 사용하도록 만들면 이렇게 매번 전체 시퀀스 기에 맞는 아웃풋 값들이 따로\n",
      "480.00 -> 509.62:  출력값들이 나오게 되는데요. 이제 그걸 그대로 이용해서 실제로 어텐션 메커니즘을 간단하게 구현할 수도 있습니다. 전반적인 내용을 확인해 보시면 이렇게 매번 단어가 출력돼서 히든스테이트가 나올 때마다 그냥 이 값들을 전부 다 출력값으로써 그냥 별도의 배열에다가 다 기록해놓습니다. 그래서 이런 식으로 각각의 단어를 거치면서 갱신되는 히든스테이트 값들을 매번 다 가지고 있는 거예요. 이렇게 해줌으로써 이렇게 매 단어가 들어왔을 때의 히든스테이트 값을 전부 가지고 있을 수 있기 때문에\n",
      "510.00 -> 536.78:  이러한 값들을 어떻게든 참고해서 이렇게 출력 단어가 매번 생성될 때마다 이러한 소스 문장 전체를 반영하겠다 라는 아이디어라고 보시면 되겠습니다. 실제로는 이렇게 디코더 파트에서 매번 히든스테이트를 갱신하게 되는데 이때 현재 단계에서 히든스테이트 값을 만든다고 하면 바로 이전의 히든스테이트 값을 이용해서 이 출력단의 히든스테이트 값과 이렇게 소스 문장단의 히든스테이트 값을 서로 묶어서\n",
      "540.00 -> 551.50:  이제 이때 그 에너지 값은 내가 현재 어떠한 단어를 출력하기 위해서 소스 문장에서 어떤 단어에 초점을 둘 필요가 있는지를 수치화해서 표현한 값입니다.\n",
      "570.00 -> 598.74:  더해준 다음에 이제 그러한 weighted sum 값을 매번 출력 단어를 만들기 위해서 반영을 하겠다라고 보시면 됩니다. 그래서 단순히 이렇게 context vector만 참고하는 것이 아니라 여기에 더불어서 소스 문장에서 출력이 되었던 모든 히든 스테이트 값들을 전부 반영해서 이러한 소스 문장의 단어들 중에서 어떤 단어에 더욱 더 주의 집중해서 출력 결과를 만들 수 있는가를 우리 모델이 고려하도록 만들어서 성능을 더욱 높일 수가 있다는 겁니다.\n",
      "600.00 -> 628.86:  매번 출력할 때마다 이 소스 문장에서 나왔던 모든 출력값들을 전부 참고하겠다 이겁니다. 즉 이렇게 압축된 컨텍스트 벡터 하나만 보는 것이 아니라 이런 출력값들을 전부 고려한 하나의 weighted sum 벡터를 구한 다음에 걔를 이렇게 같이 입력으로 넣어줘서 소스 문장에 대한 정보를 모두 고려할 수 있도록 만들기 때문에 성능이 좋아질 수 있는 것입니다. 자 그래서 실제로 이 attention 기법을 적용했을 때 디코더 파트에서 각각의 출력 단어를 만드는 과정을\n",
      "630.00 -> 656.54:  다음과 같이 정리할 수가 있는데요. 이때 i는 현재 디코더가 처리 중인 인덱스가 되겠습니다. 즉 디코더가 매번 한 번에 하나의 단어를 만드는데 그 각각의 처리 중인 인덱스가 i가 되겠고요. 이때 이 j 같은 경우는 인코더 파트에서 출력 인덱스가 되겠습니다. 즉 에너지 값이라고 하는 것은 매번 디코더가 출력 단어를 만들 때마다 모든 j를 고려하는 겁니다. 즉 인코더의 모든 출력들을 고려하겠다라고 보시면 돼요.\n",
      "660.00 -> 687.72:  단어를 만들기 위해 사용했던 히든스테이트가 되겠고요. 여기 H는 인코더 파트의 각각의 히든스테이트입니다. 즉 이걸 간단하게 정리하자면 디코더 파트에서 내가 이전에 출력했던 정보는 이거인데 이 정보와 인코더의 모든 출력값과 비교를 해서 에너지값을 구하겠다는 겁니다. 즉 어떤 H값과 가장 많은 연관성을 가지는지를 에너지값으로 구할 수가 있는 거고 이제 이러한 에너지값에 소프트맥스를 취해서 확률값을 구합니다.\n",
      "690.00 -> 714.62:  각각의 H값들 중에서 어떤 값과 가장 연관성이 높은지를 구하도록 만들고 이제 이러한 가중치 값을 실제로 H값과 곱하도록 만들어서 이러한 가중치가 반영된 각각의 인코더의 출력 결과를 더해서 그것을 활용하는 것입니다. 그래서 이 그림은 실제로 이 어텐션 메커니즘을 제안한 논문에서 보여주고 있는 그림인데요. 보시면 마찬가지로 에너지랑 가중치의 공식은 동일합니다.\n",
      "720.00 -> 746.54:  히든스테이트 값을 이용해서 만들 수가 있는데요. 현재 히든스테이트, 즉 ST를 만들기 위해서 이전에 사용했던 히든스테이트 값과 이 인코더 파트의 모든 각각의 히든스테이트 값을 같이 묶어서 에너지 값을 구한 뒤에 이제 거기에 소프트맥스를 취해서 이렇게 비율 값을 구할 수 있는 거예요. 즉 그러한 비율이 각각 이 A가 되겠습니다. 예를 들어서 만약에 입력 문장이 I am a teacher라고 해볼게요.\n",
      "750.00 -> 777.64:  단어들 중에서 어떤 걸 가장 많이 참고하면 되는지를 그 비율값을 이렇게 퍼센테이지로 구해주는 겁니다. 예를 들어서 i는 70%, m은 20%, 어는 5%, 티쳐도 5% 이런 식으로 다 더했을 때 100이 될 수 있도록 그 확률값을 구해서 그 비율만큼 실제로 이 h값을 곱한 것을 이 컨텍스트 벡터처럼 사용을 할 수 있다는 겁니다. 그래서 매번 출력을 할 때마다 이렇게 소스 문장에서 출력되었던\n",
      "780.00 -> 809.28:  이렇게 다음에 뭘 출력할지를 만들 수 있다라는 겁니다. 그래서 이렇게 매번 출력 단어를 만들 때마다 이렇게 소스 문장에서 등장했던 모든 히든스테이트 값들을 전부 반영하도록 만들어서 사용할 수가 있는 겁니다. 즉 다시 한번 용어를 정리해드리자면 이 에너지 값은 소스 문장에서 나왔던 모든 출력 값들 중에서 어떤 값과 가장 연관성이 있는지를 구하기 위해서 그 수치를 구한 것이고요. 이제 그 값들을 소프트맥스에 넣어서 상대적인 확률 값을 구한 것이\n",
      "810.00 -> 838.86:  그러한 가중치 값들을 실제로 각각의 그 소스 문장의 히든스테이트와 곱해줘서 전부 더해준 값을 실제로 디코더의 입력으로 같이 넣어주겠다라고 보시면 되겠습니다. 이제 이런 식으로 어텐션 메커니즘을 쓰게 되면 성능이 좋아질 뿐만 아니라 또 추가적인 장점으로는 이러한 어텐션은 시각화할 수도 있다는 건데요. 이런 식으로 어텐션 가중치 즉 구해진 확률 값을 이용해서 매번 출력이 나올 때마다 그 출력이 입력에서 어떤 정보를 참고했는지를 구할 수가 있습니다.\n",
      "840.00 -> 869.10:  이거는 영어를 불어로 번역한 예시인데요. 여기서 the arrangement on the European 이렇게 나오죠. 각각의 단어들이 있을 때 이제 매번 이 불어에 단어들을 출력할 때마다 이러한 입력 단어들 중에서 어떤 단어에 가장 많은 초점을 뒀는지를 구할 수가 있는 겁니다. 지금 이런 식으로 밝게 표시된 부분이 확률값이 높은 부분이라고 할 수 있습니다. 이런 식으로 매번 출력할 때마다 이 출력하는 단어가 입력 단어 중에서 어떤 단어에 더욱 많은 가중치를 두어서\n",
      "871.44 -> 899.40:  기본적으로 딥러닝은 매우 많은 파라미터를 가지고 있기 때문에 그러한 세부적인 파라미터를 일일이 분석하면서 어떤 원리로 동작을 했는지를 알아내기는 쉽지 않습니다. 그렇기 때문에 이렇게 어텐션 메커니즘은 실제로 딥러닝이 어떤 요소에 더욱더 많은 초점을 두어서 분류를 했는가 혹은 어떠한 데이터를 만들어냈는가 같은 과정을 분석할 때 용이하게 사용할 수 있습니다. 그렇다면 오늘 리뷰하고 있는 트랜스포머 논문은 어떤 원리로 동작할까요?\n",
      "905.44 -> 927.78:  그래서 논문의 원래 제목은 attention is all you need 이고요. 말 그대로 attention 기법만 잘 활용해도 다양한 자연어처리 태스크에서 좋은 성능을 얻을 수 있다는 의미입니다. 다시 말해 attention 기법만 쓰기 때문에 RNN, CNN 등을 전혀 필요로 하지 않습니다. 진짜 말 그대로 attention 기법만 사용해서 기계 번역부터 시작해서 다양한 자연어처리 태스크를 수행할 수 있는 겁니다.\n",
      "930.00 -> 955.82:  논문에서 보여주고 있는 트랜스포머의 아키텍처인데요. 여기 보이는 것과 같이 실제로 RNN과 CNN을 전혀 사용하지 않습니다. 물론 이런 식으로 RNN, CNN 등을 전혀 사용하지 않는다면 문장 안에 포함되어 있는 각각의 단어들의 순서에 대한 정보를 주기가 어렵습니다. 그렇기 때문에 트랜스포머는 문장 내 각각의 단어들에 대한 순서에 대한 정보를 알려주기 위해서 별도로 포지셔널 인코딩을 이용해서 순서에 대한 정보를 줄 수 있습니다.\n",
      "960.00 -> 988.88:  더욱 향상된 네트워크에서도 채택이 되었고요. 또한 참고로 RNN을 사용하지 않지만 마찬가지로 인코더와 디코더 파트로 구성되는 건 동일합니다. 또한 어텐션 과정을 한 번만 쓰는 게 아니라 여러 레이어를 거쳐서 반복하도록 만듭니다. 즉 이러한 인코더가 여러 번 중첩되어 즉 N번만큼 중첩되어 사용하도록 만든다는 건데요. 참고로 지금 보이는 그림에서 이 왼쪽 파트는 인코더가 되고 이 오른쪽 파트는 디코더가 되겠습니다. 한번 자세한 내용을 지금부터 알아볼게요.\n",
      "990.00 -> 1018.50:  어떠한 단어 정보를 네트워크에 넣기 위해서는 일반적으로 보통 인베딩 과정을 거칩니다. 그렇게 해주는 이유는 일단 맨 처음에 입력 차원 자체는 특정 언어에서 존재할 수 있는 단어의 개수와 같기 때문에 또한 그렇게 차원이 많을 뿐만 아니라 각각의 정보들은 원핫 인코딩 형태로 표현이 되기 때문에 일반적으로 네트워크에 넣을 때는 먼저 인베딩 과정을 거쳐서 더욱더 적은 차원의 컨티뉴어스한 값으로 표현합니다. 즉 어떠한 실수 값으로 표현할 수가 있다는 건데요.\n",
      "1020.00 -> 1046.74:  이런 식으로 I am a teacher와 같은 하나의 문장이 들어왔을 때 얘네는 실제로 input embedding matrix로 만들어집니다. 이때 일반적으로 이 matrix는 단어의 개수만큼 행의 크기를 가지고요. 즉 이런 식으로 I am a teacher라는 값이 이렇게 행 형태로 들어오게 되고 이 각각의 열 데이터는 embedding 차원과 같은 크기의 데이터가 담긴 배열을 사용하게 됩니다. 현재 그림에선 이런 식으로 총 4개의 단어가 존재하기 때문에\n",
      "1050.00 -> 1079.68:  를 포함하고 있는 인베딩 값들을 각각 구할 수가 있다는 거죠. 이런 식으로 다 구할 수가 있는 겁니다. 이러한 인베딩 디멘저는 모델 아키텍처를 만드는 사람이 임의로 설정해 줄 수 있는데요. 원본 논문에서는 512 정도의 값을 사용합니다. 물론 이 값은 모델의 아키텍처를 만드는 사람마다 다르게 설정할 수가 있는 거예요. 아무튼 그래서 이런 식으로 전통적인 인베딩은 네트워크에 넣기 전에 입력 값들을 인베딩 형태로 표현하기 위해서 사용하는 레이어라고 볼 수 있습니다. 이때 우리가 시퀀스 투 시퀀스와 같은\n",
      "1080.00 -> 1109.94:  RNN 기반의 아키텍처를 사용한다고 하면 RNN을 사용하는 것만으로도 각각의 단어가 RNN에 들어갈 때 순서에 맞게 들어가기 때문에 자동으로 각각의 히든스텔트 값은 순서에 대한 정보를 가지게 되는데요. 만약에 트랜스포머와 같이 RNN 자체를 사용하지 않는다면 위치에 대한 정보를 주기 위해서 즉 하나의 문장에 포함되어 있는 각각의 단어 중에서 어떤 단어가 앞에 오는 것이고 어떠한 단어가 뒤에 오는 것인지 그러한 정보를 알려주기 위해서는 위치에 대한 정보를 포함하고 있는 인베딩을 사용할 필요가 있습니다.\n",
      "1110.00 -> 1133.84:  이제 이를 위해 트랜스포머에서는 위치에 대한 정보를 인코딩하고 있는 위치 인코딩, 즉 포지셔널 인코딩을 사용합니다. 즉 이런 식으로 인풋 임베딩 매트릭스와 같은 크기, 즉 같은 디멘전을 가지는 별도의 위치에 대한 정보를 가지고 있는 인코딩 정보를 넣어줘서 각각 엘러먼트 와이즈로 더해줌으로써 각각의 단어가 어떤 순서를 가지는지에 대한 정보를 네트워크가 알 수 있도록 만드는 것입니다.\n",
      "1140.00 -> 1162.08:  즉 이렇게 attention이 받는 값은 입력 문장에 대한 정보에다가 실제 위치에 대한 정보까지 같이 포함되어 있는 입력 값입니다. 이제 그래서 그러한 입력을 받아서 각각의 단어들을 이용해서 attention을 수행하고요. 이제 이렇게 인코더 파트에서 수행하는 attention은 self-attention이라고 해서 각각의 단어가 서로에게 어떤 연관성을 가지고 있는지를 구하기 위해 사용합니다.\n",
      "1170.00 -> 1197.58:  시처가 각각 서로에게 attention 스코어를 구해서 각각의 단어는 다른 어떠한 단어와 높은 연관성을 가지는지에 대한 정보를 학습하도록 만들 수 있습니다. 다시 말해 이러한 attention은 이 전반적인 입력 문장에 대한 문맥에 대한 정보를 잘 학습하도록 만드는 것입니다. 또한 여기에서 추가적으로 residual learning과 같은 테크닉이 사용되는데요. 이런 residual learning 같은 경우는 대표적인 이미지 분류 네트워크인 레지넥과 같은 네트워크에서 사용되고 있는 기법으로\n",
      "1200.00 -> 1226.68:  반복적으로 단순하게 갱신하는 것이 아니라 특정 레이어를 건너뛰어서 복사가 된 값을 그대로 넣어주는 기법을 의미합니다. 이런 식으로 특정 레이어를 건너뛰어서 입력할 수 있도록 만드는 것을 일반적으로 레지듀얼 커넥션이라고 부르고요. 이렇게 해줌으로써 전체 네트워크는 기존 정보를 입력 받으면서 추가적으로 잔여된 부분만 학습하도록 만들기 때문에 전반적인 학습 난이도가 낮고 그렇기 때문에 초기의 모델 수렴 속도가 높게 되고\n",
      "1230.00 -> 1258.86:  전반적으로 다양한 네트워크에 대해서 residual learning을 사용했을 때 성능이 좋아지는 걸 많이 목격할 수 있고요. 트랜스포머 또한 마찬가지로 그런 아이디어를 전적으로 재택해서 성능을 높였다고 할 수 있는 겁니다. 그래서 이렇게 attention을 수행해주고 나온 값과 이렇게 residual connection을 이용해서 바로 더해진 값을 같이 받아서 노멀라이제이션까지 수행한 뒤에 그 결과를 내보낼 수 있도록 만듭니다. 이것이 인코더의 동작 과정이고요. 그래서 실제로 이렇게 입력값이 들어온 이후로부터\n",
      "1260.00 -> 1289.88:  레지듀얼 커넥션 이후에 노멀라이제이션, 그 다음에 다시 피드포워드 레이어를 거친 다음에 마찬가지로 레지듀얼 러닝, 그리고 노멀라이제이션을 추가해서 결과적으로 하나의 인코더 레이어에서 그 결과값을 뽑아낼 수 있습니다. 이런 식으로 어텐션과 정교화 과정을 반복하는 방식으로 여러 개의 레이어를 중첩해서 사용합니다. 이때 한 가지 유의할 점은 각각의 레이어는 서로 다른 파라미터를 가집니다. 예를 들어 레이어 1번과 레이어 2번에 포함되어 있는 어텐션 및 피드포워드 레이어에 사용되는 파라미터는\n",
      "1290.00 -> 1319.56:  서로 다릅니다. 또한 이때 이렇게 레이어를 중첩해서 사용할 수 있다는 점에서 유추할 수 있겠지만 이렇게 입력되는 값과 출력되는 값의 디멘저는 동일합니다. 이제 그래서 실제로는 다음과 같이 전체 인코더와 디코더의 아키텍처를 그려볼 수 있는데요. 이렇게 입력값이 들어온 다음에 여러 개의 인코더 레이어를 반복해서 가장 마지막의 인코더에서 나오게 된 그 출력값은 이렇게 디코더에 들어가게 됩니다. 이렇게 해주는 이유는 우리가 앞서 시퀀스 투 시퀀스 모델의 어텐션 메커니즘을 활용했을 때와 마찬가지로\n",
      "1320.00 -> 1348.42:  디코더 파트에서는 매번 출력할 때마다 입력 소스 문장 중에서 어떤 단어에게 가장 많은 초점을 둬야 되는지를 알려주기 위함입니다. 다시 말해 이렇게 디코더 파트도 마찬가지로 여러 개의 레이어로 구성이 되고 이 마지막 레이어에서 나오게 된 출력 값이 바로 실제로 우리가 번역을 수행한 결과 그 출력 단어가 되는 거고요. 이때 각각의 레이어는 이 인코더의 마지막 레이어에서 나오게 된 출력 값을 입력으로 받는 것입니다. 이것이 바로 트랜스포머의 가장 기본적인 동작 방식이고요.\n",
      "1350.00 -> 1376.32:  마지막 레이어의 출력값만 받는 게 아니라 이렇게 각각의 레이어마다 출력값을 받는 기법도 존재하긴 하지만요. 아무튼 기본적인 트랜스포머의 아키텍션은 이런 식으로 인코더의 마지막 레이어에서 나오게 된 출력값을 매번 디코더의 레이어에 넣어주는 방식으로 동작합니다. 그래서 이때 디코더 또한 마찬가지로 각각 단어 정보를 받아서 이어서 각 단어의 상대적인 위치에 대한 정보를 알려주기 위해 인코딩 값을 추가한 뒤에 입력을 넣게 되고요.\n",
      "1380.00 -> 1405.00:  첫 번째로 보이는 attention은 self-attention으로 인코더 파트와 마찬가지로 각각의 단어들이 서로가 서로에게 어떠한 가중치를 가지는지를 구하도록 만들어서 이 출력되고 있는 문장에 대한 전반적인 표현을 학습할 수 있도록 만들고요. 이렇게 이어서 디코더 레이어의 두 번째 attention에서는 인코더에 대한 정보를 attention할 수 있도록 만듭니다. 다시 말해 각각의 출력 단어가 인코더의 출력 정보를 받아와 사용할 수 있도록 만듭니다.\n",
      "1410.00 -> 1437.46:  어떤 단어와 연관성이 있는지를 구해주는 겁니다. 그래서 여기 보이는 attention은 일반적으로 인코더 디코더 attention이라고 부르고요. 그 동작 원리를 간단한 예시로 설명드리자면 예를 들어 입력 문장이 I am a teacher라고 한다면 이렇게 출력값은 차례대로 나는 선생님이다. 이런 식으로 출력을 내뱉게 될 건데요. 이때 출력되고 있는 단어들 예를 들어서 선생님이라고 단어를 번역한다고 하면 그 선생님이라는 단어는 I am a teacher 중에서\n",
      "1440.00 -> 1468.76:  그러한 정보를 매번 어텐션을 통해서 계산하도록 만들어서 이렇게 인코더 파트에서 나왔던 출력 결과를 전적으로 활용하도록 네트워크를 설계할 수 있는 것입니다. 즉 그래서 디코로 또한 마찬가지로 입력으로 들어온 입력 디멘전과 이 출력 디멘전이 같도록 만들어서 각각의 디코더 레이어는 여러 번 중첩해서 사용할 수 있습니다. 즉 다시 말해 이 트랜스포머에서는 마지막 인코더의 레이어의 출력이 모든 디코더의 레이어에 입력되는 형식으로 동작하고요.\n",
      "1470.00 -> 1488.52:  레이어의 개수가 4개일 때의 예시는 다음과 같습니다. 일반적으로 이 레이어의 개수는 인코더와 디코더가 동일하도록 맞춰주는 경우가 많고요. 즉 이렇게 인코더랑 디코더 둘 다 4개의 레이어로 구성되어 있는 걸 확인할 수 있고 이렇게 인코더 파트에서 마지막 레이어의 출력값이 각각의 디코더 레이어에 입력되는 걸 확인할 수 있습니다.\n",
      "1500.00 -> 1527.64:  바로 여기 부분에서 사용된다고 보시면 되겠습니다. 또한 말씀드렸듯이 트랜스포머에서도 인코더와 디코더의 구조를 따릅니다. 즉 RNN을 사용하지 않는다는 점이 큰 차이점이고 인코더와 디코더를 다수 사용한다는 점이 특징입니다. 여기서 재미있는 점은 원래 기본적으로 RNN을 사용할 때는 인코더 즉 LSTM이나 RNN 등은 고정된 크기로 사용하고 이 입력 단어의 개수만큼 반복적으로 인코더 레이어를 거쳐서 매번 히든 스테이트를 만들었다고 하면\n",
      "1530.00 -> 1558.96:  자체가 하나로 쭉 연결되어서 한 번에 입력이 되고 한 번에 그에 대한 어텐션 값을 구한다고 할 수 있습니다. 즉 다시 말해 RNN을 사용했을 때와 다르게 위치에 대한 정보를 한꺼번에 넣어서 한 번에 인코더를 거칠 때마다 병렬적으로 출력 값을 구해낼 수 있기 때문에 RNN을 사용했을 때와 비교하여 일반적으로 계산 복잡도가 더 낮게 형성됩니다. 또한 실제로 학습을 수행할 때는 이러한 입력 값들 전부를 한꺼번에 넣을 수 있기 때문에 RNN을 사용하지 않고 학습을 진행할 수 있다는 점이 장점인데요.\n",
      "1560.00 -> 1585.34:  실제로 모델에서 출력값을 내보낼 때는 마찬가지로 이 디코더 아키텍처를 여러 번 사용해서 이렇게 EOS가 나올 때까지 반복하도록 만들어서 출력값을 구하도록 만들 수 있습니다. 보시면 이렇게 중간에 컨텍스트 벡터를 압축하는 과정 등이 완전히 생략이 되어 있기 때문에 네트워크 자체에서 LSTM과 같은 RNN 구조를 아예 사용할 필요가 없다는 점이 장점이라고 할 수 있습니다. 이제 그래서 실제로 이 멀티헤드 어텐션이 뭔지 한번 알아보도록 할게요.\n",
      "1590.00 -> 1617.70:  라고 해서 멀티헤드 어텐션이라고 부르는데요. 그 실제 구조는 바로 다음과 같습니다. 바로 오른쪽에 보이는 그림이 멀티헤드 어텐션을 보여주고 있는 그림이고요. 이때 이렇게 중간에 scaled.product.attention이 사용되는데요. 이러한 scaled.product.attention은 바로 왼쪽 그림과 같이 구성됩니다. 이때 이러한 어텐션 메커니즘을 이해하기 위해서는 Query와 Key, Value가 무엇인지 알 필요가 있는데요. 이때 Query는 무언가를 물어보는 주체입니다. 즉, 어텐션 메커니즘을 간단히 설명하면\n",
      "1620.00 -> 1649.34:  어떠한 연관성을 가지는지를 구하는 것이라 할 수 있는데요. 이때 물어보는 주체가 쿼리이고 그 물어보는 대상이 키입니다. 예를 들어 I am a teacher라는 하나의 문장이 있을 때 I am a teacher에 포함되어 있는 각각의 단어가 다른 단어와 얼마나 연관성을 가지는지 측정하기 위해서 셀프 어텐션을 수행할 수 있는데 이때 I라는 단어가 I am a teacher 각각에 대해서 얼마나 연관성이 있는지 구한다고 치면 그때 I가 쿼리가 되는 거고요. I am a teacher 각각 단어들은 키가 되는 것입니다.\n",
      "1650.00 -> 1675.00:  어떠한 단어가 다른 어떠한 단어들에 관해서 어떠한 가중치 값을 가지는지 구하고자 한다면 이런 식으로 각각의 키에 대해서 어텐션 스코어를 구해오는 방식으로 동작하는 것입니다. 이때 그렇게 스코어를 구한 뒤에는 실제로 밸류 값들과 곱해서 결과적인 어텐션 밸류 값을 구할 수 있는 겁니다. 내용을 확인해 보시면 이런 식으로 물어보는 주체, 즉 퀄이가 들어오고 각각의 어텐션을 수행할 단어들, 그 정보가 K로 들어가는 겁니다.\n",
      "1680.00 -> 1709.80:  소프트 맥스를 취해서 각각의 키 중에서 어떤 단어와 가장 높은 연관성을 가지는지를 그 비율을 구할 수 있습니다. 앞에서 공부했었던 어텐션 메커니즘과 같다고 할 수 있죠. 그렇게 구해진 확률값과 실제로 밸류값을 곱해서 가중치가 적용된 결과적인 어텐션 밸류를 구할 수가 있는 겁니다. 이제 그러한 과정이 이렇게 스케일드 닷 파라덕 어텐션에서 수행되는 것이고요. 또한 여기에서 참고로 실제로 입력값이 들어왔을 때 그러한 입력값들은 A 체계로 구분됩니다. 즉 어떠한 입력 문장이 들어왔을 때\n",
      "1710.00 -> 1735.44:  이제 그것은 Value, Key, Query로 구분되는데 이때 H계의 서로 다른 Value와 Key, Query로 구분될 수 있도록 만드는 것입니다. 이렇게 해주는 이유는 H계의 서로 다른 Attention 컨셉을 학습하도록 만들어서 더욱더 구분된 다양한 특징들을 학습할 수 있도록 유도해준다는 장점이 있습니다. 그래서 이와 같이 입력으로 들어온 값은 3개로 복제가 되어서 각각 Value, Key, Query로 들어가게 되고\n",
      "1740.00 -> 1769.56:  H계로 구분된 각각의 쿼리 쌍들을 만들어내게 되고 이때 여기에서 H는 헤드의 개수이기 때문에 각각 서로 다른 헤드끼리 이렇게 Value, Key, Query의 쌍을 받아서 Attention을 수행해서 결과를 내보냅니다. 이제 그 다음에 앞서 말씀드렸듯이 이 Attention 메커니즘의 입력값과 이 출력값의 디멘저는 같아야 되기 때문에 이렇게 각각의 헤드로부터 나오게 된 Attention 값들을 다시 이렇게 컨켓을 수행해서 일자로 쭉 붙인 뒤에 마지막으로 이 Linear 레이어를 거쳐서 Output 값을 내보내게 됩니다.\n",
      "1770.00 -> 1797.54:  이때 결과적으로 이 입력값과 출력값의 디멘전이 같도록 만들어서 이러한 멀티헤드 어텐션 레이어를 사용한 뒤에도 디멘전이 줄어들지 않도록 만듭니다. 바로 이런 식으로 각각의 어텐션 메커니즘이 사용되는 것이고요. 이러한 멀티헤드 어텐션은 이 전체 아키텍처에서 다 동일한 함수로서 동작합니다. 이때 다른 점이라고 한다면 이렇게 사용되는 위치마다 퀄이랑 키랑 밸류를 어떻게 사용할지가 달라질 수 있는 건데 그런 점을 제외하고 기본적인 각각의 어텐션 레이어의 동작 방식은 같습니다.\n",
      "1800.00 -> 1827.82:  인코더 어텐션에서는 디코더의 출력 단어가 쿼리가 되는 것이고 각각의 출력 단어를 만들기 위해서 인코더 파트에서의 어떤 단어를 참고하면 좋은지를 구하기 위해서 이 키랑 밸류의 값으로는 인코더의 출력 값을 쓰겠다는 겁니다. 다시 말해 각각의 단어를 출력하기 위해서 어떤 정보를 참고해야 해 라고 이렇게 인코더한테 물어보는 것이기 때문에 이 디코더 파트에 있는 단어가 쿼리가 되고 인코더 파트에 있는 각각의 값들이 키와 밸류가 된다고 할 수 있습니다.\n",
      "1830.00 -> 1858.34:  더욱 더 자세하게 수식으로 표현하면 다음과 같은데요. 자 이렇게 하나의 어텐션은 쿼리와 키와 밸류를 봤고요. 이때 쿼리랑 키랑 곱해서 각 쿼리에 대해서 각각의 키에 대한 에너지 값을 구할 수 있겠죠. 이제 그런 에너지 값에 대해서 확률 값으로 표현하도록 만들어서 실제로 어떤 키에 대해서 높은 가중치를 가지는지 계산할 수가 있고요. 이때 이렇게 스케일 섹터로서 루트 DK를 사용합니다. 이때 DK는 각각의 키 디멘전이 되겠고요. 이렇게 특정한 스케일로 나눠주는 이유는\n",
      "1860.00 -> 1887.94:  가지는 특성을 생각해보시면 0 근처의 위치에서는 그래디언트가 높게 형성되는 것에 반해 값이 들쭉날쭉 조금씩 왼쪽 오른쪽으로 이동하게 되면 교육의 값이 많이 줄어들기 때문에 그래디언 베니싱 문제를 피하기 위한 방법으로 이러한 스케일링 팩터를 넣어줄 수 있다고 논문에선 말하고 있습니다. 결과적으로 이렇게 각각의 쿼리가 각각의 키에 대해서 어떠한 가중치를 가지는지 스코어 값을 구한 뒤에 이제 걔를 실제로 밸류 값과 곱해서 어테션 밸류를 만들어낼 수가 있는 것입니다.\n",
      "1890.00 -> 1919.16:  들어오는 각각의 값에 대해서 서로 다른 리니어 레이어를 거치도록 만들어서 A측의 서로 다른 각각 쿼리 키 밸류 값을 만들 수 있도록 하는 것입니다. 이런 식으로 A측의 서로 다른 컨셉을 네트워크가 구분해서 학습하도록 만들므로써 어텐션을 수행하기 위한 다양한 피처들을 학습하도록 만듭니다. 실제로 나중에 우리가 어텐션 스코어를 시각화해 볼 때는 이 H의 개수만큼 어텐션 스코어의 그림이 나오게 됩니다. 결과적으로 이렇게 각 헤드에 대한 출력값들을 구할 수 있고\n",
      "1920.00 -> 1948.56:  일자로 쭉 붙인 다음에 마지막으로 아웃풋 매트릭스랑 곱해서 결과적인 멀티헤드 어텐션의 값을 구해낼 수가 있는 것입니다. 이런 식으로 매번 입력 값이 들어왔을 때 기본적으로는 이런 식으로 value와 key와 query의 값으로 각각 들어가게 되고 이렇게 나올 때는 입력으로 들어왔던 값과 동일한 디멘저를 가지기 때문에 이러한 멀티헤드 어텐션 레이어가 포함된 하나의 인코더 혹은 디코더 레이어는 중첩해서 사용될 수 있는 것입니다. 이제 한번 자세하게 트랜스포머의 동작 원리를 알아보겠습니다.\n",
      "1950.00 -> 1978.10:  하나의 단어만 있다고 가정을 해볼게요. 이때 attention을 위해서 각각의 head마다 query와 key value 값을 만들 필요가 있습니다. 그래서 이렇게 하나의 단어가 embedding 차원으로 표현되고 있는 상태에서 이제 여기에서 linear 레이어를 거쳐서 각각 query랑 key랑 value 값을 만들 수 있습니다. 이때 embedding 차원을 deep model이라고 부를 수 있고요. 원본 논문에서는 embedding 차원을 512 차원으로 사용한다고 언급을 했고요. 이때 만약에 head의 개수가 8개라고 하면\n",
      "1980.00 -> 2009.56:  64만큼 각각의 Query Key Value의 차원이 구성되는 것입니다. 여기 보이는 그림은 그냥 간단하게 인베딩 차원이 4차원이고 헤드가 2개라고 가정한 상황입니다. 즉 이제 이럴 때는 4 곱하기 2짜리 매트릭스가 만들어지겠죠. 왜냐하면 이 4차원의 데이터를 2차원의 데이터로 맵핑해야 되기 때문에 이렇게 4 곱하기 2 가중치 매트릭스가 사용되는 것입니다. 그래서 이런 식으로 Love라는 단어가 4차원으로 표현되어 있다고 하면 이제 이것은 Query Key Value 각각 2차원으로 구성되어 있는 데이터로 표현될 수 있는 것입니다.\n",
      "2010.00 -> 2037.20:  이제 이런 식으로 키랑 쿼리랑 밸류를 다 구했다고 치면 바로 다음의 공식으로 이용해서 실제로 attention value를 구할 수가 있는데요. 이때 이 쿼리는 각각의 다른 단어들 이 키와 행렬급을 수행해서 이렇게 하나의 attention energy 값을 구할 수가 있는 겁니다. 예를 들어 I love you라고 하나의 문장이 들어왔다고 하면 이 I라는 단어는 I에 해당하는 키, love에 해당하는 키, you에 해당하는 키 값과 각각 곱해져서 하나의 attention energy 값을 구할 수가 있는 거고요.\n",
      "2040.00 -> 2068.60:  들어가는 값의 크기를 노멀라이제이션 해주기 위해서 각각 스케일링 팩터로 나누어줍니다. 이제 이후에 소프트맥스를 취해서 실제로 각각의 키 값에 대해서 어떠한 가중치를 가지는지를 구해낼 수가 있는 것입니다. 여기 보이는 그림에서는 이 i라는 단어는 i라는 단어와 72%만큼의 높은 연괄성을 가지고 이 love라는 단어와는 15% 그 다음에 u라는 단어와는 13% 이렇게 각각의 가중치를 가진다고 표현할 수가 있는 거고요. 이렇게 각각의 가중치 값에다가\n",
      "2070.00 -> 2099.46:  각각 곱한 뒤에 전부 더해줘서 결과적인 attention value 값을 만들어낼 수가 있는 것입니다. 즉 마찬가지로 weighted sum을 구할 수가 있다는 거고요. 바로 이러한 과정을 통해서 실제로 attention이 수행되는 것입니다. 그래서 한번 실제로 전체 문장이 한꺼번에 입력되는 이런 행렬과 같은 상황에서 예시를 다시 한번 확인해보겠습니다. 실제로는 이런 식으로 행렬 곡셈 연산을 이용해서 한꺼번에 연산이 가능하고요. I love you라는 하나의 문장이 있고 그 다음에 인베딩 차원이 4차원이라고 했을 때 바로 이렇게 3x4짜리 매트릭스로 구성되는데요.\n",
      "2100.00 -> 2126.50:  이때 마찬가지로 하나의 헤드에 있는 이 Query Key Value를 구하기 위한 가중체 값이 이렇게 있다고 해볼게요. 현재 헤드에서는 바로 이런 식으로 I Love You에 대한 각각의 Query 값, Key 값, Value 값이 만들어지는 것입니다. 마찬가지로 이렇게 Query와 Key와 Value의 값이 구해졌기 때문에 Attention Value를 구할 수 있게 되는 건데요. 이렇게 I와 Love와 You, Query 값들을 한꺼번에 이렇게 각 Key 값과 곱해줘서 Attention Energy를 이렇게 3x3으로 만들어낼 수 있습니다.\n",
      "2130.00 -> 2159.06:  각각의 키 값에 대해서 얼마나 높은 그 연관성을 표현하는 수치를 부여했는지를 구할 수가 있는 겁니다. 즉 이런 식으로 Attention Energy 값은 I love you 각각에 대해서 구해지는 방식으로 이렇게 행과 열은 모두 단어의 개수와 동일한 크기를 가집니다. 각각의 단어가 서로에게 어떠한 연관성을 가지는지 구할 수가 있는 것이고요. 이제 여기에 Softness를 취해서 각각의 행마다 각 키에 대한 값들을 확률 값으로 구해낼 수 있도록 만드는 거고요. 이제 그러한 가중치 값들과 Value 값을 곱해 주어서\n",
      "2160.00 -> 2187.16:  어텐션 밸류 매트릭스를 구할 수 있습니다. 보시면 이제 이렇게 어텐션 밸류 값 자체는 입력되었던 쿼리와 키와 밸류와 모두 동일한 차원을 가집니다. 또한 한 가지 알아두시면 좋은 점은 마스크 행렬을 사용할 수 있다는 점인데요. 이 마스크 행렬, 즉 마스크 매트릭스는 특정한 단어를 무시할 수 있도록 하기 위해 사용할 수 있습니다. 이렇게 어텐션 에너지 값이 있을 때 어텐션 에너지와 같은 차원의 마스크 매트릭스를 만들어서 얘를 엘러먼트 와이즈로, 즉 각각의 원소 단위로 곱해 주어서\n",
      "2190.00 -> 2214.42:  예를 들어서 이렇게 이 I라는 단어는 이 러브와 U에 해당하는 키 값은 무시하도록, 즉 이 러브와 U는 그냥 어텐션 하지 않도록 무시하고자 한다면 이렇게 어텐션 에너지 값을 전부 다 마이너스 무한이라고 할 수 있는 가능한 최대로 작은 값을 넣어주게 되면 실제로 소프트맥스를 취해서 어텐션 스코어 값이 구해졌을 때 고려하지 않도록 처리가 된 그런 단어들에 대해서는 모두 0%의 가중치 값을 가지게 됩니다.\n",
      "2220.00 -> 2249.32:  이와 같이 마스크 매트릭스를 이용해서 이러한 어텐션 에너지 값에 마스크를 적용함으로써 특정 단어는 그냥 무시해서 어텐션을 수행하지 않도록 만들 수가 있는 것입니다. 그래서 결과적으로 이렇게 각각의 헤드마다 입력으로 들어온 쿼리와 키와 밸류와 같은 차원의 벡터를 만들어내기 때문에 이렇게 각 헤드마다 쿼리와 키와 밸류의 값들을 각각 넣어서 어텐션을 수행한 값들을 이렇게 다 헤드 1부터 헤드 H까지라고 했을 때 이러한 정보들을 다 일자로 쭉 연결하게 되면\n",
      "2250.00 -> 2277.22:  맨 처음에 입력이 되었던 이런 입력 디멘전과 같은 디멘전을 가지게 되는데요. 다시 말해 이런 식으로 멀티헤드 어텐션은 각각의 헤드에 대해서 어텐션을 수행한 뒤에 그러한 결과를 다시 쭉 이어붙이기 때문에 결과적으로 만들어진 매트릭스의 이 열의 개수는 원래 입력의 임베딩 차원과 동일한 값을 가집니다. 그렇기 때문에 이제 마지막에 이 W가중치 값으로 D모델 곱하기 D모델 차원을 가지는 매트릭스를 곱해줌으로써 결과적인 멀티헤드 어텐션의 값을 구할 수 있고요.\n",
      "2280.00 -> 2307.64:  정확히 동일하기 때문에 이러한 멀티헤드 어텐션을 수행한 뒤에도 차원이 동일하게 유지가 된다는 점이 특징입니다. 또한 앞서 간단하게 말씀드렸듯이 이 트랜스포머에는 세 가지 종류의 어텐션이 사용되는데요. 트랜스포머에 쓰이는 어텐션은 항상 멀티헤드 어텐션으로 헤드가 여러 개인 어텐션이라 볼 수 있는데 이제 그러한 어텐션이 사용되는 위치에 따라서 인코더 셀프 어텐션 그리고 마스트 디코더 셀프 어텐션 인코로 디코더 어텐션 이 세 가지 종류가 존재합니다. 기본적으로 인코더의 셀프 어텐션은 말씀드렸듯이\n",
      "2310.00 -> 2336.82:  가지는지를 어텐션을 통해서 구하도록 만들고 전체 문장에 대한 레프레젠테이션을 러닝할 수 있도록 만든다는 점이 특징이고요. 다만 이제 디코더 파트에서 셀프 어텐션을 수행할 때는 이렇게 각각의 출력 단어가 다른 모든 출력 단어를 전부 참고하도록 만들진 않고 앞쪽에 등장했던 단어들만 참고할 수 있도록 만듭니다. 예를 들어 출력 문장이 나는 축구를 했다 라고 하면은 우리가 축구를 이라는 단어를 출력할 때 있어서 했다 라고 이렇게 뒤쪽에 나오는 단어가 무엇인지\n",
      "2340.00 -> 2368.78:  치팅처럼 동작을 하기 때문에 모델이 정상적으로 학습이 되기가 어렵습니다. 그렇기 때문에 이 디코더 파트에서 attention을 수행할 때는 이렇게 각각의 단어에 대해서 이 앞쪽 단어들만 참고할 수 있도록 만들므로써 치팅을 하지 않고 정상적으로 모델이 학습될 수 있도록 만드는 것입니다. 마지막으로 인코더 디코더 attention은 쿼리가 디코더에 있고 각각의 키와 밸류는 인코더에 있는 상황을 의미하는 것입니다. 예를 들어 난 널 좋아해라고 I like you라고 문장이 들어왔을 때 출력 문장이 난 널 좋아해라고 나온다고 하면\n",
      "2370.00 -> 2396.48:  단어들이 이러한 입력 단어들 중에서 어떤 정보에 더욱 더 많은 가중치를 두는지를 구할 수 있어야 되는데요. 이제 그러한 과정에서 이 디코더 파트에 있는 쿼리 값이 이렇게 인코더 파트에 있는 키와 밸류 값을 참조한다고 해서 인코더 디코더 어텐션이라고 부르는 것입니다. 또한 이어서 셀프 어텐션에 대해서 알아볼 건데요. 실제로 이러한 셀프 어텐션은 말씀드렸듯이 인코더와 디코더 모두에서 사용되고요. 시각화 과정을 통해서 어텐션 스코어로 나온 값을 그려볼 수 있습니다.\n",
      "2400.00 -> 2428.38:  구할 수가 있는 건데요. 예를 들어 이런 식으로 하나의 입력 문장이 들어왔을 때 각각의 단어들은 다른 모든 단어에 대해서 attention score 값을 구할 수가 있는 겁니다. 예를 들어 이렇게 a boy who is looking at the tree is surprised. 이런 식으로 문장이 있다고 했을 때 각각의 단어들은 다른 단어 모두에 대해서 얼마나 가중치를 부여할지를 attention을 통해서 계산할 수가 있는 건데요. 예를 들어 이렇게 it이란 단어를 출력한다고 하면 이러한 it이 의미하는 단어는 앞쪽에 있는 tree와 이렇게 동일한 it이 되겠죠.\n",
      "2444.10 -> 2458.48:  자 이제 결과적으로 우리가 앞에서 확인했던 이 트랜스포머의 전체 아키텍처에 포함되어 있는 내용들을 하나씩 확인해 보았는데요. 이렇게 인코더 파트에선 입력까지 들어와서 위치에 대한 정보를 반영해 준 입력을 실제로 첫 번째 레이어에 넣어주게 되고요.\n",
      "2460.00 -> 2487.50:  n번만큼 반복이 되어서 중첩에 사용이 되고 이제 그렇게 나온 마지막 레이어의 인코더의 출력 값이 각각의 디코더 레이어에 들어간다고 보시면 됩니다. 이제 그래서 마찬가지로 디코더 레이어도 n번만큼 중첩이 되어서 가장 마지막에 나온 그 출력 값에 Linear Layer와 Softmax를 취해서 각각의 출력 단어를 만들어낼 수가 있는 것입니다. 다만 이제 우리가 한 가지 얘기 안 한 게 있다고 하면 바로 위치 정보를 어떤 식으로 넣을지에 대한 인코딩 함수입니다. 원본 논문에서는 하나의 문장에 포함되어 있는\n",
      "2490.00 -> 2518.40:  모델에게 알려주기 위해서 바로 주기함수를 활용한 공식을 사용하는데요. 실제 공식은 바로 다음과 같습니다. 이때 p2는 포지셔널 인코딩의 약자고요. 이때 이 포스는 각각의 단어 번호가 되겠고요. 이때 이 i는 각각의 단어에 대한 인베딩 값의 위치 하나하나를 의미합니다. 이제 그래서 이런 식으로 sin함수와 같은 주기함수 값을 인코딩을 위해서 사용하는데요. 이렇게 파라미터로 들어와 있는 만과 같은 값이나 이런 sin과 cos함수는 이렇게 기본적인 sin함수와 cos함수 말고\n",
      "2520.00 -> 2549.98:  아무튼 우리 네트워크가 각각의 입력 문장이 포함되어 있는 각 단어들의 상대적인 위치에 대한 정보를 알 수 있도록 이러한 주기성을 학습할 수 있도록 만들기만 한다면 어떤 함수가 들어와도 사용할 수 있습니다. 그래서 원본 논문에서도 이렇게 sine 함수와 cosine 함수를 이용해서 정해진 그런 함수 값을 사용할 수도 있지만 우리가 위치에 대한 인베딩 값을 따로 학습하도록 만들어서 네트워크에 넣을 수 있다고 말하고 있고 실제로 그렇게 넣었을 때도 이렇게 sine 함수와 cosine 함수를 이용했을 때와 실제 성능상의 차이는 거의 없었다고 말하고 있습니다.\n",
      "2550.00 -> 2575.18:  그래서 실제로 트랜스포머 이후에 나온 다양한 아키텍처에서는 이러한 주기함수를 사용하지 않고 그냥 학습이 가능한 형태로 별도의 인베딩 레이어를 사용하기도 합니다. 더욱더 자세하게 실제로 이러한 위치 인코딩이 어떤 식으로 들어갈 수 있는지를 확인해 보시면요. 예를 들어 이렇게 입력 문장이 we are the one이라고 한번 해볼게요. 이때 각각의 단어들은 딥 마더만큼의 인베딩 차원을 가지게 됩니다. 지금 그림에서는 이 인베딩 차원이 8이 되겠죠.\n",
      "2580.00 -> 2606.76:  각각의 값에서의 각각의 인덱스 값과 동일하게 들어가는 것입니다. 예를 들어 여기는 0,3이 되는데요. 첫 번째 단어에 네 번째 인베딩이기 때문이죠. 그래서 이제 각각의 값들이 이런 함수에 들어가게 돼서 바로 입력 값과 정확히 동일한 디멘전을 가지는 위치 인코딩을 만들어낼 수 있습니다. 그래서 이제 이 값을 엘러먼트 와이저로 다 더해줘서 원소 바이 원소로 다 더해준 뒤에 그 값을 실제로 각 인코더와 디코더 레이어의 입력 값으로 사용을 한다고 보시면 되겠습니다.\n",
      "2610.00 -> 2620.16:  실조로 어떤 식으로 각 단어의 위치에 대한 인코딩 정보가 들어가는지를 그림으로 표현한 것인데요. 바로 이렇게 간단하게 맵플러 라이브러리를 이용해서 그림을 그려볼 수 있습니다.\n",
      "2620.46 -> 2640.04:  자 마찬가지로 전체 실습 코드는 제 Github 저장소에 올려놓았으니까요. 확인하실 수 있습니다. 이렇게 아래쪽에 내려와 보시면 Attention is all you need 코드 프랙티스 보이시죠? 여기 들어오셔서 전체 코드를 확인해 보실 수 있습니다. 전체 코드는 여러분들의 개인 개발 환경이 아닌 무료 딥러닝 개발 환경인 코랩에서 파손됩니다.\n",
      "2640.00 -> 2668.20:  바로 실행해볼 수 있도록 준비를 해놓았습니다. 그래서 여러분들은 구글 아이디만 있으시면 바로 여기 링크 들어오셔서 코랩에서 즉시 실행해보실 수 있습니다. 다음과 같이 전체 코드를 확인해볼 수 있는데요. 내용을 확인해보시면 기본적으로 본 코드는 트랜스포머의 논문 내용을 최대한 따르면서 구현된 코드입니다. 실제로 트랜스포머 논문은 딥러닝 기반의 자연한 처리 기법의 대표적인 기본적인 구성을 이해하고 공부하는 데 도움을 줍니다. 그래서 최근 가장 뛰어난 번역 모델들은\n",
      "2671.48 -> 2698.36:  코드를 실행하시기 전에 런타임에서 런타임 유형 변경에 들어오신 뒤에 GPU로 설정이 되어 있는지 확인해주세요. 또한 본 코드에서는 독일어를 영어로 번역을 수행해 볼 건데요. 이제 번역된 영어 문장의 성능을 평가하기 위한 척도로 블루스코어를 사용할 예정입니다. 이 블루스코어는 Ngram 기반으로 번역한 문장이 실제 정답 문장들과 비교했을 때 얼마나 유사한지를 평가해주는 평가 척도 중 하나입니다.\n",
      "2703.94 -> 2729.00:  이제 이어서 데이터의 전처리를 진행하게 될 건데요. 문장의 토큰화를 진행할 겁니다. 이때 우리는 독일어를 영어로 번역하는 테스크를 진행할 것이기 때문에 이렇게 영어와 독일어에 대해서 전처리 모듈을 설치할 수 있도록 하겠습니다. 그래서 각각 영어와 독일어에 대해서 어떠한 문장이 있을 때 그러한 문장을 토큰으로 바꿔주기 위해서 토큰으로 바꿔줄 수 있는 각각의 스페이시 라이브러리 객체를 선언해주고요.\n",
      "2730.00 -> 2753.04:  간단하게 I am a graduate student라는 내용의 문장이 있을 때 한번 이걸 토큰으로 각각 바꾸어서 출력을 해보겠습니다. 그럼 이런 식으로 영어 문장이 정상적으로 I am a graduate student라고 잘 토큰화 해제가 된 걸 확인할 수 있습니다. 이제 우리는 실제로 다수의 문장을 불러와서 각 문장마다 전부다 이러한 토큰화를 진행해서 우리 네트워크의 입력값으로 넣을 수 있도록 만들 겁니다.\n",
      "2760.00 -> 2789.48:  이렇게 마찬가지로 함수를 정의해 주시고 또 추가적으로 필드 라이브러리를 이용해서 어떠한 데이터셋이 있을 때 그러한 데이터셋에 대해서 어떻게 전처리를 수행할 건지 명시할 수 있도록 합니다. 번역 모델에 입력을 넣을 때는 각각의 문장들의 앞부분에는 sos 토큰을 붙이고 가장 뒷부분에는 eos 토큰을 붙이는 것이 일반적입니다. 또한 각각의 단어들은 모두 소문자로 바꿔주는 것이 일반적이고요. 또한 트랜스포머에 입력을 넣을 때는 텐서의 차원에서 시퀀스보다는 배치가\n",
      "2790.00 -> 2818.42:  먼저 오도록 만드는 경우가 많기 때문에 이렇게 배치 퍼스트의 값으로는 트루 값을 넣어주겠습니다. 이제 이어서 약 3만개 정도의 영어, 독어, 쌍을 가지고 있는 번역 데이터셋인 멀티 30k를 불러와서 데이터를 초기화할 수 있도록 해줄게요. 이때 각각 이 필드 라이브러리를 이용해서 독일어를 영어로 바꾸는 테스크에 대해서 각각 앞서 정의했던 전처리를 수행할 수 있도록 하는 것입니다. 이제 학습 데이터셋과 평가 데이터셋, 테스트 데이터에 대해서 개수를 출력하도록 만들어 보시면\n",
      "2820.00 -> 2849.54:  2만 9천 개 그리고 평가 데이터와 테스트 데이터가 각각 약 천 개 정도 문장으로 구성되어 있는 걸 확인할 수 있고요. 이때 한번 간단하게 인덱스 30번에 해당하는 학습 문장을 출력하도록 만들어 보시면 바로 이러한 독일어 문장이 들어왔을 때 이러한 영어 문장을 출력하도록 학습 데이터가 구성되어 있는 걸 확인할 수 있고요. 자 그래서 이제 실제로 vocabulary 세트를 만들 수 있습니다. 이 vocabulary를 만들어주는 이유는 독일어를 영어로 번역할 때 각각의 초기 input dimension의 얼마인지를 구할 수가 있기 때문입니다.\n",
      "2850.00 -> 2876.06:  그래서 전체 단어들 중에서 최소 2번 이상 등장한 단어들만을 선택하도록 만들어서 vocabulary 세트를 만든 뒤에 각각의 length를 출력하도록 만들어 보시면 독일어는 7,855개의 유의미한 단어가 있고 그리고 영어는 5,893개의 각각의 단어들이 존재한다고 볼 수 있는 거예요. 그래서 이런 vocabulary 객체에서 string2i 함수를 호출해가지고 각각의 단어가 어떠한 인덱스에 해당하는지를 구해볼 수 있습니다.\n",
      "2880.00 -> 2909.02:  이제 확인할 수 있는데요. 이때 만약에 이런 식으로 애초에 등장하지 않았던 없는 단어라면 0이라고 뱉는 걸 확인할 수 있고요. 이렇게 아예 의미가 없는 그런 공간에 해당하는 패딩 값은 1로 들어가 있는 걸 확인할 수 있고요. 이제 이렇게 sos와 eos는 기본적으로 2번과 3번에 해당합니다. 또한 이렇게 실제로 존재하는 단어 같은 경우는 그 단어의 인덱스가 나오는 걸 확인할 수 있고요. 이제 이와 같이 앞쪽에 붙는 4개의 토큰들이 차례대로 unknown 토큰, 그 다음 패딩 토큰, sos 토큰, eos 토큰이라고 불리고요.\n",
      "2910.00 -> 2939.42:  실제로 존재하는 단어는 아니지만 네트워크가 각각의 문장들을 적절하게 학습할 수 있도록 만들기 위해 사용하는 토큰들입니다. 이어서 한 문장에 포함된 단어들이 순서대로 나열된 상태로 네트워크에 입력이 되는데요. 이때 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사할 수 있도록 만들기 위해 bucket iterator를 사용할 수 있습니다. 이때 이런 식으로 배치 사이즈를 정해줄 수 있는데요. 하나의 배치에 포함되어 있는 각각의 문장들의 그 시퀀스 랭스가 가능한 유사도록 만들어서 길이가 짧은 문장들에 대해서\n",
      "2940.00 -> 2966.56:  패딩 토큰이 최대한 적게 들어갈 수 있도록 하여 실제로 학습을 위해 네트워크에 입력으로 들어가는 데이터의 차원을 줄일 수 있습니다. 그래서 이렇게 각각 학습용, 평가용, 테스트용 데이터셋을 이터레이터로 만들어주고요. 한번 간단하게 이 트랜 이터레이터에 포함되어 있는 첫 번째 배치를 확인한 뒤에 하나의 문장에 대한 정보를 출력해 볼 수 있습니다. 확인해 보시면 현재 배치에 포함되어 있는 문장들 중에서 가장 긴 문장의 시퀀스 길이가 35가 되겠고요.\n",
      "2970.00 -> 2994.72:  이런 식으로 어떤 문장이 들어가 있는데 여기 2번 같은 경우는 SOS가 되고 3번은 EOS라고 했죠. 그래서 이제 SOS와 EOS 안에 포함되어 있는 각각의 데이터가 실제 하나하나씩 단어를 의미하는 거고요. 이제 이렇게 EOS가 나온 뒤에는 뒤쪽에 다 패딩 토큰이 붙어가지고 의미가 없는 토큰이라는 것을 알려줄 수 있도록 하는 것입니다. 이제 이런 식으로 현재 배치에 있는 하나의 문장에 포함된 정보를 출력할 수 있습니다.\n",
      "3000.00 -> 3028.80:  출력하도록 만들 수가 있는 것입니다. 이제 그래서 실제로 트랜스포머의 논문과 최대한 유사하게 각각의 아키텍처를 정의해서 모델을 학습할 수가 있는데요. 가장 먼저 멀티헤드 어텐션입니다. 확인해보시면 이렇게 어텐션의 세 가지 요소를 입력으로 봤고요. 쿼리와 키와 밸류입니다. 기본적으로 쿼리, 키, 밸류의 차원들은 모두 d 모델을 h로 나눈 값으로 차원을 모두 같도록 만들면 간단하게 정의할 수 있습니다. 이렇게 파라미터로 세 개를 봤는데 먼저 히든 디넨저는 하나의 단어에 대한 인베딩이 되겠고요.\n",
      "3030.00 -> 3059.34:  해지는 말 그대로 h가 되겠습니다. 즉 헤드의 개수가 되는 거고 그 다음에 드랍아웃 레이셔는 별도의 정교화 테크닉으로 드랍아웃을 적용할 때의 비율을 설정하는 것입니다. 참고로 여기에서 우리가 앞서 공부했을 때는 각각의 쿼리와 키 멜류들은 히든 디멘전에서 이 키의 차원으로 바뀌어진다고 말씀을 드렸는데요. 실제로 구현할 때는 이와 같이 그냥 히든 디멘전을 히든 디멘전으로 맵핑하도록 만든 다음에 이 결과 디멘전을 h로 쪼개서 사용할 수 있는 겁니다. 그래서 확인해 보시면 각 헤드에 포함되어 있는\n",
      "3060.00 -> 3089.70:  인베딩 차원은 헤드 디멘전이라고 해서 이 단어들의 인베딩 차원을 h로 나눈 값을 사용하도록 만듭니다. 또한 스케일 값도 앞서 설명했던 내용과 마찬가지로 각각의 쿼리와 키와 밸류에 해당하는 그 값에 루트를 씌운 값을 나중에 나눠주어서 소프트맥스에 넣어줄 수 있도록 만듭니다. 그래서 이때 각각 쿼리와 키 밸류가 들어오는데요. 이때 쿼리 랭스는 단어의 개수가 되겠죠. 그래서 이와 같이 각각 쿼리와 키 밸류로 그대로 다 맵핑을 해주고요. 이때 여기에서 우리는 이러한 쿼리와 키의 밸류에\n",
      "3090.00 -> 3115.94:  결과값들을 A측으로 나눠 사용할 수 있는 것입니다. 즉 A측의 각각마다 헤드 디멘점만큼의 크기로 차원을 가지도록 만들어서 앞서 확인했던 그림에서 이렇게 리니어를 각각 거친 값을 뽑아낼 수가 있는 것입니다. 이제 그래서 각각 A측의 Value, Key, Query들을 만든 것과 마찬가지입니다. 이제 그렇게 나누어진 상태에서 각각의 헤드마다 Query와 Key를 서로 곱하도록 만들어주고 스케일로 나눠줍니다.\n",
      "3120.00 -> 3145.10:  이때 마스크 값이 0인 부분을 전부 다 마이너스 무한에 가까운 값으로 넣어주어서 실제로 소프트맥스에 들어간 결과 값이 거의 0%가 될 수 있도록 만드는 거고요. 그래서 소프트맥스를 취한 다음에 그렇게 나오게 된 어텐션 가중치 값들을 실제로 v값과 곱해서 어텐션 밸류 값들을 결과적으로 만들어줄 수가 있는 거고요. 이제 얘를 다시 일자로 쭉 늘어뜨려서 컨켓을 수행한 것과 동일한 결과를 뽑을 수 있도록 만듭니다.\n",
      "3150.00 -> 3176.56:  그리고 어텐션 스코어 값은 따로 또 출력하도록 만들어서 나중에 시각화를 수행할 수 있도록 만듭니다. 이어서 포인트와이즈 피드포워드 아키텍처인데요. 마찬가지로 이렇게 히든 디멘젼만큼의 차원이 들어왔을 때 히든 디멘젼으로 그대로 내보낸다는 점이 특징이고요. 즉 입력과 출력의 차원이 동일합니다. 이제 이어서 앞에 정의했던 어텐션과 피드포워드 레이어에 대한 클래스를 이용해서 실제로 하나의 인코더 레이어 아키텍처를 정의할 수가 있는데요. 마찬가지로 입력과 출력의 차원이 같고요.\n",
      "3180.00 -> 3207.44:  여러 번 중첩해서 하나의 전체 인코더에 들어가게 됩니다. 자 그래서 인코더 레이어를 확인해 보시면은 4개의 실질적인 레이어가 들어가 있는 걸 확인할 수 있는데요. 여기 그림을 확인해 보시면 실제 인코더 레이어는 말씀드렸듯이 attention 이후에 residual connection과 normalization 그 다음에 feed for layer와 다시 한번 residual connection과 normalization 수행을 한다고 말씀을 드렸죠. 그래서 각각의 레이어가 이렇게 4개 차례대로 들어가 있는 거고요. 가장 먼저 하나의 입력값이 들어왔을 때\n",
      "3210.00 -> 3239.98:  그대로 복제해서 같은 값들을 넣어주도록 만들고요. 우리가 특정 단어에 대해서는 어텐션을 수행하지 않도록 하기 위해서 마스크 벡터를 씌울 수 있다고 했습니다. 그대로 건너온 입력 인베딩 매트릭스와 마스크를 그대로 넣을 수 있도록 하는 거고요. 그래서 가장 먼저 이렇게 셀프 어텐션을 수행해서 결과를 뽑아낸 뒤에 구해진 값을 더해줘서 레지듀얼 커넥션을 수행한 뒤에 실제로 노멀라이제이션을 수행한 결과가 나올 수 있도록 하고요. 이제 마찬가지로 피드포워드 레이어를 거친 다음에 다시 한번 레지듀얼 커넥션을 거쳐서 만들어진 아웃풋을 내보내주고\n",
      "3240.00 -> 3267.34:  그래서 실제 인코더 아키텍처에서는 이렇게 앞서 정의했던 인코더 레이어를 총 n 레이어만큼 반복해서 쌓도록 만듭니다. 확인해보시면 인코더 파트에서는 실제 단어들의 개수에 해당하는 인풋 디멘전에 해당하는 매트릭스가 들어왔을 때 전부 다 인베딩 차원으로 바꿔주고요. 그 다음 또 추가적으로 여기에서 바로 이 부분이 실제 논문과는 다르게 구현된 부분이라고 할 수 있습니다. 현재 코드에서는 원본 논문과 다르게 위치 인베딩을 직접 학습하는 형태로 구현합니다.\n",
      "3270.00 -> 3296.28:  사인과 코사인 함수를 사용하는 것이 아니라 이런 위치 임베딩을 학습하도록 만들어도 비슷한 결과를 내보낼 수 있기 때문에 본 코드에선 이렇게 임베딩 값을 학습하는 레이어로서 별도의 학습 레이어로서 사용할 수 있다고 보여주는 것입니다. 또한 이제 실제로 마스크 값은 이 패딩 토큰에 대해서 0값이 들어가는 형태로 만들어지는데요. 말씀드렸듯이 하나의 배치 안에는 여러 개의 입력 문장이 들어가 있는데 이때 짧은 문장에 대해서는 뒤쪽에 패딩 토큰으로 채워진다고 했습니다.\n",
      "3300.00 -> 3326.86:  단어들끼리 어텐션을 수행할 때 패딩 토큰은 무시하도록 만들어야 하기 때문에 이렇게 패딩 토큰에 대해서 마스크를 씌워준다고 보시면 되겠습니다. 그래서 먼저 맨 처음에 인풋 디멘전으로 입력이 들어왔을 때 실제 인베딩 차원으로 바꿔주고 거기다가 위치에 대한 정보 값을 더해주도록 만들기 위해서 별도의 포스 인베딩 레이어를 추가해 준 거고요. 이제 인코더 레이어는 반복적으로 중첩돼서 사용이 되기 때문에 모듈 리스트를 이용해서 N 레이어만큼 반복할 수 있도록 만듭니다.\n",
      "3330.00 -> 3358.78:  말 그대로 문장의 개수가 되겠고요. 이 source length는 각 문장들 중에서 단어의 개수가 가장 많은 문장의 단어 개수가 되겠습니다. 이제 그래서 포지셔럴 인코딩은 차례대로 0부터 가장 긴 문장에 해당하는 번호까지 들어갈 수 있도록 만들고 이제 그것을 각각의 문장마다 적용하도록 만들기 위해서 리핏을 수행합니다. 그래서 결과적으로 입력 인베딩 값에 그러한 위치에 대한 정보가 포함된 데이터를 실제 입력 값으로 사용을 해주는 거고요. 그래서 그러한 입력 값이 여러 개의 레이어를 반복적으로 거치면서\n",
      "3360.00 -> 3388.60:  forward를 수행할 수 있도록 만듭니다. 마지막 인코더 레이어에서 나오게 된 그 출력값을 결과적으로 사용할 수 있도록 만드는 것입니다. 이어서 디코더 레이어의 아키텍처를 확인해보겠습니다. 마찬가지로 입력과 출력의 차원이 같고요. 그래서 트랜스포머의 디코더는 이러한 디코더 레이어를 여러 번 중첩해서 사용한다고 말씀드렸죠. 또한 내부적으로 두 개의 멀티헤드 어텐션이 사용된다고 말씀드렸습니다. 하나는 셀프 어텐션이고요. 그리고 하나는 인코더에 대한 정보를 받아오기 위한 인코더 디코더 어텐션이라고 말씀드렸습니다.\n",
      "3390.00 -> 3417.34:  총 6개의 레이어가 사용되는 걸 알 수 있고요. 이는 여기 그림과 마찬가지입니다. 셀프 어텐션 이후에 레지듀얼 커넥션, 그 다음에 인코더 디코더 어텐션 이후에 레지듀얼 커넥션 이후에 퓨드 포워드 레이어와 레지듀얼 커넥션을 한 번 더 거쳐서 결과를 뽑아낼 수 있도록 만듭니다. 이제 이러한 레이어가 N번만큼 중첩되어 쓰이고요. 그렇게 마지막 레이어에서 나오게 된 출력값이 실제 출력 문장에 대한 정보를 가지고 있다고 했죠. 따라서 이와 같이 6가지 레이어를 전부 초기화할 수 있도록 만들어주었고요.\n",
      "3420.00 -> 3449.48:  그렇기 때문에 쿼리와 키 밸류는 모두 자기 자신을 넣을 수 있도록 만들어주고요. 그 다음 이후에 레지듈러 커넥션을 수행한 뒤에 그 다음 인코더에서 정보를 가져오는 인코더 디코더 어텐션을 수행하는데요. 이때 쿼리는 바로 디코더에 포함되어 있는 출력 단어들에 대한 정보가 되겠고요. 인코더에서 가장 마지막 출력 값으로 나온 그 값을 키로 사용하는 것을 알 수 있습니다. 그래서 결과적으로 이렇게 정의된 디코더 레이어 아키텍처를 이용해서 전체 디코더 아키텍처에서는 그러한 디코더 레이어를 여러 번 중첩해서 사용하고요.\n",
      "3450.00 -> 3478.58:  원본 논문과는 다르게 마찬가지로 위치 임베딩을 우리가 별도로 학습하는 형태로 구현을 해서 사인함수와 코사인함수를 사용하지 않도록 만들 것입니다. 또한 참고로 말씀드렸듯이 타겟 문장에서 각각의 단어는 다음 단어가 무엇인지 알 수 없도록 즉 이전에 출력한 단어만 볼 수 있도록 하기 위해서 별도의 마스크 벡터를 사용할 수 있다고 했습니다. 그래서 아까랑 동일하게 단어의 개수와 같은 그 디멘전을 임베딩 차원으로 바꾸어 주고요. 마찬가지로 위치에 대한 정보를 주기 위해서 전체 시퀀스의 랭스에 해당하는 디멘전을\n",
      "3480.00 -> 3509.18:  수 있도록 만듭니다. 또한 이어서 이러한 디코더 레이어는 반복적으로 중첩해 사용할 수 있다고 말씀드렸습니다. 자 그래서 실제로 forward를 수행할 때 인코더의 마지막 레이어에서 나오게 된 출력값과 실제로 타겟 문장에 대한 정보를 받고요. 마찬가지로 타겟 문장 또한 즉 출력하기 위한 문장 또한 0부터 그 단어의 개수까지에 대한 위치에 대한 정보가 담겨야 하기 때문에 하나의 텐서를 초기화한 뒤에 각 문장에 대해서 모두 동일하게 적용할 수 있도록 만듭니다. 그래서 문장이 임배된 값에 위치에 대한 정보를 더한 것을\n",
      "3510.00 -> 3535.88:  그래서 디코더 레이어를 여러 번 거친 다음에 마지막에 나오게 된 아웃풋 값에 출력을 위한 리니어 레이어를 거치도록 만들어서 결과적인 아웃풋 값을 뽑아낼 수 있습니다. 그래서 결과적인 트랜스포머 아키텍처는 이렇게 전체 인코더 아키텍처와 전체 디코더 아키텍처를 받아서 입력 문장에 따라서 마스크를 붙여서 실제 결과를 뽑아낼 수 있도록 만듭니다. 이때 소스 문장 같은 경우는 패딩 토큰에 대해서만 마스크 값을 0으로 설정해서 학습하도록 만들고요.\n",
      "3540.00 -> 3568.02:  무엇인지 알 수 없도록 하기 위해서 마찬가지로 마스크 행렬을 사용할 수 있습니다. 여기 보이는 패딩 마스크는 소스 문장과 마찬가지로 패딩 토큰에 대해서 적용하도록 만들어서 학습할 때 이러한 마스크를 사용할 수 있도록 하고요. 추가적으로 앞쪽에 있는 단어들만 볼 수 있도록 만들기 위해서 별도의 마스크를 하나 더 만든 다음에 이 두 마스크에 대해서 엘러먼트 YG로 엔드 연산을 수행한 뒤에 결과적으로 이렇게 두 마스크에 대해서 둘 다 1의 값을 가지는 그런 위치에 대해서만 실제 어텐션 스코어를 구할 수 있도록 만들어서\n",
      "3570.00 -> 3599.18:  그래서 결과적으로 소스와 타겟이 들어왔을 때 마스크를 각각 만든 뒤에 먼저 인코더에 이러한 소스 문장을 넣어서 인코더의 출력값들을 뽑은 뒤에 이제 디코더는 매번 그러한 인코더의 출력값을 어태션할 수 있도록 만들어서 결과적으로 마지막에 나온 아웃풋 값이 우리 네트워크의 번역 결과라고 할 수 있습니다. 이제 그래서 실제로 학습을 진행할 수 있는데요. 학습할 땐 이와 같이 인풋 디멘전과 아웃풋 디멘전은 각각 소스 언어에 포함되어 있는 언어의 개수, 그 다음에 타겟 언어에 포함되어 있는 언어의 개수가 될 수 있도록 합니다.\n",
      "3600.00 -> 3625.58:  각각의 단어에 대한 인베딩 차원은 256으로 설정하고 레이가 총 3번씩 중첩되어 사용할 수 있도록 만들었습니다. 여기 보이는 파라미터는 실제 논문에서 제한된 파라미터에 비하면 크기가 많이 작은 편이지만 그럼에도 불구하고 충분히 좋은 성능을 낼 수 있습니다. 그래서 결과적으로 이렇게 트랜스포머 객체를 만들어 준 뒤에 파라미터까지 다 초기화를 진행해 주시고요. 이렇게 전체 네트워크에 포함되어 있는 모든 파라미터를 확인해 볼 수 있습니다. 그래서 이제 한번 학습을 진행할 수 있는데요.\n",
      "3630.00 -> 3659.80:  모델 학습할 수 있도록 하겠습니다. 그래서 학습함수와 평가함수를 따로 정의해서 학습을 진행하면요. 이렇게 학습이 진행되는 걸 확인할 수 있고요. 학습을 진행할 때마다 validation loss가 더 감소하는 경우에만 모델 파라미터를 새로운 파일로 기록하도록 만드는 것을 알 수 있습니다. 결과적으로 이와 같이 학습이 완료된 걸 확인할 수 있고요. 이렇게 학습된 결과를 여러분들의 컴퓨터에 기록하고자 한다면 이렇게 파일 라이브러리를 이용해서 학습이 완료된 데이터를 다운로드 받을 수 있도록 합니다.\n",
      "3660.00 -> 3684.36:  자 이후에 이와 같이 학습 완료된 모델을 이용해서 테스트 데이터에 대해서 이벨레이션을 진행하게 되면 다음과 같이 테스트 데이터에 대한 로스 값을 구해볼 수 있습니다. 이제 이어서 여러분들만의 문장을 넣어서 실제로 우리 모델을 사용해볼 수 있는데요. 이렇게 트랜슬러의 센텐스 함수 안에 내용이 정의되어 있는 걸 알 수 있습니다. 먼저 하나의 문장이 들어왔을 때 토큰을 진행한 뒤에 앞뒤로 sos와 eos 토큰을 붙입니다.\n",
      "3690.00 -> 3712.38:  이어서 마스크를 만든 뒤에 실제로 인코더에 이러한 소스 문장을 넣어서 출력값을 구할 수 있도록 합니다. 그 다음에 이제 실제 출력 문장은 sos 토큰부터 출발해서 max length까지 한 번씩 반복적으로 모델의 디코더에 넣어서 출력값을 만들어낼 수 있도록 합니다. 그래서 이때 매번 디코더에 넣었을 때 가장 마지막 단어가 출력 문장으로서 하나씩 추가가 되는 거고요.\n",
      "3720.00 -> 3748.56:  모든 단어들이 전체 출력 문장이 되겠습니다. 그래서 실제 결과로 뽑기 위해서 각각의 인덱스를 다시 문자열로 바꾸어서 리턴해주는 것을 알 수 있습니다. 그래서 간단하게 한번 테스트 데이터 중에서 10번째 데이터를 확인해보도록 할게요. 내용 확인해보시면 얘가 독일어 문장이 되겠습니다. 이 내용이 원래 의미하는 내용은 한 명의 어머니와 그의 자식은 노래를 부른다. 야외에서 좋은 날을 즐기며 라는 내용이죠. 이게 실제로 출력된 결과를 확인해보면\n",
      "3750.00 -> 3779.18:  어머니와 그의 어린 아들이 야외에서 작은 나날을 즐기고 있다. 이런 식으로 번역되어 있는 걸 확인할 수 있습니다. 완전히 동일한 문장은 아니더라도 이런 전반적인 의미 자체가 잘 전달되도록 번역이 이루어진 걸 확인할 수 있습니다. 또한 이렇게 출력된 attention 값은 총 8개의 헤드로 구성된 attention 스코어들의 집합이라고 할 수 있는데요. 전체 그림에다가 각각의 헤드에 대한 attention 스코어 값을 출력하도록 만들 수 있습니다. 자 그래서 이와 같이 실제 그림까지 출력하도록 만들어 보시면\n",
      "3780.00 -> 3805.40:  이렇게 각각의 단어가 출력되기 위해서 소스 문장에서의 어떤 정보를 많이 참고했는지를 알 수 있습니다. 이 영어 문장으로 mother와 이 독일어 문장에 해당 단어는 마찬가지로 어머니라는 의미를 가지고 있습니다. 그렇기 때문에 이렇게 mother라는 단어를 출력하기 위해서 독일어 문장에 이러한 단어를 참고했다는 것을 시각적으로 확인할 수 있습니다. 이와 같이 총 8개의 헤드 각각에 대해서 attention score 값이 매겨지는 걸 확인할 수 있고요.\n",
      "3810.00 -> 3838.46:  학습하도록 만들어지기 때문에 이렇게 각각 attention score 값이 구해진 결과가 조금씩 다를 수 있다는 점 또한 확인할 수 있습니다. 이제 마지막으로 간단하게 blue score를 계산해서 학습이 완료된 트랜스포먼 모델의 스코어를 구해볼 수 있습니다. 지금 예시에서는 각각의 입력 문장이 있을 때 그 정답 문장 또한 하나씩만 존재하기 때문에 하나의 예측 값에 대해서 그 정답 값이 1대 1로 매칭될 수 있도록 바로 그 정답 문장을 하나의 리스트로 감싸서 하나씩 나열하는 걸 알 수 있습니다.\n",
      "3840.00 -> 3867.00:  문장 100개당 한 번씩 예측과 정답 값을 출력하도록 만들어 본 거고요. 자 이렇게 실제로 문장을 보시면 꽤 유사하게 예측이 이루어진 걸 확인할 수 있습니다. 예를 들어 마지막 문장 같은 경우는 나이가 많은 한 남자는 미디어 게임을 플레이하고 있다라고 번역을 했는데요. 실제 정답 값과 비교했을 때 거의 유사하게 번역을 수행한 걸 확인할 수 있습니다. 또한 여기 보이는 이 블루4 스코어가 일반적으로 알려져 있는 블루 스코어와 같은 값을 가지는데요.\n",
      "3871.02 -> 3874.76:  이어서 같이 한번 논문 리딩 진행해보도록 하겠습니다.\n",
      "3900.00 -> 3928.70:  어텐션 기법만 사용해서 기계 번역 태스크에서 좋은 성능을 얻었습니다. 바로 한번 abstract부터 읽어볼게요. 보시는 바와 같이 이전까지는 인코더와 디코더를 포함한 형태로 복잡한 RNN 혹은 CNN 기반의 시퀀스 간 변형이 이루어지는 RNN 및 CNN을 기반으로 하는 모델을 많이 사용했습니다. 여기에서 transduction은 변화 혹은 형질의 변형과 같은 의미를 가지고 있는데요. 말 그대로 어떠한 시퀀스 간 변형을 의미하는 겁니다. 대표적인 예시가 기계 번역이 있겠죠.\n",
      "3930.00 -> 3959.74:  그래서 이렇게 RNN 혹은 CNN을 전적으로 활용한 모델들이 많이 사용되었습니다. 또한 그러한 인코더 디코더 아키텍처의 어텐션 메커니즘을 활용했을 때 보다 성능이 좋아질 수 있었다는 그런 결과를 이전 논문에서 확인해 볼 수가 있었죠. 그래서 본 논문에선 이렇게 트랜스포머라는 이름의 아키텍처를 제안하고 이 아키텍처는 전적으로 어텐션 메커니즘에 기반을 하고 있는 아키텍처입니다. 이때 말 그대로 리커런스나 컨볼루션 자체를 제거한 형태로 어텐션 메커니즘만 활용을 했다는 거고요.\n",
      "3960.00 -> 3988.14:  어텐션 메커니즈만 활용함으로써 리커런스하게 각각의 시퀀스를 처리할 필요가 없어지기 때문에 그냥 행렬 곡을 이용해서 완전히 병렬적으로 시퀀스 데이터를 처리할 수 있기 때문에 훨씬 더 빠르게 처리가 가능하다는 점이 장점입니다. 그래서 결과적으로 굉장히 유명한 두 가지 태스크인 WMT-14용도 데이터셋을 이용해서 영어를 독일어로 번역하는 태스크 그리고 영어를 불어로 번역하는 태스크에서 각각 훨씬 개선된 성능을 보여준 것을 확인할 수 있습니다.\n",
      "3990.00 -> 4019.92:  테스크에 대해서 완전히 state-of-the-art의 성능을 보인 것을 확인할 수 있습니다. 또한 이와 같이 8개의 PBAC GPU를 이용해서 상대적으로 더 적은 시간을 들여서 학습을 마칠 수 있었다고 하고요. 이는 이전까지 제한되었던 모델과 비교했을 때 훨씬 더 학습 효율이 높다고 할 수 있습니다. 이러한 트랜스포머는 비단 기계 번역 뿐만 아니라 시퀀스 데이터를 처리하는 다양한 테스크에 대해서 일반화가 가능하고 성능이 잘 나오는 것 또한 보여주었습니다. 대표적으로 구문 분석 분야에서의 이러한 컨스티튜언시\n",
      "4020.00 -> 4049.64:  펄싱테스크에 대해서도 잘 동작하는 걸 확인할 수 있었다고 합니다. 이전에 이와 같이 RNN 그리고 LSTM 그리고 GRLU와 같은 다양한 모델들이 제안이 되었고요. 이러한 네트워크들은 시퀀스 모델링을 위해서 효과적으로 사용이 되고 있었습니다. 다만 기본적으로 이러한 리커런트 모델들은 한 번에 한 단어씩 넣는 방식처럼 시퀀스에 포함되어 있는 각각의 토큰들에 대한 순서 정보를 먼저 정렬시킨 뒤에 이것을 반복적으로 입력으로 넣어서 이러한 히든스테이트 값을 갱신시키는 방법으로\n",
      "4050.00 -> 4078.96:  그렇기 때문에 이런 식으로 리커런트하게 동작하는 모델인 경우 시퀀스의 길이 즉 토큰의 개수만큼 뉴럴 네트워크에 입력을 넣어야 되기 때문에 당연히 병렬적인 처리가 어렵다는 문제가 존재하고요. 다시 말해 레이어의 아웃풋을 행렬급으로 바로 구할 수 있는 게 아니라 즉 번역에서는 문장의 길이만큼 입력을 수행할 필요가 있기 때문에 이는 메모리 및 속도 측면에서 비효율성을 야기할 수 있다고 지적하고 있는 것입니다. 이어서 어텐션 메커니즘이 등장을 했었는데요.\n",
      "4080.00 -> 4107.46:  메커니즘을 활용하면서 매번 출력 단어를 만들어낼 때마다 소스 문장의 출력 정보 중에서 어떤 정보가 가장 중요한지에 대해서 가중치를 부여하도록 해서 그러한 가중치가 적용되어 곱해진 히든스테이트 값을 이용하도록 해서 출력 단어를 보다 효과적으로 생성할 수 있도록 만들 수 있다고 했죠. 다만 이러한 어텐션 메커니즘도 기본적으로 어뢰는과 같이 사용되는 경우가 많았고요. 그래서 본 논문에서는 그냥 리커런스한 특성 자체를 완전하게 제거해버린 겁니다.\n",
      "4110.00 -> 4139.88:  전적으로 의존해서 모델의 결과를 내보낼 수 있도록 합니다. 어텐션 메커니즘을 활용하기 때문에 한 번의 행렬 곡으로 위치 정보가 포함된 전체 시퀀스를 한 번에 처리할 수 있다는 점에서 다시 말해 순차적으로 입력을 넣지 않아도 되기 때문에 병렬 처리가 가능하다고 볼 수 있는 것입니다. 그래서 이러한 특징을 활용했더니 성능이 훨씬 좋아진 걸 확인할 수가 있고 8개의 P100 GPU를 이용해서 학습을 해본 결과 현실적인 시간, 즉 12시간 만에 상당히 좋은 베이스 모델의 성능을 얻을 수 있었다고\n",
      "4140.00 -> 4160.52:  자 그래서 기반이 되고 있는 다양한 백그라운드 논문을 소개하고 있고요. 이 중에서 셀프 어텐션이라고 하는 것은 타겟 문장을 만들기 위해서 소스 문장에서의 히든 정보를 참고하는 것이 아니라 어떠한 문장이 있을 때 자기 자신의 문장 스스로에게 어텐션을 수행해서 레프레젠테이션을 러닝할 수 있도록 만들어주는 게 셀프 어텐션 메커니즘입니다.\n",
      "4170.00 -> 4198.14:  시퀀스에 대한 레프레젠테이션을 효과적으로 학습하고 표현할 수 있도록 만들어주는 것입니다. 예를 들어 I am a teacher라는 하나의 문장이 있을 때 4개의 단어들은 서로가 서로에게 어텐션을 수행해서 가중치를 부여하도록 할 수 있다는 거죠. 그래서 본 논문에서는 이와 같이 트랜스포머는 전적으로 이러한 어텐션 메커니즘에 기반하는 사실상 최초의 시퀀스 간 변형이 가능하도록 만들어준 네트워크라고 할 수 있고 본 논문은 이러한 아이디어를 통해서 매우 좋은 성능을 이끌어낼 수가 있었고요.\n",
      "4200.00 -> 4229.66:  최근에 나온 GPT나 BERT와 같은 다양한 아키텍처들은 이러한 트랜스포머에서 제한되었던 아키텍처를 많이 따르고 있습니다. 그래서 시퀀스 간 변형 모델에 대해서 많은 아키텍처는 인코더, 디코더 구조를 따르고 있고요. 여기에서 35번 논문 같은 경우는 원본 시퀀스 투 시퀀스 논문을 의미하고요. 이런 식으로 X1부터 Xn까지 총 N개의 토큰으로 구성된 입력 시퀀스가 있을 때 이것을 컨티뉴어스한 어떠한 인베딩 벡터로서 바꾸어주고 이러한 인베딩 벡터인\n",
      "4230.00 -> 4258.16:  이 디코더는 Y1부터 YM까지 총 M개의 토큰으로 구성된 출력 문장을 만드는 방식으로 동작합니다. 이때 기본적으로 RNN 구조를 따르고 있는 모델들은 오토 리그레시브하게 시퀀스의 길이만큼 네트워크에 입력해 주어지는 방식으로 동작합니다. 다시 말해 이전 단계에서 생성되었던 심볼을 이용해서 다음번에 나올 출력값을 만드는 방식으로 동작한다는 거죠. 엄밀히 말하면 트랜스포머 또한 마찬가지로 인코더와 디코더 파트로 구성이 되어 있으며\n",
      "4260.00 -> 4289.92:  활용하고 있습니다. 단지 다른 점이라고 한다면 모델을 리커런트하게 이용하진 않고 어텐션 메커니즘만 활용해서 시퀀스에 대한 정보를 한 번에 입력으로 준다는 점이 그 특징이라고 할 수 있습니다. 자 그래서 여기 보이는 그림이 전체 트랜스포머의 아키텍처라고 할 수 있는데요. 자 이와 같이 어른에는 사용하지 않는 대신에 문장 내에 포함되어 있는 각각의 단어들의 위치 정보를 인코딩해서 입력하기 위해 포지셔널 인코딩을 사용하고요. 이렇게 입력 인베딩과 같은 디멘전으로 합치기를 수행해서 이렇게 만들어진 결과를\n",
      "4290.00 -> 4318.86:  실제 인베딩 벡터로서 사용을 하고요. 이제 얘가 이렇게 쿼리, 키, 밸류의 값으로 각각 복제가 되어 입력됩니다. 여기 보이는 멀티헤드 어텐션은 셀프 어텐션으로 동작을 하며 보시는 바와 같이 쿼리와 키와 밸류의 값이 모두 동일합니다. 또한 어텐션은 입력과 출력이 차원이 같다고 말씀드렸죠. 그래서 결과적으로 이렇게 들어갈 때의 차원과 이 어텐션을 나왔을 때의 차원은 동일하고요. 마찬가지로 레지돌 커넥션을 수행한 뒤에 정규화를 수행해주고 이렇게 피드포워드 레이어를 거치고 다시 한번 정규화를 수행할 때까지\n",
      "4320.00 -> 4349.90:  레이어에 대한 입력과 출력의 디멘저는 같다고 보시면 됩니다. 그래서 이제 이러한 과정 자체를 총 n번만큼 반복해서 총 n개의 인코더 레이어가 차곡차곡 반복적으로 수행이 돼서 마지막에 나온 그 출력값을 이와 같이 매 인코더 디코더 어텐션에서 사용할 수 있도록 만든다고 보시면 되겠습니다. 또한 이렇게 디코더 파트에서는 지금까지 출력된 단어만 어텐션 할 수 있도록 하기 위해서 학습을 수행할 때 이렇게 마스크를 씌워서 뒤쪽에 있는 단어는 미리 알지 못하도록 막는 방식으로 모델이 정상적인 데이터만을 학습할\n",
      "4350.00 -> 4379.88:  수 있도록 만들어주고요. 마찬가지로 여기 보이는 이 디코더의 첫 번째 attention에서는 query와 key value의 값이 같기 때문에 self-attention이 수행된다고 할 수 있습니다. 이렇게 두 번째 attention에서는 이 query의 값이 디코더에 있기 때문에 각각의 출력 단어를 만들기 위해서 이 인코더 파트에서 어떠한 정보를 참고하면 좋은지를 attention이 수행한다고 할 수 있는 겁니다. 즉 다시 말해 이 key와 value의 값들은 인코더 파트에서 받습니다. 그래서 마찬가지로 feedforward 레이어를 거친 뒤에 마지막에 linear 레이어를 거치고 softmax를 취해서 실제로 각각의 출력 문장의\n",
      "4380.00 -> 4407.04:  포함된 단어들이 실제로 어떤 단어에 해당하는지 구할 수 있도록 할 수 있습니다. 또 이제 여기에서 추가적으로 레이블 스무싱을 적용해서 정규화 효과를 더해서 성능을 더 높일 수 있습니다. 그래서 이러한 트랜스포머의 아키텍처는 향후 많은 논문에 영향을 미치게 되었습니다. 자 그래서 설명했던 내용과 동일하게 인코더 파트는 기본적으로 여러 번 인코더 레이어가 중첩이 되어 사용이 되고요. 보시는 바와 같이 본 논문에서는 총 6번 인코더 레이어를 중첩해서 사용할 수 있다고 말했고요.\n",
      "4410.00 -> 4435.02:  거치기 전에 그 입력 값으로 아이덴티티 맵핑을 수행할 수 있도록 만들어 주었고요. 또한 이와 같이 인베딩 벡터의 차원은 512차원으로 설정했다고 말하고 있습니다. 마찬가지로 디코더에서도 총 6개의 디코더 레이어를 쌓을 수 있도록 만들었고 실제 구현상에서 이와 같이 인코더와 같은 레이어의 개수를 가지도록 만드는 경우가 많습니다. 그래서 멀티헤드 어텐션을 수행할 때 인코더의 아웃풋 값에 대해서 어텐션을 수행할 수 있도록 만든다고 했고요.\n",
      "4440.00 -> 4469.46:  좋은 글로벌 옵티마를 찾을 수 있도록 모델을 설계했습니다. 또한 디코더 파트에서 쓰이는 셀프 어텐션에서는 이전에 등장한 단어들만 참고할 수 있는 형태로 마스크를 씌워서 마스크가 붙은 형태의 멀티헤드 어텐션을 사용할 수 있도록 만들었다고 합니다. 자 그래서 여기에서는 어텐션 메커니즘에 대해 설명하고 있는데요. 이때 하나의 쿼리는 말 그대로 어떠한 질문을 날리는 겁니다. 특정 키에게 물어보는 것과 같습니다. 즉 이때 쿼리라고 하는 것은 말씀드렸듯이 질문을 하는 주체라고 할 수 있고요. 이때 이 키는 어텐션을 수행할 대상이라고 할 수 있습니다.\n",
      "4470.00 -> 4498.16:  예를 들어서 사랑해라는 하나의 단어가 생성되기 위해서 I love you라는 문장에 포함된 단어들 중에 어떤 단어가 가장 중요했는지를 물어보는 방식으로 각각의 쿼리가 키에 대해서 어텐션을 수행하는 메커니즘으로 이해할 수 있습니다. 그래서 여기 보이는 그림이 실제로 멀티헤드 어텐션을 잘 설명하고 있는 그림이고요. 이러한 멀티헤드 어텐션은 각각의 인코더와 디코더 레이어에서 사용된다고 했습니다. 이때 멀티헤드 어텐션은 내부적으로 scaled.product 어텐션을 가지고 있는데요.\n",
      "4500.00 -> 4529.94:  같이 생겼습니다. 자 이때 이렇게 쿼리와 키와 밸류가 들어오게 되면 각각의 쿼리가 이 키에 대해서 질문을 하는 내용이 바로 이렇게 행렬 고부로 이루어지고요. 또한 소프트맥스에 들어가는 값에 대해서 스케일링을 하기 위해 스케일 레이어를 포함합니다. 이때 스케일 레이어는 여기 들어오는 키의 차원에 루트를 씌운 값을 나눠줄 수 있는 형태로 사용합니다. 또한 이렇게 마스크 벡터 같은 경우는 필요할 때 사용을 하고요. 그래서 이와 같이 소프트맥스를 취해서 각각의 키에 대해서 얼마나 중요한지에 대한 값을 확률 형태로 표현할 수 있도록\n",
      "4530.00 -> 4559.20:  이제 그러한 확률 값을 각각 실제 value와 곱해서 attention value 값을 만들어낼 수 있는 겁니다. 이제 이러한 과정들이 각각의 head마다 서로 다르게 이루어진 뒤에 다시 이렇게 결과를 합쳐서 linear layer를 거친 뒤에 output 값을 내보낸다고 할 수 있습니다. 자 이때 실제 구현상으로는 이렇게 입력 값이 들어왔을 때 입력 값은 이 v와 k와 key에 대해서 각각 복제가 되어서 들어가도록 할 수 있고 이때 각각의 linear layer는 embedding 차원을 key, query, value의 차원으로 바꿔줍니다.\n",
      "4560.00 -> 4589.94:  나온 값들을 각각 어떤 차원을 수행한 뒤에 동일한 차원이 나오게 되면 다시 얘네들을 묶어주어서 기존의 인베딩 차원과 결과적으로 같은 차원이 될 수 있도록 만들어준다는 거죠. 또한 여기에서 실제로 구현할 때는 예를 들어서 이렇게 입력으로 들어오는 인베딩 차원이 512라고 하고 이 h가 8이라고 한다면 각각의 리니얼 레이어는 64차원으로 맵핑을 해주는 것입니다. 다만 여기에서 실제로 구현할 때는 그냥 512 곱하기 512로 병렬적으로 그냥 한 번의 행렬급을 구한 뒤에 그 결과 값을 8개로 쪼개서 사용할 수도 있습니다.\n",
      "4590.00 -> 4618.82:  자 그래서 실제로 어텐션 메커니즘에서 핵심이 되는 스케일리닷 프로덕 어텐션에 대해서 설명하고 있는데요. 자 확인해보시면 이렇게 실제로는 행렬 형태로 한 번에 쿼리와 키들을 묶어서 병렬적으로 계산할 수가 있고요. 말씀드린 내용과 마찬가지로 먼저 쿼리랑 키랑 곱하고요. 이때 쿼리와 키는 기본적으로 차원이 같도록 만들어서 이와 같이 곱셈이 수행될 수 있도록 만듭니다. 이제 그래서 스케일 팩터만큼 나눠준 뒤에 확률값을 구하고 얘를 실제로 이 밸류값과 행렬급을 수행할 수 있다고 했죠.\n",
      "4620.00 -> 4647.68:  이러한 방식은 additive attention과는 약간 구별되는 방식인데요. 바로 여기 확인해보시면 이렇게 키랑 쿼리를 그냥 곱해가지고 한 번에 attention을 구할 수가 있습니다. 사실 우리가 앞서 확인했던 sequence to sequence의 attention 메커니즘을 활용했을 때는 이 쿼리와 키가 특정한 행렬 곱에 함께 입력되는 형태로 동작을 했었는데요. 여기서는 그냥 쿼리와 키를 바로 서로 곱하도록 만들어서 not product attention 형태로 사용했다고 볼 수 있고요. 논문에선 이와 같이 not product attention을 사용했을 때\n",
      "4650.00 -> 4677.34:  것이 프랙티컬리 빠르고 공간 효율적이었다고 말하고 있습니다. 또한 이렇게 내적을 이용하는 방식인 경우 스케일링을 하지 않으면 결과가 많이 안 좋았다고 하는데요. 본 논문에서는 그러한 이유를 다음과 같이 추측하고 있습니다. 바로 소프트맥스 같은 경우는 이 중간 부분이 그래디언트가 상대적으로 크고 사이드로 가면 갈수록 그래디언트가 작아지는 특징을 가지고 있는데요. 그렇기 때문에 값이 너무 커지거나 하면 너무 그래디언트가 작아질 수 있어서 학습이 잘 안될 수가 있겠죠.\n",
      "4680.00 -> 4707.78:  곱해주어서 값을 작게 만들어 학습이 잘 이루어질 수 있도록 만드는 것입니다. 또한 말씀드렸듯이 기존의 인베딩 차원을 D모델이라고 했을 때 얘를 H만큼 즉 헤드의 개수만큼 나누어서 각각 키나 밸류, 코리아 같은 차원들을 결정할 수 있다고 했고요. 이러한 벡터들은 나중에 다시 이어붙여지기 때문에 결과적으로는 입력과 출력의 디멘전이 같도록 만들 수 있습니다. 자 그래서 멀티헤드 어텐션은 말 그대로 헤드가 여러개라는 의미라서 이렇게 멀티헤드라고 이름이 붙은 거고요.\n",
      "4710.00 -> 4739.94:  전부 다 attention을 수행한 값을 다시 이렇게 이어붙인 뒤에 out 값을 내보내기 위해서 행렬급을 수행합니다. 이제 여기에서 이 i 값은 각각의 head에 대한 인덱스라고 할 수 있고요. 그래서 이와 같이 실제로 call이나 key를 만들기 위한 행렬의 크기는 dmodel 곱하기 dk가 사용이 되고요. 다시 말해서 dmodel 차원의 embedding vector를 dk 차원의 call이야 key vector의 차원으로 만들 수 있도록 하는 것입니다. 물론 말씀드렸듯이 실제로 구현할 때는 그냥 dmodel 곱하기 dmodel만큼의 행렬을 곱한 뒤에\n",
      "4740.00 -> 4767.84:  그냥 결과 값 자체를 나누어서 사용할 수도 있는 거예요. 그래서 본 논문에서는 D 모델을 512 그리고 H를 8로 설정해서 쿼리 벡터의 차원을 64라고 설정을 했고요. 또한 엄밀히 말하면 이 밸류 값 같은 경우는 차원을 쿼리 혹은 키와 똑같이 맞출 필요는 없지만 여기서는 이렇게 쿼리, 키, 밸류 전부 다 같은 차원인 64차원으로 사용할 수 있다고 말하고 있습니다. 그래서 이제 이러한 어텐션이 실제로 어디에 쓰이는지 확인해 보시면 앞서 말씀드렸듯이 총 3가지 위치에서 사용이 되는데요.\n",
      "4770.00 -> 4796.82:  헤드가 여러개인 멀티 헤드 어텐션이며 사용되는 위치에 따라서 3가지로 구분되는 것입니다. 먼저 인코더 디코더 어텐션은 디코더 파트에서 사용이 되는 거고요. 이때 이 쿼리는 이 디코더에서 오는 것이고 이때 이 키와 밸류 값은 인코더의 출력 파트에서 가져온다고 했어요. 이 내용은 다시 말하면 우리가 출력 단어를 만들기 위해서 소스 문장에 포함되어 있는 단어들 중에서 어떤 정보에 보다 초점을 맞추면 되는지를 계산하는 과정이라고 비유할 수 있다고 했죠.\n",
      "4800.00 -> 4828.06:  키와 밸류가 모두 같은 형태를 의미하고요. 바로 인코더 파트에서 그대로 사용이 됩니다. 그리고 디코더 파트 또한 마찬가지로 맨 처음에 입력 임베딩이 들어왔을 때 셀프 어텐션을 수행할 수 있다고 했는데요. 이제 여기서는 마스크를 씌워서 다시 말해 소프트맥스에 들어가는 값이 마이너스 무한이 될 수 있도록 해서 0%가 부여될 수 있도록 하여 각각의 단어가 앞부분에 있는 단어에 대한 정보만 참고할 수 있도록 만들었다고 보시면 되겠습니다. 또한 이와 같이 포지션 와이즈 피드폴 네트워크를 사용할 수 있다고 했는데요.\n",
      "4830.00 -> 4859.14:  터무는 렐루 액티베이션에 대한 내용을 보여주고 있는 것입니다. 그래서 이와 같이 입력값과 출력값은 모두 같은 차원을 가지게 되고요. 이렇게 중간에 히듬 디멘전으로 약간 고차원 공간에 맵핑이 되었다가 출력 레이어를 통해서 피드폴드가 수행될 수 있도록 만든다고 보시면 되겠습니다. 또한 마찬가지로 인베딩과 소프트맥스가 사용이 되었는데요. 이건 이제 기본적으로 시퀀스 투 시퀀스 모델에서 사용되는 내용과 같다고 할 수 있습니다. 그냥 인풋 디멘전, 즉 특정한 언어에 포함되어 있는 단어의 개수와 비례하는\n",
      "4865.16 -> 4889.98:  또한 이어서 트랜스포머에서는 어텐션 메커니즘만 전적으로 활용하기 때문에 위치에 대한 정보값을 같이 주기 위해서 인코딩 정보를 같이 더해서 넣어줄 수 있다고 했습니다. 다시 말해 리커런스 및 컨볼루션 둘 다 사용하지 않기 때문에 위치에 대한 정보를 같이 넣어줄 필요가 있는 겁니다. 그래서 법 논문에서는 다음과 같이 사인과 코사인과 같은 주기함수를 사용해서 입력을 넣었다고 하고요. 말씀드렸듯이 우리가 위치에 대한 정보를 넣어줄 때\n",
      "4890.00 -> 4919.86:  꼭 사인이나 코사인 함수를 이러한 형태로 사용할 필요는 없고요. 이런 함수를 사용하지 않고 이러한 인베딩 레이어 또한 우리가 별도로 학습하도록 만들어서 네트워크를 구성할 수도 있습니다. 실제로 본 논문에서는 이렇게 학습이 가능한 형태로 인베딩 레이어를 사용해봤다고 하는데요. 실제로 성능상에는 차이가 없었다고 합니다. 사실 그렇기 때문에 우리가 파이톨치와 같은 프레임웍을 사용할 때 그냥 학습하도록 하는 게 구현이 더 쉬울 수도 있기 때문에 실제로 우리가 아까 실습을 진행했을 때 별도의 학습 가능한 인베딩 레이어를 사용을 했던 겁니다.\n",
      "4920.00 -> 4946.72:  다만 본 논문에서는 이러한 정연파 함수를 사용했을 때 보다 긴 시퀀스가 들어왔을 때 성능이 더 잘 나올 수 있다고 언급하고 있습니다. 이어서 섹션 4에서는 이러한 셀프어텐션이 왜 더 유리한가에 대해서 설명하고 있는데요. 본 논문의 저자들은 세 가지의 어떠한 열망하는 그러한 장점들을 목표로 두고 이러한 셀프어텐션을 고안했다고 말하고 있는데요. 첫 번째로는 각각의 레이어마다 계산 복잡도가 줄어든다는 장점이 있습니다.\n",
      "4950.00 -> 4979.26:  마지막으로 롱 레인지 디펜던시에 대해서도 잘 처리할 수 있다고 말하고 있습니다. 자 그래서 이렇게 위쪽에 보이는 테이블이 RNN을 사용했을 때와 컨볼루션을 사용했을 때 그리고 이렇게 어텐션 메커니즘을 활용했을 때에 대한 효율성을 분석하고 있는 건데요. 이때 N은 시퀀스의 길이 즉 단어의 개수라고 할 수 있고요. 이때 확인해 보시면 이런 어텐션 기법을 사용할 때 시퀀스 데이터를 처리할 때 단 한 번에 병렬적으로 구할 수 있기 때문에 RNN과 비교했을 때 훨씬 네트워크에 들어가는 입력의 횟수가 적다는 걸 확인할 수 있고요.\n",
      "4980.00 -> 5004.10:  그렇지 복잡도를 비교했을 때에도 이 n은 단어의 개수이기 때문에 일반적으로 이 d보다는 조금 더 작게 형성되는 경우가 많습니다. n 제곱 곱하기 d가 n 곱하기 d 제곱보다는 더 낮을 확률이 높기 때문에 보다 유리한 복잡도를 가진다고 할 수 있습니다. 실제로 이렇게 본문에서도 내용이 설명되고 있는데요. 보통 이 n 즉 시퀀스의 길이가 이 d보다는 짧은 경우가 많기 때문에 훨씬 효율적일 수 있다고 말하고 있습니다.\n",
      "5010.00 -> 5038.26:  같다고 할 수 있죠. 그런 경우를 생각해 보았을 때 확실히 이 n이 일반적으로 d보다는 작게 형성되는 경우가 많습니다. 실제로 다양한 번역 데이터셋을 확인해 보시면 한 문장에 포함되어 있는 단어의 개수가 그렇게 많지 않기 때문에 그런 측면을 보았을 때 단어의 개수가 이러한 n이 되기 때문에 그러한 측면에서 효과적이라고 할 수 있습니다. 또한 추가적으로 attention 메커니즘 자체가 우리 뉴런 네트워크를 보다 설명 가능한 형태로 만들어준다는 점이 장점이라고 할 수 있습니다. 실제로 우리는 각 단어를 출력할 때\n",
      "5040.00 -> 5067.54:  가장 많이 참고해서 만들었는지를 시각적으로 출력해 볼 수 있습니다. 단순히 그냥 각각의 헤드에 포함되어 있는 그 셀프 어텐션 메커니즘의 소프트맥스 값을 출력해 보면 되겠죠. 또한 트레이닝을 진행할 때 설정했던 내용들은 다음과 같은데요. 말씀드렸듯이 영어 독일어 그리고 영어 프랑스어 이 두 가지 대표적인 데이터 세트에 대해서 실험했다고 하고요. WMT 2014년도 영어 독어 데이터 세트는 약 450만 개 정도의 문장 쌍이 존재하고요.\n",
      "5070.00 -> 5096.34:  3600만개 정도의 문장상이 존재하는 데이터셋을 이용했고요. 학습을 위한 하드웨어로는 8개의 NVIDIA P100 GPU를 사용했고요. 베이스 모델만으로도 State of the Art의 성능이 나오고 학습시간 또한 매우 빠른 12시간밖에 걸리지 않았다고 말하고 있습니다. 또한 이와 같이 Adam Optimizer를 사용하고 세부적인 파라미터는 다음과 같습니다. 또한 정교화 효과를 위해서 레지듀얼 러닝을 수행할 때 이 드랍아웃도 같이 사용할 수 있도록 만들었고요.\n",
      "5100.00 -> 5129.12:  우리 모델이 특정 출력값에 대해서 확신을 가지지 않도록 함으로써 정규화 효과를 더할 수 있다고 했습니다. 사실 이것도 굉장히 잘 알려진 정규화 기법이고 이미지 분류 등에서도 이미 많이 사용되고 있는 기법 중 하나죠. 그래서 이와 같이 어큐러시와 블루스코어를 높일 수 있었다고 말하고 있습니다. 그래서 여기 보이는 표가 실제 트랜스포머 아키텍처의 성능을 잘 보여주고 있는데요. 이와 같이 기본적인 베이스 모델만 가지고도 기존 State-of-the-Art 네트워크와 필적하는 좋은 성능을 내는 걸 확인할 수 있었고요.\n",
      "5130.00 -> 5156.56:  학습 시간은 훨씬 짧았다는 걸 확인할 수 있습니다. 또한 이러한 트랜스포머의 파라미터 수를 훨씬 늘려서 큰 모델을 사용했을 때에도 이전 연구와 비교했을 때 학습 효율이 높았으며 성능은 훨씬 더 개선된 걸 확인할 수 있습니다. 또한 이런 트랜스포머 아키텍처에서 어떤 컴포넌트가 상대적으로 중요한지에 대한 내용을 확인하기 위해서 모델 베리에이션 실험 또한 진행을 했는데요. 간단하게 헤드의 수를 줄여보거나 특정 파라미터의 수를 늘려보거나 줄여보거나 이런 실험들을 해본 겁니다.\n",
      "5160.00 -> 5188.54:  이렇게 여기 보이는 이 베이스 모델이 가장 기본적으로 사용한 하이퍼 파라미터라고 할 수 있고요. 여기서 A 같은 경우는 이 헤드의 디멘전을 바꿔가지고 그에 따라서 이 키와 밸류의 디멘전 또한 바뀔 수 있도록 한 겁니다. 말씀드렸듯이 D 모델을 H로 나눈 값이 키와 밸류의 디멘전으로 사용될 수 있다고 했죠. 그래서 확인해 보시면 이렇게 헤드를 8개 사용했을 때 가장 성능이 좋은 걸 확인할 수 있고요. 또한 B 같은 경우는 별도로 헤드와 상관없이 이 키 밸류의 디멘전을 더 줄여본 겁니다.\n",
      "5190.00 -> 5219.50:  당연히 파라미터의 수가 줄어서 모델의 캐퍼시티 또한 감소하게 되겠죠. 실제로 결과 또한 더 안 좋아지는 걸 확인할 수가 있었고요. 그래서 이렇게 64를 사용했을 때가 16이나 32를 사용했을 때보다 더 좋은 걸 확인할 수 있습니다. 또한 이렇게 모델의 크기를 더 키웠을 때 더 성능이 좋아진 것도 확인할 수 있습니다. 보시면 이런 식으로 인베딩 차원을 높이거나 이 Feed Forward 레이어에 포함되어 있는 차원을 높였을 때 더 성능이 좋아지는 걸 확인할 수 있고요. 또한 드랍아웃 기법은 이처럼 오버피팅 방지에 매우 효과적인 걸 확인할 수 있습니다.\n",
      "5220.00 -> 5248.52:  그래서 이렇게 드랍아웃을 썼을 때 더 성능이 많이 좋아진 걸 확인할 수 있습니다. 또한 위치에 대한 정보를 주기 위해 사인과 코사인 함수를 이용한 인코딩 대신에 별도의 인베딩 레이어를 사용했을 때 이때는 베이스 모델과 비교했을 때 성능의 차이는 거의 없는 걸 확인할 수 있습니다. 또한 이러한 트랜스포머는 비대한 기계 번역 뿐만 아니라 다양한 자연어 처리 테스트에서 사용이 가능한데요. 대표적으로 구문 분석 분야에 대해서 실험한 결과 또한 보여주고 있습니다. 여기 테이블 4번이 그 내용을 간단하게 보여주고 있고요.\n",
      "5250.00 -> 5279.26:  이 경우를 제외하고 나머지는 다른 State of the Art 네트워크와 비교했을 때 더 성능이 좋은 걸 확인할 수 있습니다. Semi-supervised case에 대해서도 마찬가지로 더 좋은 성능이 나오는 걸 확인할 수 있습니다. 그래서 결과적으로 이와 같이 본 논문에서는 트랜스포머 아키텍처를 제안했고요. 본 논문은 기존까지의 논문과 다르게 전적으로 Attention Mechanism만 활용을 해서 Recurrent한 네트워크 자체를 전부 다 아키텍처에서 빼버렸고요. 이로 인해 보다 높은 병렬성을 얻게 되고 성능 또한 많이 개선될 수 있었습니다.\n",
      "5280.00 -> 5296.18:  실제로 기계 번역 태스크에 대해서 기존까지 존재했던 다른 아키텍처에 비해서 더 좋은 성능을 보여줄 수가 있었고 비단 기계 번역 뿐만 아니라 다양한 태스크에 대해서도 적용 가능성이 높다는 것까지 잘 보여주었습니다. 이상으로 이번 시간에는 트랜스포머에 대해서 알아보았습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import math\n",
    "import requests\n",
    "import tempfile\n",
    "import os\n",
    "import concurrent.futures\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import threading\n",
    "import ffmpeg\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, total_size, desc=\"Downloading\"):\n",
    "        self.pbar = tqdm(total=total_size, unit='iB', unit_scale=True, desc=desc)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def update(self, size):\n",
    "        with self.lock:\n",
    "            self.pbar.update(size)\n",
    "\n",
    "    def close(self):\n",
    "        self.pbar.close()\n",
    "\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.1,\n",
    "        status_forcelist=[500, 502, 503, 504]\n",
    "    )\n",
    "    adapter = HTTPAdapter(\n",
    "        max_retries=retry,\n",
    "        pool_connections=100,\n",
    "        pool_maxsize=100\n",
    "    )\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def download_chunk(args):\n",
    "    url, start, end, chunk_number, temp_dir, progress_bar = args\n",
    "    \n",
    "    headers = {'Range': f'bytes={start}-{end}'}\n",
    "    session = create_session()\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, headers=headers, stream=True)\n",
    "        chunk_path = os.path.join(temp_dir, f'chunk_{chunk_number:04d}')\n",
    "        \n",
    "        with open(chunk_path, 'wb') as f:\n",
    "            for data in response.iter_content(chunk_size=8192):\n",
    "                size = f.write(data)\n",
    "                progress_bar.update(size)\n",
    "        \n",
    "        return chunk_path, chunk_number\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading chunk {chunk_number}: {str(e)}\")\n",
    "        return None, chunk_number\n",
    "\n",
    "def parallel_download(url, temp_dir, num_chunks=10):\n",
    "    session = create_session()\n",
    "    response = session.head(url)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    if total_size == 0:\n",
    "        raise ValueError(\"Could not determine file size\")\n",
    "    \n",
    "    chunk_size = total_size // num_chunks\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size - 1 if i < num_chunks - 1 else total_size - 1\n",
    "        chunks.append((start, end))\n",
    "    \n",
    "    progress_bar = ProgressBar(total_size, \"Parallel downloading\")\n",
    "    \n",
    "    download_args = [\n",
    "        (url, start, end, i, temp_dir, progress_bar)\n",
    "        for i, (start, end) in enumerate(chunks)\n",
    "    ]\n",
    "    \n",
    "    chunk_paths = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_chunks) as executor:\n",
    "        futures = executor.map(download_chunk, download_args)\n",
    "        chunk_paths = [(path, num) for path, num in futures if path is not None]\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    chunk_paths.sort(key=lambda x: x[1])\n",
    "    output_path = os.path.join(temp_dir, \"complete_audio.mp4\")\n",
    "    \n",
    "    with open(output_path, 'wb') as outfile:\n",
    "        for chunk_path, _ in chunk_paths:\n",
    "            with open(chunk_path, 'rb') as infile:\n",
    "                outfile.write(infile.read())\n",
    "            os.remove(chunk_path)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def convert_to_wav(input_path, output_path):\n",
    "    \"\"\"MP4를 WAV로 변환\"\"\"\n",
    "    try:\n",
    "        stream = ffmpeg.input(input_path)\n",
    "        stream = ffmpeg.output(stream, output_path, \n",
    "                             acodec='pcm_s16le', \n",
    "                             ar='16000',\n",
    "                             ac='1')\n",
    "        ffmpeg.run(stream, capture_stdout=True, capture_stderr=True)\n",
    "        return True\n",
    "    except ffmpeg.Error as e:\n",
    "        print('FFmpeg error:', e.stderr.decode())\n",
    "        return False\n",
    "\n",
    "def process_audio_chunk(chunk_data):\n",
    "    \"\"\"개별 오디오 청크 처리\"\"\"\n",
    "    model, audio_path, start_time, duration = chunk_data\n",
    "    try:\n",
    "        segments, info = model.transcribe(\n",
    "            audio_path,\n",
    "            beam_size=5,\n",
    "            batch_size=32,\n",
    "            word_timestamps=True,\n",
    "            initial_prompt=None\n",
    "        )\n",
    "        \n",
    "        # segments를 리스트로 변환하고 시간 조정\n",
    "        chunk_segments = []\n",
    "        for segment in segments:\n",
    "            segment_dict = {\n",
    "                'start': segment.start + start_time,\n",
    "                'end': segment.end + start_time,\n",
    "                'text': segment.text,\n",
    "                'words': [\n",
    "                    {\n",
    "                        'start': word.start + start_time,\n",
    "                        'end': word.end + start_time,\n",
    "                        'word': word.word,\n",
    "                        'probability': word.probability\n",
    "                    }\n",
    "                    for word in segment.words\n",
    "                ]\n",
    "            }\n",
    "            chunk_segments.append(segment_dict)\n",
    "        \n",
    "        return chunk_segments\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk at {start_time}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def process_with_progress(url, model, chunk_duration=30, num_download_chunks=10):\n",
    "    \"\"\"\n",
    "    전체 처리 프로세스 관리\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        print(\"Starting parallel download...\")\n",
    "        mp4_path = parallel_download(url, temp_dir, num_download_chunks)\n",
    "        print(\"Download complete!\")\n",
    "        \n",
    "        # MP4를 WAV로 변환\n",
    "        wav_path = os.path.join(temp_dir, \"audio.wav\")\n",
    "        if not convert_to_wav(mp4_path, wav_path):\n",
    "            raise Exception(\"Failed to convert audio to WAV format\")\n",
    "        \n",
    "        # WAV 파일 정보 읽기\n",
    "        wav_info = sf.info(wav_path)\n",
    "        total_duration = wav_info.duration\n",
    "        \n",
    "        # 청크 계산\n",
    "        total_chunks = math.ceil(total_duration / chunk_duration)\n",
    "        \n",
    "        # 진행률 표시\n",
    "        pbar = tqdm(total=total_chunks, desc=\"Processing audio chunks\")\n",
    "        \n",
    "        # 청크 처리를 위한 데이터 준비\n",
    "        chunks_data = []\n",
    "        for i in range(total_chunks):\n",
    "            start_time = i * chunk_duration\n",
    "            chunk_wav_path = os.path.join(temp_dir, f\"chunk_{i}.wav\")\n",
    "            \n",
    "            # 청크 추출\n",
    "            duration = min(chunk_duration, total_duration - start_time)\n",
    "            stream = ffmpeg.input(wav_path, ss=start_time, t=duration)\n",
    "            stream = ffmpeg.output(stream, chunk_wav_path, \n",
    "                                 acodec='pcm_s16le', \n",
    "                                 ar='16000',\n",
    "                                 ac='1')\n",
    "            ffmpeg.run(stream, quiet=True)\n",
    "            \n",
    "            chunks_data.append((model, chunk_wav_path, start_time, duration))\n",
    "        \n",
    "        # 청크 처리 및 결과 수집\n",
    "        all_segments = []\n",
    "        for chunk_data in chunks_data:\n",
    "            segments = process_audio_chunk(chunk_data)\n",
    "            all_segments.extend(segments)\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # 사용한 청크 파일 삭제\n",
    "            if os.path.exists(chunk_data[1]):\n",
    "                os.remove(chunk_data[1])\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "    return all_segments\n",
    "\n",
    "# 모델 초기화\n",
    "model = WhisperModel(\n",
    "    \"large-v3\", \n",
    "    device='cuda', \n",
    "    compute_type=\"float16\"  # bfloat16 대신 float16 사용\n",
    ")\n",
    "print(\"Whisper 모델 초기화 완료\")\n",
    "model = BatchedInferencePipeline(model=model)\n",
    "\n",
    "# 트랜스크립션 실행\n",
    "segments = process_with_progress(\n",
    "    audio_stream.url,\n",
    "    model,\n",
    "    chunk_duration=30,\n",
    "    num_download_chunks=10\n",
    ")\n",
    "\n",
    "# 결과 저장\n",
    "for i, segment in enumerate(segments):\n",
    "    print(f\"{segment['start']:.2f} -> {segment['end']:.2f}: {segment['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper 모델 초기화 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio chunks:   0%|          | 0/177 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 157\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhisper 모델 초기화 완료\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# 트랜스크립션 실행\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_stream_with_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 유튜브 URL\u001b[39;49;00m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[1;32m    161\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# 결과 출력\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m segments:\n",
      "Cell \u001b[0;32mIn[1], line 76\u001b[0m, in \u001b[0;36mprocess_stream_with_progress\u001b[0;34m(url, model, chunk_duration)\u001b[0m\n\u001b[1;32m     72\u001b[0m chunk_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# 청크 읽기\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     audio_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m audio_chunk:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# url = \"https://youtu.be/AA621UofTUA?si=gn4XutRMWUDSYLFL\"\n",
    "\n",
    "# from faster_whisper import WhisperModel\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# import tempfile\n",
    "# import os\n",
    "# import ffmpeg\n",
    "# import subprocess\n",
    "# from yt_dlp import YoutubeDL\n",
    "# import io\n",
    "\n",
    "# def get_audio_stream(url):\n",
    "#     \"\"\"URL에서 오디오 스트림 정보를 가져옵니다.\"\"\"\n",
    "#     ydl_opts = {\n",
    "#         'format': 'bestaudio/best',\n",
    "#         'quiet': True,\n",
    "#         'no_warnings': True,\n",
    "#         'extract_audio': True\n",
    "#     }\n",
    "    \n",
    "#     with YoutubeDL(ydl_opts) as ydl:\n",
    "#         info = ydl.extract_info(url, download=False)\n",
    "#         audio_url = info['url']\n",
    "#         duration = info.get('duration', 0)\n",
    "        \n",
    "#         return audio_url, duration\n",
    "\n",
    "# def process_stream_with_progress(url, model, chunk_duration=30):\n",
    "#     \"\"\"\n",
    "#     스트리밍 방식으로 오디오를 처리합니다.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - url: 오디오 URL\n",
    "#     - model: WhisperModel 인스턴스\n",
    "#     - chunk_duration: 각 청크의 길이(초)\n",
    "#     \"\"\"\n",
    "#     # 스트림 URL 가져오기\n",
    "#     audio_url, total_duration = get_audio_stream(url)\n",
    "    \n",
    "#     # ffmpeg 명령어 설정\n",
    "#     ffmpeg_cmd = [\n",
    "#         'ffmpeg',\n",
    "#         '-i', audio_url,\n",
    "#         '-f', 'wav',\n",
    "#         '-ar', '16000',\n",
    "#         '-ac', '1',\n",
    "#         '-hide_banner',\n",
    "#         '-loglevel', 'error',\n",
    "#         'pipe:1'\n",
    "#     ]\n",
    "    \n",
    "#     # 진행률 표시 설정\n",
    "#     total_chunks = int(np.ceil(total_duration / chunk_duration))\n",
    "#     pbar = tqdm(total=total_chunks, desc=\"Processing audio chunks\")\n",
    "    \n",
    "#     # 결과 저장용 리스트\n",
    "#     all_segments = []\n",
    "    \n",
    "#     try:\n",
    "#         # ffmpeg 프로세스 시작\n",
    "#         process = subprocess.Popen(\n",
    "#             ffmpeg_cmd,\n",
    "#             stdout=subprocess.PIPE,\n",
    "#             bufsize=10**8  # 버퍼 크기 설정\n",
    "#         )\n",
    "        \n",
    "#         # 임시 디렉토리 생성\n",
    "#         with tempfile.TemporaryDirectory() as temp_dir:\n",
    "#             chunk_size = int(16000 * chunk_duration * 2)  # 16000Hz * seconds * 2 bytes per sample\n",
    "#             chunk_number = 0\n",
    "            \n",
    "#             while True:\n",
    "#                 # 청크 읽기\n",
    "#                 audio_chunk = process.stdout.read(chunk_size)\n",
    "#                 if not audio_chunk:\n",
    "#                     break\n",
    "                \n",
    "#                 # 청크를 임시 파일로 저장\n",
    "#                 chunk_path = os.path.join(temp_dir, f'chunk_{chunk_number}.wav')\n",
    "#                 with open(chunk_path, 'wb') as f:\n",
    "#                     # WAV 헤더 작성\n",
    "#                     f.write(b'RIFF')\n",
    "#                     f.write((chunk_size + 36).to_bytes(4, 'little'))\n",
    "#                     f.write(b'WAVE')\n",
    "#                     f.write(b'fmt ')\n",
    "#                     f.write((16).to_bytes(4, 'little'))\n",
    "#                     f.write((1).to_bytes(2, 'little'))  # PCM\n",
    "#                     f.write((1).to_bytes(2, 'little'))  # Mono\n",
    "#                     f.write((16000).to_bytes(4, 'little'))  # Sample rate\n",
    "#                     f.write((32000).to_bytes(4, 'little'))  # Byte rate\n",
    "#                     f.write((2).to_bytes(2, 'little'))  # Block align\n",
    "#                     f.write((16).to_bytes(2, 'little'))  # Bits per sample\n",
    "#                     f.write(b'data')\n",
    "#                     f.write(len(audio_chunk).to_bytes(4, 'little'))\n",
    "#                     f.write(audio_chunk)\n",
    "                \n",
    "#                 try:\n",
    "#                     # 청크 처리\n",
    "#                     segments, _ = model.transcribe(\n",
    "#                         chunk_path,\n",
    "#                         beam_size=5,\n",
    "#                         batch_size=32,\n",
    "#                         word_timestamps=True,\n",
    "#                         condition_on_previous_text=True\n",
    "#                     )\n",
    "                    \n",
    "#                     # 시간 오프셋 조정 및 세그먼트 저장\n",
    "#                     time_offset = chunk_number * chunk_duration\n",
    "#                     for segment in segments:\n",
    "#                         segment_dict = {\n",
    "#                             'start': segment.start + time_offset,\n",
    "#                             'end': segment.end + time_offset,\n",
    "#                             'text': segment.text,\n",
    "#                             'words': [\n",
    "#                                 {\n",
    "#                                     'start': word.start + time_offset,\n",
    "#                                     'end': word.end + time_offset,\n",
    "#                                     'word': word.word,\n",
    "#                                     'probability': word.probability\n",
    "#                                 }\n",
    "#                                 for word in segment.words\n",
    "#                             ]\n",
    "#                         }\n",
    "#                         all_segments.append(segment_dict)\n",
    "                \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing chunk {chunk_number}: {str(e)}\")\n",
    "                \n",
    "#                 finally:\n",
    "#                     # 임시 파일 삭제\n",
    "#                     if os.path.exists(chunk_path):\n",
    "#                         os.remove(chunk_path)\n",
    "                \n",
    "#                 # 진행률 업데이트\n",
    "#                 pbar.update(1)\n",
    "#                 chunk_number += 1\n",
    "    \n",
    "#     finally:\n",
    "#         pbar.close()\n",
    "#         if process.poll() is None:\n",
    "#             process.terminate()\n",
    "#             process.wait()\n",
    "    \n",
    "#     return all_segments\n",
    "\n",
    "# # 모델 초기화\n",
    "# model = WhisperModel(\n",
    "#     \"large-v3\", \n",
    "#     device='cuda', \n",
    "#     compute_type=\"float16\"\n",
    "# )\n",
    "# print(\"Whisper 모델 초기화 완료\")\n",
    "\n",
    "# # 트랜스크립션 실행\n",
    "# segments = process_stream_with_progress(\n",
    "#     url,  # 유튜브 URL\n",
    "#     model,\n",
    "#     chunk_duration=30\n",
    "# )\n",
    "\n",
    "# # 결과 출력\n",
    "# for segment in segments:\n",
    "#     print(f\"{segment['start']:.2f} -> {segment['end']:.2f}: {segment['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"script.json\",\"r\",encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "            Document(page_content=\"\\n\".join([t[\"text\"] for t in data]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the last chapter, you and I started to step through the internal workings of a transformer.\\nThis is one of the key pieces of technology inside large language models, and a lot of\\nother tools in the modern wave of AI.\\nIt first hit the scene in a now-famous 2017 paper called Attention is All You Need, and\\nin this chapter, you and I will dig into what this attention mechanism is, visualizing how\\nit processes data.\\nAs a quick recap, here's the important context I want you to have in mind.\\nThe goal of the model that you and I are studying is to take in a piece of text and predict\\nwhat word comes next.\\nThe input text is broken up into little pieces that we call tokens, and these are very often\\nwords or pieces of words, but just to make the examples in this video easier for you\\nand me to think about, let's simplify by pretending that tokens are always just words.\\nThe first step in a transformer is to associate each token with a high-dimensional vector,\\nwhat we call its embedding.\\nNow the most important idea I want you to have in mind is how directions in this high-dimensional\\nspace of all possible embeddings can correspond with semantic meaning.\\nIn the last chapter we saw an example for how direction can correspond to gender, in\\nthe sense that adding a certain step in this space can take you from the embedding of a\\nmasculine noun to the embedding of the corresponding feminine noun.\\njust one example, you could imagine how many other directions in this high-dimensional space\\ncould correspond to numerous other aspects of a word's meaning. The aim of a transformer is to\\nprogressively adjust these embeddings so that they don't merely encode an individual word,\\nbut instead they bake in some much, much richer contextual meaning. I should say up front that a\\nlot of people find the attention mechanism, this key piece in a transformer, very confusing, so\\ndon't worry if it takes some time for things to sink in. I think that before we dive into the\\ncomputational details and all the matrix multiplications, it's worth thinking about a\\ncouple examples for the kind of behavior that we want attention to enable. Consider the phrases\\nAmerican true mole, one mole of carbon dioxide, and take a biopsy of the mole. You and I know\\nthat the word mole has different meanings in each one of these, based on the context.\\nBut after the first step of a transformer, the one that breaks up the text and associates each\\ntoken with a vector, the vector that's associated with mole would be the same in all three of these\\ncases, because this initial token embedding is effectively a lookup table with no reference to\\nthe context. It's only in the next step of the transformer that the surrounding embeddings have\\nthe chance to pass information into this one. The picture you might have in mind is that there\\nare multiple distinct directions in this embedding space encoding the multiple distinct meanings of\\nthe word mole, and that a well-trained attention block calculates what you need to add to the\\ngeneric embedding to move it to one of these more specific directions, as a function of the context.\\nTo take another example, consider the embedding of the word tower. This is presumably some very\\ngeneric, non-specific direction in the space, associated with lots of other large, tall nouns.\\nIf this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update\\nthis vector so that it points in a direction that more specifically encodes the Eiffel Tower,\\nmaybe correlated with vectors associated with Paris and France and things made of steel.\\nIf it was also preceded by the word miniature, then the vector should be updated even further\\nso that it no longer correlates with large tall things. More generally than just refining the\\nmeaning of a word, the attention block allows the model to move information encoded in one\\nembedding to that of another, potentially ones that are quite far away, and potentially\\nwith information that's much richer than just a single word.\\nWhat we saw in the last chapter was how after all of the vectors flow through the network,\\nincluding many different attention blocks, the computation that you perform to produce\\na prediction of the next token is entirely a function of the last vector in the sequence.\\nSo imagine, for example, that the text you input is most of an entire mystery novel,\\nway up to a point near the end which reads, therefore the murderer was, if the model is\\ngoing to accurately predict the next word, that final vector in the sequence which began its life\\nsimply embedding the word was will have to have been updated by all of the attention blocks\\nto represent much much more than any individual word, somehow encoding all of the information\\nfrom the full context window that's relevant to predicting the next word. To step through the\\nthe computations though let's take a much simpler example. Imagine that the input includes the\\nphrase a fluffy blue creature roamed the verdant forest and for the moment suppose that the only\\ntype of update that we care about is having the adjectives adjust the meanings of their\\ncorresponding nouns. What I'm about to describe is what we would call a single head of attention\\nand later we will see how the attention block consists of many different heads run in parallel.\\nAgain, the initial embedding for each word is some high-dimensional vector\\nthat only encodes the meaning of that particular word with no context.\\nActually, that's not quite true. They also encode the position of the word.\\nThere's a lot more to say about the specific way that positions are encoded,\\nbut right now all you need to know is that the entries of this vector are enough to tell you\\nboth what the word is and where it exists in the context. Let's go ahead and denote these\\nembeddings with the letter E, the goal is to have a series of computations produce a\\nnew refined set of embeddings where, for example, those corresponding to the nouns have ingested\\nthe meaning from their corresponding adjectives.\\nAnd playing the deep learning game, we want most of the computations involved to look\\nlike matrix-vector products where the matrices are full of tunable weights, things that the\\nmodel will learn based on data.\\nTo be clear, I'm making up this example of adjectives updating nouns just to illustrate\\nthe type of behavior that you could imagine an intention had doing.\\nAs with so much deep learning, the true behavior is much harder to parse, because it's based\\non tweaking and tuning a huge number of parameters to minimize some cost function.\\nIt's just that as we step through all of the different matrices filled with parameters\\nthat are involved in this process, I think it's really helpful to have an imagined example\\nof something that it could be doing to help keep it all more concrete.\\nFor the first step of this process, you might imagine each noun, like creature, asking the\\nquestion, hey, are there any adjectives sitting in front of me, and for the words fluffy and\\nblue to each be able to answer, yeah, I'm an adjective and I'm in that position.\\nThat question is somehow encoded as yet another vector, another list of numbers, which we\\ncall the query for this word.\\nThis query vector, though, has a much smaller dimension than the embedding vector, say 128.\\nComputing this query looks like taking a certain matrix, which I'll label wq, and multiplying\\nit by the embedding.\\nCompressing things a bit, let's write that query vector as q, and then anytime you see\\nme put a matrix next to an arrow like this one, it's meant to represent that multiplying\\nthis matrix by the vector at the arrow's start gives you the vector at the arrow's end.\\nIn this case, you multiply this matrix by all of the embeddings in the context, producing\\none query vector for each token.\\nThe entries of this matrix are parameters of the model, which means the true behavior\\nis learned from data, and in practice what this matrix does in a particular attention\\nhead is challenging to parse.\\nBut for our sake, imagining an example that we might hope it would learn, we'll suppose\\nthat this query matrix maps the embeddings of nouns to certain directions in this smaller\\nquery space that somehow encodes the notion of looking for adjectives in preceding positions.\\nAs to what it does to other embeddings, who knows, maybe it simultaneously tries to accomplish\\nsome other goal with those, right now we're laser focused on the nouns.\\nAt the same time, associated with this is a second matrix called the key matrix, which\\nyou also multiply by every one of the embeddings.\\nThis produces a second sequence of vectors that we call the keys.\\nConceptually you want to think of the keys as potentially answering the queries.\\nThis key matrix is also full of tunable parameters, and just like the query matrix it maps the\\nembedding vectors to that same smaller dimensional space.\\nYou think of the keys as matching the queries whenever they closely align with each other.\\nIn our example, you would imagine that the key matrix maps the adjectives, like fluffy\\nand blue, to vectors that are closely aligned with the query produced by the word creature.\\nTo measure how well each key matches each query, you compute a dot product between each\\npossible key-query pair.\\nI like to visualize a grid full of a bunch of dots, where the bigger dots correspond\\nthe larger dot products, the places where the keys and queries align. For our adjective-noun example,\\nthat would look a little more like this, where if the keys produced by fluffy and blue really do\\nalign closely with the query produced by creature, then the dot products in these two spots would be\\nsome large positive numbers. In the lingo, machine learning people would say that this means the\\nembeddings of fluffy and blue attend to the embedding of creature. By contrast to the dot\\nproduct between the key for some other word like the and the query for creature would be some small\\nor negative value that reflects that these are unrelated to each other. So we have this grid of\\nvalues that can be any real number from negative infinity to infinity giving us a score for how\\nrelevant each word is to updating the meaning of every other word. The way we're about to use these\\nscores is to take a certain weighted sum along each column weighted by the relevance. So instead\\nInstead of having values range from negative infinity to infinity, what we want is for\\nthe numbers in these columns to be between 0 and 1, and for each column to add up to\\n1, as if they were a probability distribution.\\nIf you're coming in from the last chapter, you know what we need to do then.\\nWe compute a softmax along each one of these columns to normalize the values.\\nIn our picture, after you apply softmax to all of the columns, we'll fill in the grid\\nwith these normalized values.\\nAt this point, you're safe to think about each column as giving weights\\naccording to how relevant the word on the left is to the corresponding value at the top.\\nWe call this grid an attention pattern.\\nNow, if you look at the original Transformer paper,\\nthere's a really compact way that they write this all down.\\nHere, the variables q and k represent the full arrays of query and key vectors respectively,\\nthose little vectors you get by multiplying the embeddings by the query and the key matrices.\\nThis expression up in the numerator is a really compact way to represent the grid of all possible\\ndot products between pairs of keys and queries. A small technical detail that I didn't mention\\nis that for numerical stability it happens to be helpful to divide all of these values by the\\nsquare root of the dimension in that key query space. Then this softmax that's wrapped around\\nthe full expression, is meant to be understood to apply column by column.\\nAs to that V term, we'll talk about it in just a second.\\nBefore that, there's one other technical detail that so far I've skipped.\\nDuring the training process, when you run this model on a given text example, and all\\nof the weights are slightly adjusted and tuned to either reward or punish it based on how\\nhigh a probability it assigns to the true next word in the passage, it turns out to\\nmake the whole training process a lot more efficient if you simultaneously have it predict\\nevery possible next token following each initial sub-sequence of tokens in this passage.\\nFor example, with the phrase that we've been focusing on, it might also be predicting what\\nwords follow creature, and what words follow the.\\nThis is really nice, because it means what would otherwise be a single training example\\neffectively acts as many.\\nFor the purposes of our attention pattern, it means that you never want to allow later\\nwords to influence earlier words, since otherwise they could kind of give away the answer for\\nwhat comes next. What this means is that we want all of these spots here, the ones representing\\nlater tokens influencing earlier ones, to somehow be forced to be zero. The simplest thing you might\\nthink to do is to set them equal to zero, but if you did that the columns wouldn't add up to one\\nanymore, they wouldn't be normalized. So instead a common way to do this is that before applying\\nsoftmax you set all of those entries to be negative infinity. If you do that then after\\nAfter applying softmax, all of those get turned into zero, but the columns stay normalized.\\nThis process is called masking.\\nThere are versions of attention where you don't apply it, but in our GPT example, even\\nthough this is more relevant during the training phase than it would be, say, running it as\\na chatbot or something like that, you do always apply this masking to prevent later tokens\\nfrom influencing earlier ones.\\nAnother fact that's worth reflecting on about this attention pattern is how its size is\\nequal to the square of the context size.\\nSo this is why context size can be a really huge bottleneck for large language models,\\nand scaling it up is non-trivial.\\nAs you might imagine, motivated by a desire for bigger and bigger context windows, recent\\nyears have seen some variations to the attention mechanism aimed at making context more scalable.\\nBut right here, you and I are staying focused on the basics.\\nOkay, great, computing this pattern lets the model deduce which words are relevant to which\\nother words.\\nNow you need to actually update the embeddings, allowing words to pass information to whichever\\nother words they're relevant to.\\nFor example, you want the embedding of fluffy to somehow cause a change to creature that\\nmoves it to a different part of this 12,000 dimensional embedding space that more specifically\\nencodes a fluffy creature.\\nWhat I'm going to do here is first show you the most straightforward way that you could\\ndo this, though there's a slight way that this gets modified in the context of multi-headed\\nattention.\\nThis most straightforward way would be to use a third matrix, what we call the value\\nmatrix, which you multiply by the embedding of that first word, for example fluffy.\\nThe result of this is what you would call a value vector, and this is something that\\nyou add to the embedding of the second word, in this case something you add to the embedding\\nof creature.\\nSo, this value vector lives in the same very high dimensional space as the embeddings.\\nWhen you multiply this value matrix by the embedding of a word, you might think of it\\nas saying if this word is relevant to adjusting the meaning of something else, what exactly should\\nbe added to the embedding of that something else in order to reflect this? Looking back in our\\ndiagram, let's set aside all of the keys and the queries, since after you compute the attention\\npattern you're done with those, then you're going to take this value matrix and multiply it by every\\none of those embeddings to produce a sequence of value vectors. You might think of these value\\nvectors as being kind of associated with the corresponding keys.\\nFor each column in this diagram, you multiply each of the value vectors by the corresponding\\nweight in that column.\\nFor example, here, under the embedding of creature, you would be adding large proportions\\nof the value vectors for fluffy and blue, while all of the other value vectors get zeroed\\nout, or at least nearly zeroed out.\\nAnd then finally, the way to actually update the embedding associated with this column,\\npreviously encoding some context-free meaning of creature, you add together all of these\\nrescaled values in the column, producing a change that you want to add that I'll label\\ndelta E, and then you add that to the original embedding.\\nHopefully what results is a more refined vector encoding the more contextually rich meaning,\\nlike that of a fluffy blue creature.\\nAnd of course you don't just do this to one embedding, you apply the same weighted sum\\nacross all of the columns in this picture, producing a sequence of changes.\\nAdding all of those changes to the corresponding embeddings produces a full sequence of more\\nrefined embeddings popping out of the attention block.\\nZooming out, this whole process is what you would describe as a single head of attention.\\nAs I've described things so far, this process is parameterized by three distinct matrices,\\nall filled with tunable parameters, the key, the query, and the value.\\nI want to take a moment to continue what we started in the last chapter with the scorekeeping\\nwhere we count up the total number of model parameters using the numbers from GPT-3.\\nThese key and query matrices each have 12,288 columns, matching the embedding dimension,\\nand 128 rows, matching the dimension of that smaller key query space.\\nThis gives us an additional 1.5 million or so parameters for each one.\\nIf you look at that value matrix by contrast, the way I've described things so far would\\nsuggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both\\nits inputs and its outputs live in this very large embedding space.\\nIf true, that would mean about 150 million added parameters.\\nAnd to be clear, you could do that, you could devote orders of magnitude more parameters\\nto the value map than to the key and query.\\nBut in practice, it is much more efficient if instead you make it so that the number\\nof parameters devoted to this value map is the same as the number devoted to the key\\nin the query.\\nThis is especially relevant in the setting of running multiple attention heads in parallel.\\nThe way this looks is that the value map is factored as a product of two smaller matrices.\\nConceptually, I would still encourage you to think about the overall linear map, one\\nwith inputs and outputs both in this larger embedding space, for example taking the embedding\\nof blue to this blueness direction that you would add to nouns.\\nIt's just that it's broken up into two separate steps.\\nThe first matrix on the right here has a smaller number of rows, typically the same size as\\nthe key query space.\\nWhat this means is you can think of it as mapping the large embedding vectors down to\\na much smaller space.\\nThis is not the conventional naming, but I'm going to call this the value down matrix.\\nThe second matrix maps from this smaller space back up to the embedding space, producing\\nthe vectors that you use to make the actual updates.\\nI'm going to call this one the value-up matrix, which, again, is not conventional.\\nThe way that you would see this written in most papers looks a little different.\\nI'll talk about it in a minute.\\nIn my opinion, it tends to make things a little more conceptually confusing.\\nTo throw in linear algebra jargon here, what we're basically doing is constraining the\\noverall value map to be a low-rank transformation.\\nTurning back to the parameter count, all four of these matrices have the same size, and\\nThen adding them all up, we get about 6.3 million parameters for one attention head.\\nAs a quick side note, to be a little more accurate, everything described so far is what\\npeople would call a self-attention head, to distinguish it from a variation that comes\\nup in other models that's called cross-attention.\\nThis isn't relevant to our GPT example, but if you're curious, cross-attention involves\\nmodels that process two distinct types of data, like text in one language and text in\\nanother language that's part of an ongoing generation of a translation.\\nOr maybe audio input of speech, and an ongoing transcription.\\nA cross-attention head looks almost identical.\\nThe only difference is that the key and query maps act on different datasets.\\nIn a model doing translation, for example, the keys might come from one language, while\\nthe queries come from another, and the attention pattern could describe which words from one\\nlanguage correspond to which words in another.\\nAnd in this setting there would typically be no masking, since there's not really any\\nnotion of later tokens affecting earlier ones.\\nStaying focused on self-attention though, if you understood everything so far, and if\\nyou were to stop here, you would come away with the essence of what attention really\\nis.\\nAll that's really left to us is to lay out the sense in which you do this many, many\\ndifferent times.\\nIn our central example we focused on adjectives updating nouns, but of course there are lots\\nof different ways that context can influence the meaning of a word.\\nIf the words they crashed the preceded the word car, it has implications for the shape\\nand the structure of that car, and a lot of associations might be less grammatical.\\nIf the word wizard is anywhere in the same passage as Harry, it suggests that this might\\nbe referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were\\nin that passage, then perhaps the embedding of Harry should instead be updated to refer\\nto the prince.\\nFor every different type of contextual updating that you might imagine, the parameters of\\nthese key and query matrices would be different to capture the different attention patterns,\\nand the parameters of our value map would be different based on what should be added to the\\nembeddings. And again, in practice the true behavior of these maps is much more difficult\\nto interpret, where the weights are set to do whatever the model needs them to do to best\\naccomplish its goal of predicting the next token. As I said before, everything we described is a\\nsingle head of attention, and a full attention block inside a transformer consists of what's\\ncalled multi-headed attention where you run a lot of these operations in parallel each with its own\\ndistinct key query and value maps. GPT-3 for example uses 96 attention heads inside each block.\\nConsidering that each one is already a bit confusing it's certainly a lot to hold in your\\nhead. Just to spell it all out very explicitly this means you have 96 distinct key and query\\nmatrices producing 96 distinct attention patterns. Then each head has its own distinct value matrices\\nused to produce 96 sequences of value vectors. These are all added together using the\\ncorresponding attention patterns as weights. What this means is that for each position in the\\ncontext, each token, every one of these heads produces a proposed change to be added to the\\nembedding in that position. So what you do is you sum together all of those proposed changes,\\none for each head, and you add the result to the original embedding of that position.\\nThis entire sum here would be one slice of what's outputted from this multi-headed attention block,\\na single one of those refined embeddings that pops out the other end of it.\\nAgain, this is a lot to think about, so don't worry at all if it takes some time to sink in.\\nThe overall idea is that by running many distinct heads in parallel,\\nyou're giving the model the capacity to learn many distinct ways that context changes meaning.\\nPulling up our running tally for parameter count with 96 heads, each including its own variation\\nof these four matrices, each block of multi-headed attention ends up with around 600 million\\nparameters. There's one added slightly annoying thing that I should really mention for any of you\\nwho go on to read more about transformers. You remember how I said that the value map is factored\\nout into these two distinct matrices, which I labeled as the value down and the value up\\nmatrices. The way that I framed things would suggest that you see this pair of matrices\\ninside each attention head, and you could absolutely implement it this way. That would\\nbe a valid design. But the way that you see this written in papers and the way that it's\\nimplemented in practice looks a little different. All of these value up matrices for each head\\nappear stapled together in one giant matrix that we call the output matrix, associated with\\nthe entire multi-headed attention block. And when you see people refer to the value matrix for a\\ngiven attention head, they're typically only referring to this first step, the one that I\\nwas labeling as the value down projection into the smaller space. For the curious among you,\\nI've left an on-screen note about it. It's one of those details that runs the risk of distracting\\nfrom the main conceptual points, but I do want to call it out just so that you know if you read\\nabout this in other sources. Setting aside all the technical nuances, in the preview from the\\nlast chapter, we saw how data flowing through a transformer doesn't just flow through a single\\nattention block. For one thing, it also goes through these other operations called multi-layer\\nperceptrons. We'll talk more about those in the next chapter. And then it repeatedly goes through\\nmany, many copies of both of these operations. What this means is that after a given word imbibes\\nsome of its context, there are many more chances for this more nuanced embedding to be influenced\\nby its more nuanced surroundings. The further down the network you go, with each embedding\\ntaking in more and more meaning from all the other embeddings, which themselves are getting\\nmore and more nuanced, the hope is that there's the capacity to encode higher level and more\\nabstract ideas about a given input beyond just descriptors and grammatical structure.\\nThings like sentiment and tone and whether it's a poem and what underlying scientific truths are\\nare relevant to the piece, and things like that.\\nTurning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the\\ntotal number of key, query, and value parameters is multiplied by another 96, which brings\\nthe total sum to just under 58 billion distinct parameters devoted to all of the attention\\nheads.\\nThat is a lot, to be sure, but it's only about a third of the 175 billion that are\\nin the network in total.\\nSo even though attention gets all of the attention, the majority of parameters come from the blocks\\nsitting in between these steps.\\nIn the next chapter, you and I will talk more about those other blocks and also a lot more\\nabout the training process.\\nA big part of the story for the success of the attention mechanism is not so much any\\nspecific kind of behavior that it enables, but the fact that it's extremely parallelizable,\\nmeaning that you can run a huge number of computations in a short time using GPUs.\\nthat one of the big lessons about deep learning in the last decade or two has been that scale\\nalone seems to give huge qualitative improvements in model performance. There's a huge advantage to\\nparallelizable architectures that let you do this. If you want to learn more about this stuff, I've\\nleft lots of links in the description. In particular, anything produced by Andre Karpathy or Chris Ola\\ntend to be pure gold. In this video, I wanted to just jump into attention in its current form,\\nbut if you're curious about more of the history for how we got here and how you might reinvent\\nthis idea for yourself, my friend Vivek just put up a couple videos giving a lot more of\\nthat motivation. Also, Britt Cruz from the channel The Art of the Problem\\nhas a really nice video about the history of large language models.\\nyou\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def calculate_tokens(text, model=\"gpt-4o-mini\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5728"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_tokens(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "summarize_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=2000, chunk_overlap=500\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = summarize_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(split_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5574/723517548.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7, streaming=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import hub\n",
    "summary_prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7, streaming=True)\n",
    "summary_chain = create_stuff_documents_chain(llm, summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "sumaries = []\n",
    "for split_doc in split_docs:\n",
    "    print(type(split_doc.page_content))\n",
    "    partial_summary = summary_chain.invoke({\"context\": [split_doc]})\n",
    "    sumaries.append(partial_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_summary = Document(page_content= \"\\n\".join(sumaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_RESULT = summary_chain.invoke(\n",
    "                {\"context\": partial_summaries_doc}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SUMMARY_RESULT.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_qa_with_sources_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pytubefix import YouTube\n",
    "import asyncio\n",
    "import torch\n",
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # .env 파일에서 환경 변수 로드\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(url):\n",
    "    yt = YouTube(url)\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    return {\n",
    "        \"title\": yt.title,\n",
    "        \"audio_url\": audio_stream.url if audio_stream else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Title: 가장 쉬운 까르보나라\n"
     ]
    }
   ],
   "source": [
    "video_url = \"https://www.youtube.com/shorts/a--NSC19MXM\"\n",
    "video_info = get_video_info(video_url)\n",
    "print(f\"Video Title: {video_info['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = WhisperModel(\"large-v3\", device=device, compute_type=compute_type)\n",
    "\n",
    "def transcribe_audio(audio_url):\n",
    "    segments, info = whisper_model.transcribe(audio_url)\n",
    "    transcript = [{\"text\": segment.text, \"start\": segment.start, \"end\": segment.end} for segment in segments]\n",
    "    return {\"script\": transcript, \"language\": info.language}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "transcribe.py       :324  2024-10-12 11:30:14,287 Processing audio with duration 00:59.118\n",
      "transcribe.py       :425  2024-10-12 11:30:19,512 Detected language 'ko' with probability 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript Language: ko\n",
      "First few lines of transcript: [{'text': ' 한 남자가 베이컨을 가져오는데요', 'start': 0.0, 'end': 10.46}, {'text': ' 갑자기 계란을 가져와 깨부수기 시작합니다', 'start': 10.46, 'end': 12.74}, {'text': ' 노른자를 건져내 따로 담아줍니다', 'start': 12.74, 'end': 14.74}]\n"
     ]
    }
   ],
   "source": [
    "transcript = transcribe_audio(video_info['audio_url'])\n",
    "print(f\"Transcript Language: {transcript['language']}\")\n",
    "print(f\"First few lines of transcript: {transcript['script'][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/0f7nfvt16ln8630csjtkk_1w0000gn/T/ipykernel_41602/2953908480.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "summary_prompt = hub.pull(\"teddynote/summary-stuff-documents-korean\")\n",
    "llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            temperature=0.7,\n",
    "            streaming=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader, TextLoader\n",
    "docs = TextLoader(\"script.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'script.txt'}, page_content=\"In the last chapter, you and I started to step through the internal workings of a transformer.\\nThis is one of the key pieces of technology inside large language models, and a lot of\\nother tools in the modern wave of AI.\\nIt first hit the scene in a now-famous 2017 paper called Attention is All You Need, and\\nin this chapter, you and I will dig into what this attention mechanism is, visualizing how\\nit processes data.\\nAs a quick recap, here's the important context I want you to have in mind.\\nThe goal of the model that you and I are studying is to take in a piece of text and predict\\nwhat word comes next.\\nThe input text is broken up into little pieces that we call tokens, and these are very often\\nwords or pieces of words, but just to make the examples in this video easier for you\\nand me to think about, let's simplify by pretending that tokens are always just words.\\nThe first step in a transformer is to associate each token with a high-dimensional vector,\\nwhat we call its embedding.\\nNow the most important idea I want you to have in mind is how directions in this high-dimensional\\nspace of all possible embeddings can correspond with semantic meaning.\\nIn the last chapter we saw an example for how direction can correspond to gender, in\\nthe sense that adding a certain step in this space can take you from the embedding of a\\nmasculine noun to the embedding of the corresponding feminine noun.\\njust one example, you could imagine how many other directions in this high-dimensional space\\ncould correspond to numerous other aspects of a word's meaning. The aim of a transformer is to\\nprogressively adjust these embeddings so that they don't merely encode an individual word,\\nbut instead they bake in some much, much richer contextual meaning. I should say up front that a\\nlot of people find the attention mechanism, this key piece in a transformer, very confusing, so\\ndon't worry if it takes some time for things to sink in. I think that before we dive into the\\ncomputational details and all the matrix multiplications, it's worth thinking about a\\ncouple examples for the kind of behavior that we want attention to enable. Consider the phrases\\nAmerican true mole, one mole of carbon dioxide, and take a biopsy of the mole. You and I know\\nthat the word mole has different meanings in each one of these, based on the context.\\nBut after the first step of a transformer, the one that breaks up the text and associates each\\ntoken with a vector, the vector that's associated with mole would be the same in all three of these\\ncases, because this initial token embedding is effectively a lookup table with no reference to\\nthe context. It's only in the next step of the transformer that the surrounding embeddings have\\nthe chance to pass information into this one. The picture you might have in mind is that there\\nare multiple distinct directions in this embedding space encoding the multiple distinct meanings of\\nthe word mole, and that a well-trained attention block calculates what you need to add to the\\ngeneric embedding to move it to one of these more specific directions, as a function of the context.\\nTo take another example, consider the embedding of the word tower. This is presumably some very\\ngeneric, non-specific direction in the space, associated with lots of other large, tall nouns.\\nIf this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update\\nthis vector so that it points in a direction that more specifically encodes the Eiffel Tower,\\nmaybe correlated with vectors associated with Paris and France and things made of steel.\\nIf it was also preceded by the word miniature, then the vector should be updated even further\\nso that it no longer correlates with large tall things. More generally than just refining the\\nmeaning of a word, the attention block allows the model to move information encoded in one\\nembedding to that of another, potentially ones that are quite far away, and potentially\\nwith information that's much richer than just a single word.\\nWhat we saw in the last chapter was how after all of the vectors flow through the network,\\nincluding many different attention blocks, the computation that you perform to produce\\na prediction of the next token is entirely a function of the last vector in the sequence.\\nSo imagine, for example, that the text you input is most of an entire mystery novel,\\nway up to a point near the end which reads, therefore the murderer was, if the model is\\ngoing to accurately predict the next word, that final vector in the sequence which began its life\\nsimply embedding the word was will have to have been updated by all of the attention blocks\\nto represent much much more than any individual word, somehow encoding all of the information\\nfrom the full context window that's relevant to predicting the next word. To step through the\\nthe computations though let's take a much simpler example. Imagine that the input includes the\\nphrase a fluffy blue creature roamed the verdant forest and for the moment suppose that the only\\ntype of update that we care about is having the adjectives adjust the meanings of their\\ncorresponding nouns. What I'm about to describe is what we would call a single head of attention\\nand later we will see how the attention block consists of many different heads run in parallel.\\nAgain, the initial embedding for each word is some high-dimensional vector\\nthat only encodes the meaning of that particular word with no context.\\nActually, that's not quite true. They also encode the position of the word.\\nThere's a lot more to say about the specific way that positions are encoded,\\nbut right now all you need to know is that the entries of this vector are enough to tell you\\nboth what the word is and where it exists in the context. Let's go ahead and denote these\\nembeddings with the letter E, the goal is to have a series of computations produce a\\nnew refined set of embeddings where, for example, those corresponding to the nouns have ingested\\nthe meaning from their corresponding adjectives.\\nAnd playing the deep learning game, we want most of the computations involved to look\\nlike matrix-vector products where the matrices are full of tunable weights, things that the\\nmodel will learn based on data.\\nTo be clear, I'm making up this example of adjectives updating nouns just to illustrate\\nthe type of behavior that you could imagine an intention had doing.\\nAs with so much deep learning, the true behavior is much harder to parse, because it's based\\non tweaking and tuning a huge number of parameters to minimize some cost function.\\nIt's just that as we step through all of the different matrices filled with parameters\\nthat are involved in this process, I think it's really helpful to have an imagined example\\nof something that it could be doing to help keep it all more concrete.\\nFor the first step of this process, you might imagine each noun, like creature, asking the\\nquestion, hey, are there any adjectives sitting in front of me, and for the words fluffy and\\nblue to each be able to answer, yeah, I'm an adjective and I'm in that position.\\nThat question is somehow encoded as yet another vector, another list of numbers, which we\\ncall the query for this word.\\nThis query vector, though, has a much smaller dimension than the embedding vector, say 128.\\nComputing this query looks like taking a certain matrix, which I'll label wq, and multiplying\\nit by the embedding.\\nCompressing things a bit, let's write that query vector as q, and then anytime you see\\nme put a matrix next to an arrow like this one, it's meant to represent that multiplying\\nthis matrix by the vector at the arrow's start gives you the vector at the arrow's end.\\nIn this case, you multiply this matrix by all of the embeddings in the context, producing\\none query vector for each token.\\nThe entries of this matrix are parameters of the model, which means the true behavior\\nis learned from data, and in practice what this matrix does in a particular attention\\nhead is challenging to parse.\\nBut for our sake, imagining an example that we might hope it would learn, we'll suppose\\nthat this query matrix maps the embeddings of nouns to certain directions in this smaller\\nquery space that somehow encodes the notion of looking for adjectives in preceding positions.\\nAs to what it does to other embeddings, who knows, maybe it simultaneously tries to accomplish\\nsome other goal with those, right now we're laser focused on the nouns.\\nAt the same time, associated with this is a second matrix called the key matrix, which\\nyou also multiply by every one of the embeddings.\\nThis produces a second sequence of vectors that we call the keys.\\nConceptually you want to think of the keys as potentially answering the queries.\\nThis key matrix is also full of tunable parameters, and just like the query matrix it maps the\\nembedding vectors to that same smaller dimensional space.\\nYou think of the keys as matching the queries whenever they closely align with each other.\\nIn our example, you would imagine that the key matrix maps the adjectives, like fluffy\\nand blue, to vectors that are closely aligned with the query produced by the word creature.\\nTo measure how well each key matches each query, you compute a dot product between each\\npossible key-query pair.\\nI like to visualize a grid full of a bunch of dots, where the bigger dots correspond\\nthe larger dot products, the places where the keys and queries align. For our adjective-noun example,\\nthat would look a little more like this, where if the keys produced by fluffy and blue really do\\nalign closely with the query produced by creature, then the dot products in these two spots would be\\nsome large positive numbers. In the lingo, machine learning people would say that this means the\\nembeddings of fluffy and blue attend to the embedding of creature. By contrast to the dot\\nproduct between the key for some other word like the and the query for creature would be some small\\nor negative value that reflects that these are unrelated to each other. So we have this grid of\\nvalues that can be any real number from negative infinity to infinity giving us a score for how\\nrelevant each word is to updating the meaning of every other word. The way we're about to use these\\nscores is to take a certain weighted sum along each column weighted by the relevance. So instead\\nInstead of having values range from negative infinity to infinity, what we want is for\\nthe numbers in these columns to be between 0 and 1, and for each column to add up to\\n1, as if they were a probability distribution.\\nIf you're coming in from the last chapter, you know what we need to do then.\\nWe compute a softmax along each one of these columns to normalize the values.\\nIn our picture, after you apply softmax to all of the columns, we'll fill in the grid\\nwith these normalized values.\\nAt this point, you're safe to think about each column as giving weights\\naccording to how relevant the word on the left is to the corresponding value at the top.\\nWe call this grid an attention pattern.\\nNow, if you look at the original Transformer paper,\\nthere's a really compact way that they write this all down.\\nHere, the variables q and k represent the full arrays of query and key vectors respectively,\\nthose little vectors you get by multiplying the embeddings by the query and the key matrices.\\nThis expression up in the numerator is a really compact way to represent the grid of all possible\\ndot products between pairs of keys and queries. A small technical detail that I didn't mention\\nis that for numerical stability it happens to be helpful to divide all of these values by the\\nsquare root of the dimension in that key query space. Then this softmax that's wrapped around\\nthe full expression, is meant to be understood to apply column by column.\\nAs to that V term, we'll talk about it in just a second.\\nBefore that, there's one other technical detail that so far I've skipped.\\nDuring the training process, when you run this model on a given text example, and all\\nof the weights are slightly adjusted and tuned to either reward or punish it based on how\\nhigh a probability it assigns to the true next word in the passage, it turns out to\\nmake the whole training process a lot more efficient if you simultaneously have it predict\\nevery possible next token following each initial sub-sequence of tokens in this passage.\\nFor example, with the phrase that we've been focusing on, it might also be predicting what\\nwords follow creature, and what words follow the.\\nThis is really nice, because it means what would otherwise be a single training example\\neffectively acts as many.\\nFor the purposes of our attention pattern, it means that you never want to allow later\\nwords to influence earlier words, since otherwise they could kind of give away the answer for\\nwhat comes next. What this means is that we want all of these spots here, the ones representing\\nlater tokens influencing earlier ones, to somehow be forced to be zero. The simplest thing you might\\nthink to do is to set them equal to zero, but if you did that the columns wouldn't add up to one\\nanymore, they wouldn't be normalized. So instead a common way to do this is that before applying\\nsoftmax you set all of those entries to be negative infinity. If you do that then after\\nAfter applying softmax, all of those get turned into zero, but the columns stay normalized.\\nThis process is called masking.\\nThere are versions of attention where you don't apply it, but in our GPT example, even\\nthough this is more relevant during the training phase than it would be, say, running it as\\na chatbot or something like that, you do always apply this masking to prevent later tokens\\nfrom influencing earlier ones.\\nAnother fact that's worth reflecting on about this attention pattern is how its size is\\nequal to the square of the context size.\\nSo this is why context size can be a really huge bottleneck for large language models,\\nand scaling it up is non-trivial.\\nAs you might imagine, motivated by a desire for bigger and bigger context windows, recent\\nyears have seen some variations to the attention mechanism aimed at making context more scalable.\\nBut right here, you and I are staying focused on the basics.\\nOkay, great, computing this pattern lets the model deduce which words are relevant to which\\nother words.\\nNow you need to actually update the embeddings, allowing words to pass information to whichever\\nother words they're relevant to.\\nFor example, you want the embedding of fluffy to somehow cause a change to creature that\\nmoves it to a different part of this 12,000 dimensional embedding space that more specifically\\nencodes a fluffy creature.\\nWhat I'm going to do here is first show you the most straightforward way that you could\\ndo this, though there's a slight way that this gets modified in the context of multi-headed\\nattention.\\nThis most straightforward way would be to use a third matrix, what we call the value\\nmatrix, which you multiply by the embedding of that first word, for example fluffy.\\nThe result of this is what you would call a value vector, and this is something that\\nyou add to the embedding of the second word, in this case something you add to the embedding\\nof creature.\\nSo, this value vector lives in the same very high dimensional space as the embeddings.\\nWhen you multiply this value matrix by the embedding of a word, you might think of it\\nas saying if this word is relevant to adjusting the meaning of something else, what exactly should\\nbe added to the embedding of that something else in order to reflect this? Looking back in our\\ndiagram, let's set aside all of the keys and the queries, since after you compute the attention\\npattern you're done with those, then you're going to take this value matrix and multiply it by every\\none of those embeddings to produce a sequence of value vectors. You might think of these value\\nvectors as being kind of associated with the corresponding keys.\\nFor each column in this diagram, you multiply each of the value vectors by the corresponding\\nweight in that column.\\nFor example, here, under the embedding of creature, you would be adding large proportions\\nof the value vectors for fluffy and blue, while all of the other value vectors get zeroed\\nout, or at least nearly zeroed out.\\nAnd then finally, the way to actually update the embedding associated with this column,\\npreviously encoding some context-free meaning of creature, you add together all of these\\nrescaled values in the column, producing a change that you want to add that I'll label\\ndelta E, and then you add that to the original embedding.\\nHopefully what results is a more refined vector encoding the more contextually rich meaning,\\nlike that of a fluffy blue creature.\\nAnd of course you don't just do this to one embedding, you apply the same weighted sum\\nacross all of the columns in this picture, producing a sequence of changes.\\nAdding all of those changes to the corresponding embeddings produces a full sequence of more\\nrefined embeddings popping out of the attention block.\\nZooming out, this whole process is what you would describe as a single head of attention.\\nAs I've described things so far, this process is parameterized by three distinct matrices,\\nall filled with tunable parameters, the key, the query, and the value.\\nI want to take a moment to continue what we started in the last chapter with the scorekeeping\\nwhere we count up the total number of model parameters using the numbers from GPT-3.\\nThese key and query matrices each have 12,288 columns, matching the embedding dimension,\\nand 128 rows, matching the dimension of that smaller key query space.\\nThis gives us an additional 1.5 million or so parameters for each one.\\nIf you look at that value matrix by contrast, the way I've described things so far would\\nsuggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both\\nits inputs and its outputs live in this very large embedding space.\\nIf true, that would mean about 150 million added parameters.\\nAnd to be clear, you could do that, you could devote orders of magnitude more parameters\\nto the value map than to the key and query.\\nBut in practice, it is much more efficient if instead you make it so that the number\\nof parameters devoted to this value map is the same as the number devoted to the key\\nin the query.\\nThis is especially relevant in the setting of running multiple attention heads in parallel.\\nThe way this looks is that the value map is factored as a product of two smaller matrices.\\nConceptually, I would still encourage you to think about the overall linear map, one\\nwith inputs and outputs both in this larger embedding space, for example taking the embedding\\nof blue to this blueness direction that you would add to nouns.\\nIt's just that it's broken up into two separate steps.\\nThe first matrix on the right here has a smaller number of rows, typically the same size as\\nthe key query space.\\nWhat this means is you can think of it as mapping the large embedding vectors down to\\na much smaller space.\\nThis is not the conventional naming, but I'm going to call this the value down matrix.\\nThe second matrix maps from this smaller space back up to the embedding space, producing\\nthe vectors that you use to make the actual updates.\\nI'm going to call this one the value-up matrix, which, again, is not conventional.\\nThe way that you would see this written in most papers looks a little different.\\nI'll talk about it in a minute.\\nIn my opinion, it tends to make things a little more conceptually confusing.\\nTo throw in linear algebra jargon here, what we're basically doing is constraining the\\noverall value map to be a low-rank transformation.\\nTurning back to the parameter count, all four of these matrices have the same size, and\\nThen adding them all up, we get about 6.3 million parameters for one attention head.\\nAs a quick side note, to be a little more accurate, everything described so far is what\\npeople would call a self-attention head, to distinguish it from a variation that comes\\nup in other models that's called cross-attention.\\nThis isn't relevant to our GPT example, but if you're curious, cross-attention involves\\nmodels that process two distinct types of data, like text in one language and text in\\nanother language that's part of an ongoing generation of a translation.\\nOr maybe audio input of speech, and an ongoing transcription.\\nA cross-attention head looks almost identical.\\nThe only difference is that the key and query maps act on different datasets.\\nIn a model doing translation, for example, the keys might come from one language, while\\nthe queries come from another, and the attention pattern could describe which words from one\\nlanguage correspond to which words in another.\\nAnd in this setting there would typically be no masking, since there's not really any\\nnotion of later tokens affecting earlier ones.\\nStaying focused on self-attention though, if you understood everything so far, and if\\nyou were to stop here, you would come away with the essence of what attention really\\nis.\\nAll that's really left to us is to lay out the sense in which you do this many, many\\ndifferent times.\\nIn our central example we focused on adjectives updating nouns, but of course there are lots\\nof different ways that context can influence the meaning of a word.\\nIf the words they crashed the preceded the word car, it has implications for the shape\\nand the structure of that car, and a lot of associations might be less grammatical.\\nIf the word wizard is anywhere in the same passage as Harry, it suggests that this might\\nbe referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were\\nin that passage, then perhaps the embedding of Harry should instead be updated to refer\\nto the prince.\\nFor every different type of contextual updating that you might imagine, the parameters of\\nthese key and query matrices would be different to capture the different attention patterns,\\nand the parameters of our value map would be different based on what should be added to the\\nembeddings. And again, in practice the true behavior of these maps is much more difficult\\nto interpret, where the weights are set to do whatever the model needs them to do to best\\naccomplish its goal of predicting the next token. As I said before, everything we described is a\\nsingle head of attention, and a full attention block inside a transformer consists of what's\\ncalled multi-headed attention where you run a lot of these operations in parallel each with its own\\ndistinct key query and value maps. GPT-3 for example uses 96 attention heads inside each block.\\nConsidering that each one is already a bit confusing it's certainly a lot to hold in your\\nhead. Just to spell it all out very explicitly this means you have 96 distinct key and query\\nmatrices producing 96 distinct attention patterns. Then each head has its own distinct value matrices\\nused to produce 96 sequences of value vectors. These are all added together using the\\ncorresponding attention patterns as weights. What this means is that for each position in the\\ncontext, each token, every one of these heads produces a proposed change to be added to the\\nembedding in that position. So what you do is you sum together all of those proposed changes,\\none for each head, and you add the result to the original embedding of that position.\\nThis entire sum here would be one slice of what's outputted from this multi-headed attention block,\\na single one of those refined embeddings that pops out the other end of it.\\nAgain, this is a lot to think about, so don't worry at all if it takes some time to sink in.\\nThe overall idea is that by running many distinct heads in parallel,\\nyou're giving the model the capacity to learn many distinct ways that context changes meaning.\\nPulling up our running tally for parameter count with 96 heads, each including its own variation\\nof these four matrices, each block of multi-headed attention ends up with around 600 million\\nparameters. There's one added slightly annoying thing that I should really mention for any of you\\nwho go on to read more about transformers. You remember how I said that the value map is factored\\nout into these two distinct matrices, which I labeled as the value down and the value up\\nmatrices. The way that I framed things would suggest that you see this pair of matrices\\ninside each attention head, and you could absolutely implement it this way. That would\\nbe a valid design. But the way that you see this written in papers and the way that it's\\nimplemented in practice looks a little different. All of these value up matrices for each head\\nappear stapled together in one giant matrix that we call the output matrix, associated with\\nthe entire multi-headed attention block. And when you see people refer to the value matrix for a\\ngiven attention head, they're typically only referring to this first step, the one that I\\nwas labeling as the value down projection into the smaller space. For the curious among you,\\nI've left an on-screen note about it. It's one of those details that runs the risk of distracting\\nfrom the main conceptual points, but I do want to call it out just so that you know if you read\\nabout this in other sources. Setting aside all the technical nuances, in the preview from the\\nlast chapter, we saw how data flowing through a transformer doesn't just flow through a single\\nattention block. For one thing, it also goes through these other operations called multi-layer\\nperceptrons. We'll talk more about those in the next chapter. And then it repeatedly goes through\\nmany, many copies of both of these operations. What this means is that after a given word imbibes\\nsome of its context, there are many more chances for this more nuanced embedding to be influenced\\nby its more nuanced surroundings. The further down the network you go, with each embedding\\ntaking in more and more meaning from all the other embeddings, which themselves are getting\\nmore and more nuanced, the hope is that there's the capacity to encode higher level and more\\nabstract ideas about a given input beyond just descriptors and grammatical structure.\\nThings like sentiment and tone and whether it's a poem and what underlying scientific truths are\\nare relevant to the piece, and things like that.\\nTurning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the\\ntotal number of key, query, and value parameters is multiplied by another 96, which brings\\nthe total sum to just under 58 billion distinct parameters devoted to all of the attention\\nheads.\\nThat is a lot, to be sure, but it's only about a third of the 175 billion that are\\nin the network in total.\\nSo even though attention gets all of the attention, the majority of parameters come from the blocks\\nsitting in between these steps.\\nIn the next chapter, you and I will talk more about those other blocks and also a lot more\\nabout the training process.\\nA big part of the story for the success of the attention mechanism is not so much any\\nspecific kind of behavior that it enables, but the fact that it's extremely parallelizable,\\nmeaning that you can run a huge number of computations in a short time using GPUs.\\nthat one of the big lessons about deep learning in the last decade or two has been that scale\\nalone seems to give huge qualitative improvements in model performance. There's a huge advantage to\\nparallelizable architectures that let you do this. If you want to learn more about this stuff, I've\\nleft lots of links in the description. In particular, anything produced by Andre Karpathy or Chris Ola\\ntend to be pure gold. In this video, I wanted to just jump into attention in its current form,\\nbut if you're curious about more of the history for how we got here and how you might reinvent\\nthis idea for yourself, my friend Vivek just put up a couple videos giving a lot more of\\nthat motivation. Also, Britt Cruz from the channel The Art of the Problem\\nhas a really nice video about the history of large language models.\\nyou\\n\")]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "document = [Document(page_content=\"\\n\".join([t[\"text\"] for t in transcript[\"script\"]]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_client.py          :1786 2024-10-12 11:31:22,301 HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "summary_chain = create_stuff_documents_chain(llm,summary_prompt)\n",
    "result = await summary_chain.ainvoke({\"context\": document})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- 🥓 한 남자가 베이컨을 가져옴.  \\n- 🥚 계란을 깨고 노른자를 따로 담음.  \\n- 🥄 흰자는 생으로 먹고, 소금을 넣음.  \\n- 🌳 나무젓가락을 사용함.  \\n- 🍝 파스타를 넣고 재료를 섞음.  \\n- 🧂 후추와 그라라빠다노를 뿌림.  \\n- 🔥 베이컨을 구워 계란치즈 소스를 섞음.  \\n- 🍽️ 까르보나라를 예쁘게 담아 완성함.  \\n- 🤤 저도 한번 꼭 먹어보고 싶네요.  '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c4/0f7nfvt16ln8630csjtkk_1w0000gn/T/ipykernel_41602/2497576997.py:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- 🥓 한 남자가 베이컨을 가져옴.  ',\n",
       " '- 🥚 계란을 깨고 노른자를 따로 담음.  ',\n",
       " '- 🥄 흰자는 생으로 먹고, 소금을 넣음.  ',\n",
       " '- 🌳 나무젓가락을 사용함.  ',\n",
       " '- 🍝 파스타를 넣고 재료를 섞음.  ',\n",
       " '- 🧂 후추와 그라라빠다노를 뿌림.  ',\n",
       " '- 🔥 베이컨을 구워 계란치즈 소스를 섞음.  ',\n",
       " '- 🍽️ 까르보나라를 예쁘게 담아 완성함.  ',\n",
       " '- 🤤 저도 한번 꼭 먹어보고 싶네요.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m documents \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39mcreate_documents([t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transcript[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m----> 3\u001b[0m     doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43msummary\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary' is not defined"
     ]
    }
   ],
   "source": [
    "documents = text_splitter.create_documents([t[\"text\"] for t in transcript[\"script\"]])\n",
    "for doc in documents:\n",
    "    doc.page_content += \"\\n\" + summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "you_url = \"https://youtube.com/shorts/a--NSC19MXM?si=yiun-HK_7wX1sNvL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'id': 'sync-26147017-ff6e-408d-9622-80c484868c42-e1', 'status': 'IN_QUEUE'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = \"https://api.runpod.ai/v2/uq96boxkzy99ev/runsync\"\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터 (내부적으로 사용할 파라미터 설정)\n",
    "body = {\"input\":{\n",
    "    \"api\":{\n",
    "        \"method\":\"POST\",\n",
    "        \"endpoint\":\"/ping\",\n",
    "    },\n",
    "    \"payload\":{},\n",
    "}}\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=body, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    print(\"Response:\", response.json())\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: IN_QUEUE\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 작업 ID (작업 완료된 job ID)\n",
    "job_id = response.json()['id']\n",
    "\n",
    "# RunPod API STATUS 엔드포인트 URL\n",
    "status_url = f\"https://api.runpod.ai/v2/wm1xrz07all039/status/{job_id}\"\n",
    "\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# 작업 상태 및 결과 확인 요청 보내기\n",
    "response = requests.get(status_url, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    job_result = response.json()\n",
    "    if job_result.get(\"status\") == \"COMPLETED\":\n",
    "        print(\"Job Completed! Result:\", job_result.get(\"output\"))\n",
    "    else:\n",
    "        print(f\"Job Status: {job_result.get('status')}\")\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'delayTime': 6071, 'executionTime': 3825, 'id': 'sync-c9bea7a9-08ea-447d-bd62-e481515985b4-e1', 'output': {'hashtags': '#파뿌리 #생일 #친구 #예능 #노랭이', 'title': '수제 김밥 30줄로 생일 파티합니다!! 역대급 선물 언박싱까지!!!!!!!'}, 'status': 'COMPLETED', 'workerId': 'vto3bvdf9z7v0a'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "you_url = \"https://youtu.be/omEk2BNDt1I?si=xjtbYANtlux5CTfB\"\n",
    "\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/get_title_hash\",\n",
    "        \"method\": \"GET\",\n",
    "        # \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"url\": you_url},\n",
    "    }\n",
    "}\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    print(\"Response:\", response.json())\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "# api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "# endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# # RunPod RUNSYNC 엔드포인트 URL\n",
    "# url = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "\n",
    "# # FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "# payload = {\n",
    "#     \"input\": {\n",
    "#         \"endpoint\": \"/get_script_summary\",\n",
    "#         \"method\": \"GET\",\n",
    "#         \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "#         \"params\": {\"url\": you_url},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # 요청 헤더에 API 키 추가\n",
    "# headers = {\n",
    "#     \"Authorization\": f\"Bearer {api_key}\",\n",
    "#     \"Content-Type\": \"application/json\"\n",
    "# }\n",
    "\n",
    "# # RUNSYNC 요청 보내기\n",
    "# response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# # 응답 확인\n",
    "# if response.status_code == 200:\n",
    "#     print(\"Response:\", response.json())\n",
    "# else:\n",
    "#     print(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Response: {'id': 'c2cb7372-07b6-4146-b2a4-7c8ad4e0eb34-e1', 'status': 'IN_QUEUE'}\n",
      "Current status: IN_QUEUE\n",
      "Current status: IN_QUEUE\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: IN_PROGRESS\n",
      "Current status: COMPLETED\n",
      "결과값:{'delayTime': 6033, 'executionTime': 191359, 'id': 'c2cb7372-07b6-4146-b2a4-7c8ad4e0eb34-e1', 'output': {'language': 'ko', 'script': [{'end': 1.92, 'start': 0, 'text': ' 오늘 우리 노랭이 생일입니다!'}, {'end': 8.32, 'start': 4.38, 'text': ' 팝콘이가 멤버들 생일을 맞은 소원을 좀 들어주고 있어요.'}, {'end': 10.08, 'start': 8.32, 'text': ' 오늘 노랭이의 소원은 바로'}, {'end': 12.36, 'start': 10.08, 'text': ' 노랭이가 가장 좋아하는 음식 뭐죠?'}, {'end': 13.8, 'start': 12.36, 'text': ' 김밥 파티입니다.'}, {'end': 16.06, 'start': 13.8, 'text': ' 우와 김밥 맛있겠다.'}, {'end': 17.36, 'start': 16.06, 'text': ' 김밥이 약간 좀 생소하지 않나?'}, {'end': 21.04, 'start': 17.36, 'text': ' 노랭이가 김밥을 좋아한다는 것을 모르는 분들도 좀 있을 텐데'}, {'end': 23.76, 'start': 21.04, 'text': ' 사실 저희 어머니는 아십니다.'}, {'end': 25.72, 'start': 23.76, 'text': ' 제가 김밥을 좋아하기 때문에'}, {'end': 28.04, 'start': 25.72, 'text': ' 기절 때 김밥 해달라고 제가 조르거든요.'}, {'end': 31.4, 'start': 28.04, 'text': ' 명절에 먹을 정도면 이건 인정입니까?'}, {'end': 33.12, 'start': 31.4, 'text': ' 솔직히 사먹는 김밥 중에'}, {'end': 34.96, 'start': 33.12, 'text': ' 우리 엄마 김밥이 없더라고.'}, {'end': 39.5, 'start': 35.98, 'text': ' 그래서 오늘 우리가 노랭이 생일 기념으로'}, {'end': 41.72, 'start': 39.5, 'text': ' 김밥을 한번 만들어보려고 이렇게 재료들을'}, {'end': 43.76, 'start': 41.72, 'text': ' 야오미!'}, {'end': 47.72, 'start': 43.76, 'text': ' 과연 이 재료로 노랭이의 향수를 저희가 느끼게 해줄 수 있을지'}, {'end': 49.98, 'start': 47.72, 'text': ' 어머니만큼은 안 돼.'}, {'end': 53.24, 'start': 49.98, 'text': ' 힘들겠지만 아버지만큼은 할 수 있지.'}, {'end': 56.72, 'start': 53.24, 'text': ' 우리 다 같이 이렇게 김밥 오순노소 만들면서 재밌게 놀아보자고.'}, {'end': 57.64, 'start': 56.72, 'text': ' 좋습니다.'}, {'end': 59.4, 'start': 57.64, 'text': ' 결국엔 김밥 맛이 중요한 게 아니야.'}, {'end': 60.52, 'start': 59.4, 'text': ' 오순도순이 중요한 거야.'}, {'end': 61.64, 'start': 60.52, 'text': ' 아니야. 근데 맛도 중요하잖아.'}, {'end': 62.88, 'start': 61.64, 'text': ' 아 맞나.'}, {'end': 63.88, 'start': 62.88, 'text': ' 맛있게 먹어야지.'}, {'end': 65.32, 'start': 63.88, 'text': ' 아니야.'}, {'end': 66.14, 'start': 65.32, 'text': ' 아니야. 아니야.'}, {'end': 66.72, 'start': 66.14, 'text': ' 아니야.'}, {'end': 68.82, 'start': 66.72, 'text': ' 거기서 준비되면 안 돼.'}, {'end': 71.52, 'start': 68.82, 'text': ' 무슨 말만 하면 요즘 딱 후산이 뭐.'}, {'end': 72.76, 'start': 71.52, 'text': ' 오늘의 룰 있습니까?'}, {'end': 73.52, 'start': 72.76, 'text': ' 생일 룰.'}, {'end': 74.88, 'start': 73.52, 'text': ' 아 생일 룰?'}, {'end': 75.72, 'start': 74.88, 'text': ' 욕 금지.'}, {'end': 76.32, 'start': 75.72, 'text': ' 아 욕.'}, {'end': 77.16, 'start': 76.32, 'text': ' 아 욕 금지.'}, {'end': 78.72, 'start': 77.16, 'text': ' 저번에 말했던 대로 한번'}, {'end': 80.1, 'start': 78.72, 'text': ' 외래어 금지 한번 해볼까요?'}, {'end': 81.2, 'start': 80.1, 'text': ' 아 진짜로 외래어를 금지.'}, {'end': 82.4, 'start': 81.2, 'text': ' 외래어 금지.'}, {'end': 83.8, 'start': 82.4, 'text': ' 막 이렇게 쓰면?'}, {'end': 85.24, 'start': 83.8, 'text': ' 쓰면 짜오가'}, {'end': 85.6, 'start': 85.24, 'text': ' 어머.'}, {'end': 86.6, 'start': 85.6, 'text': ' 움직여줘.'}, {'end': 87.56, 'start': 86.6, 'text': ' 아 그러면 자기가 쓰면'}, {'end': 88.8, 'start': 87.56, 'text': ' 자기가 자기를 응징하나?'}, {'end': 90.96, 'start': 89.8, 'text': ' 아 저는 깍두기.'}, {'end': 92, 'start': 90.96, 'text': ' 네가 왜.'}, {'end': 93.7, 'start': 92, 'text': ' 저는 진행을 해야 되니깐요.'}, {'end': 97.14, 'start': 95.7, 'text': ' 약간 이 둘이가 불리해.'}, {'end': 98.04, 'start': 97.14, 'text': ' 영문학과.'}, {'end': 100.08, 'start': 98.04, 'text': ' 또 외고 출신.'}, {'end': 101.38, 'start': 100.08, 'text': ' 큰일 났네.'}, {'end': 104.84, 'start': 101.38, 'text': ' 외래어가 금지된 노잉이의 김밥 생일 잔치.'}, {'end': 106.48, 'start': 104.84, 'text': ' 시작해 보겠습니다.'}, {'end': 107.34, 'start': 106.48, 'text': ' 쉽지 않네.'}, {'end': 108.78, 'start': 107.34, 'text': ' 발 없나? 발.'}, {'end': 109.34, 'start': 108.78, 'text': ' 발?'}, {'end': 109.68, 'start': 109.34, 'text': ' 어.'}, {'end': 111.08, 'start': 109.68, 'text': ' 발.'}, {'end': 114.76, 'start': 113.78, 'text': ' 어 욕 금지.'}, {'end': 115.68, 'start': 114.76, 'text': ' 욕 금지.'}, {'end': 116.42, 'start': 115.68, 'text': ' 욕하면 안 돼.'}, {'end': 117.56, 'start': 116.42, 'text': ' 표정으로 욕하고'}, {'end': 118.32, 'start': 117.56, 'text': ' 알아서 욕 금지.'}, {'end': 119.9, 'start': 118.32, 'text': ' 발이 시라이.'}, {'end': 120.86, 'start': 119.9, 'text': ' 이거 뭐야?'}, {'end': 121.96, 'start': 120.86, 'text': ' 이거 누구 거야?'}, {'end': 122.9, 'start': 121.96, 'text': ' 삼겹살이네.'}, {'end': 124.7, 'start': 122.9, 'text': ' 아 요즘 삼겹살에요.'}, {'end': 125.6, 'start': 124.7, 'text': ' 깻잎 넣어가지고'}, {'end': 126.5, 'start': 125.6, 'text': ' 볼도 말고.'}, {'end': 128.04, 'start': 126.5, 'text': ' 이거 형이 만든 거야.'}, {'end': 128.74, 'start': 128.04, 'text': ' 너가 써도 돼.'}, {'end': 130.08, 'start': 128.74, 'text': ' 나는 진행을 해야 되기 때문에.'}, {'end': 131.54, 'start': 130.08, 'text': ' 싫어.'}, {'end': 132.58, 'start': 131.54, 'text': ' 작정하고 막.'}, {'end': 133.68, 'start': 132.58, 'text': ' 작정하고 골팡 나고.'}, {'end': 134.92, 'start': 133.68, 'text': ' 아니.'}, {'end': 136.48, 'start': 134.92, 'text': ' 야 우리 밥 몇 개나 쓰려나.'}, {'end': 137.98, 'start': 136.48, 'text': ' 역시 안박서 12개인데.'}, {'end': 138.58, 'start': 137.98, 'text': ' 어? 어?'}, {'end': 140.48, 'start': 138.58, 'text': ' 당신 그 말 조심해야 됩니다.'}, {'end': 141.52, 'start': 140.48, 'text': ' 상자.'}, {'end': 143.66, 'start': 141.52, 'text': ' 자 여러분들 이제 진짜 시작합니다.'}, {'end': 145.16, 'start': 143.66, 'text': ' 생일자가 서른 원입니다. 서른 원.'}, {'end': 146.68, 'start': 145.16, 'text': ' 외래어 금지 시작이라고.'}, {'end': 147.4, 'start': 146.68, 'text': ' 외래어 금지.'}, {'end': 148.8, 'start': 147.56, 'text': ' 쌈도.'}, {'end': 150.36, 'start': 148.8, 'text': ' 오케이 마지막입니다.'}, {'end': 151.36, 'start': 150.36, 'text': ' 시작입니다.'}, {'end': 153.8, 'start': 151.36, 'text': ' 자 여기 수많은 재료들이 있는데 짱배는?'}, {'end': 155.76, 'start': 153.8, 'text': ' 아 저 같은 경우에는 바로 이'}, {'end': 157.4, 'start': 155.76, 'text': ' 새우를 이용할 겁니다.'}, {'end': 157.9, 'start': 157.4, 'text': ' 새우?'}, {'end': 159.34, 'start': 157.9, 'text': ' 이 노래가 새우 좋아하는 거 아니야?'}, {'end': 161.78, 'start': 159.34, 'text': ' 근데 생새우를 사용하면은'}, {'end': 163.14, 'start': 161.78, 'text': ' 이 형들이'}, {'end': 164.04, 'start': 163.14, 'text': ' 아'}, {'end': 167.22, 'start': 166.04, 'text': ' 어우 상하게 얘기해.'}, {'end': 168.24, 'start': 167.22, 'text': ' 왜왜왜.'}, {'end': 170.78, 'start': 168.24, 'text': ' 거부 반응이 일어나가지고'}, {'end': 172.16, 'start': 170.78, 'text': ' 거부 반응 때문에 좀'}, {'end': 172.96, 'start': 172.16, 'text': ' 데칠 겁니다.'}, {'end': 177.02, 'start': 175.56, 'text': ' 아 만들어서 하나씩 쪼서 먹는구나.'}, {'end': 177.5, 'start': 177.02, 'text': ' 김밥을'}, {'end': 177.52, 'start': 177.5, 'text': ' 김밥을'}, {'end': 177.56, 'start': 177.52, 'text': ' 김밥을'}, {'end': 179, 'start': 177.56, 'text': ' 김밥을 이렇게 쪼서 먹는 맛이 있어.'}, {'end': 181.64, 'start': 179, 'text': ' 이거 햄은 뭐 구워서 넣어? 아니면 그냥 넣어?'}, {'end': 182.86, 'start': 181.64, 'text': ' 그냥 넣어 그냥 넣어 그냥 넣어 그냥 넣어.'}, {'end': 184.04, 'start': 182.86, 'text': ' 하자 볶아야지.'}, {'end': 185.6, 'start': 184.04, 'text': ' 그럼 짱배가 좀 볶아줄래?'}, {'end': 187.8, 'start': 185.6, 'text': ' 그래 내가 내가 볶아야 돼.'}, {'end': 189.24, 'start': 187.8, 'text': ' 혹시'}, {'end': 190.7, 'start': 189.24, 'text': ' 아냐아냐 그거 말고 좀 좀 좀.'}, {'end': 192.9, 'start': 190.7, 'text': ' 이거 왜 이렇게 새까매?'}, {'end': 194.88, 'start': 192.9, 'text': ' 이거 행성 폭발이잖아 이거.'}, {'end': 196.92, 'start': 194.88, 'text': ' 이거 스모키 향 내.'}, {'end': 199.54, 'start': 196.92, 'text': ' 스모키 용도로 쓰는 거 아니야.'}, {'end': 200.52, 'start': 199.54, 'text': ' 잠시만요.'}, {'end': 201.92, 'start': 200.52, 'text': ' 잠시만요.'}, {'end': 202.78, 'start': 201.92, 'text': ' 안녕.'}, {'end': 204.28, 'start': 202.78, 'text': ' 빵.'}, {'end': 206.56, 'start': 204.28, 'text': ' 생각보다 좀 심해야겠네.'}, {'end': 207.52, 'start': 206.56, 'text': ' 모래이가 생각하는.'}, {'end': 209.16, 'start': 207.52, 'text': ' 김밥의 생명은 뭐죠?'}, {'end': 210.46, 'start': 209.16, 'text': ' 단무지.'}, {'end': 212.8, 'start': 210.46, 'text': ' 단무지 안 들어가면 솔직히 김밥 맛이 안 나.'}, {'end': 215.3, 'start': 212.8, 'text': ' 단무지가 약간 김밥의 정체성이다.'}, {'end': 216.16, 'start': 215.3, 'text': ' 네.'}, {'end': 217.3, 'start': 216.16, 'text': ' 그 좀 특별합니까?'}, {'end': 219.76, 'start': 217.3, 'text': ' 그 어머니의 김밥에 들어가는 재료들이.'}, {'end': 220.9, 'start': 219.76, 'text': ' 아 특별하지 않아.'}, {'end': 223.24, 'start': 220.9, 'text': ' 햄, 계란, 단무지, 시금치.'}, {'end': 225, 'start': 223.24, 'text': ' 한 요 정도만 들어가는 거 같아.'}, {'end': 226.24, 'start': 225, 'text': ' 그렇게만 했는데도'}, {'end': 227.88, 'start': 226.24, 'text': ' 밖에서 사 먹으면 그 맛이 안 나.'}, {'end': 231.04, 'start': 227.88, 'text': ' 약간 어머니 고유의 밑간이나 이런 게 또 있나 봐.'}, {'end': 231.64, 'start': 231.04, 'text': ' 아 그럼요 그럼요.'}, {'end': 234.04, 'start': 231.64, 'text': ' 노랭이 어머니가 진짜로 진짜 요리를 잘하세요.'}, {'end': 236.88, 'start': 234.04, 'text': ' 아 진짜 거의 뭐 한식 전문가급이라서.'}, {'end': 237.48, 'start': 236.88, 'text': ' 맛도 제법.'}, {'end': 238.34, 'start': 237.48, 'text': ' 어 진짜.'}, {'end': 239.98, 'start': 238.34, 'text': ' 어?'}, {'end': 242.46, 'start': 239.98, 'text': ' 하하하하.'}, {'end': 244.18, 'start': 242.46, 'text': ' 김밥.'}, {'end': 246.52, 'start': 244.18, 'text': ' 아 얼마나 그 부분을.'}, {'end': 247.42, 'start': 246.52, 'text': ' 어 너무 재밌다.'}, {'end': 249.6, 'start': 247.42, 'text': ' 잠깐만.'}, {'end': 250.62, 'start': 249.6, 'text': ' 진행자예요.'}, {'end': 252.66, 'start': 250.62, 'text': ' 아 카메라 잡고 있어.'}, {'end': 253.52, 'start': 252.66, 'text': ' 하하하하.'}, {'end': 254.36, 'start': 253.52, 'text': ' 내가 형님.'}, {'end': 255.56, 'start': 254.36, 'text': ' 김밥.'}, {'end': 256.6, 'start': 255.56, 'text': ' 하하하하.'}, {'end': 257.36, 'start': 256.6, 'text': ' 어 야 이거.'}, {'end': 258.34, 'start': 257.36, 'text': ' 방심하면 안 되겠다.'}, {'end': 260.36, 'start': 258.34, 'text': ' 어 근데 인디업 한번 봐주는 겁니까?'}, {'end': 261.08, 'start': 260.36, 'text': ' 어?'}, {'end': 261.64, 'start': 261.08, 'text': ' 어?'}, {'end': 262.04, 'start': 261.64, 'text': ' 어?'}, {'end': 262.58, 'start': 262.04, 'text': ' 어?'}, {'end': 263.1, 'start': 262.58, 'text': ' 어?'}, {'end': 263.64, 'start': 263.1, 'text': ' 어?'}, {'end': 265.28, 'start': 263.64, 'text': ' 진행자가.'}, {'end': 266.84, 'start': 265.28, 'text': ' 아 근데 우리 다 했잖아 근데.'}, {'end': 269.08, 'start': 266.84, 'text': ' 아니 오늘 제가 입 닫고 있었어.'}, {'end': 271.12, 'start': 269.08, 'text': ' 자 그러면 우리 밥 끓여야겠네.'}, {'end': 272.34, 'start': 271.12, 'text': ' 원숭이.'}, {'end': 274.42, 'start': 272.34, 'text': ' 하하하하.'}, {'end': 276.52, 'start': 274.42, 'text': ' 맛살 요정도 쟤 쓰면 뭐 어때?'}, {'end': 277.22, 'start': 276.52, 'text': ' 하하하하.'}, {'end': 278.26, 'start': 277.22, 'text': ' 맛살 무덤이다.'}, {'end': 279.52, 'start': 278.26, 'text': ' 산산산 쌓이겠다.'}, {'end': 280.46, 'start': 279.52, 'text': ' 아 이건 그냥 먹어야지.'}, {'end': 280.96, 'start': 280.46, 'text': ' 어.'}, {'end': 283.72, 'start': 280.96, 'text': ' 햄을 굽겠습니다.'}, {'end': 284.82, 'start': 283.72, 'text': ' 어.'}, {'end': 285.96, 'start': 284.82, 'text': ' 기름 부이는 거 조심하고.'}, {'end': 286.8, 'start': 285.96, 'text': ' 아잉.'}, {'end': 287.3, 'start': 286.8, 'text': ' 잘 됐다.'}, {'end': 289.1, 'start': 287.3, 'text': ' 김밥도 만들어보자.'}, {'end': 291.1, 'start': 289.1, 'text': ' 이게 진짜 김밥 아이가?'}, {'end': 292.4, 'start': 291.1, 'text': ' 하하하하.'}, {'end': 294.46, 'start': 292.4, 'text': ' 김이 펄펄 나는 밥이 무슨.'}, {'end': 295.6, 'start': 294.46, 'text': ' 와.'}, {'end': 296.2, 'start': 295.6, 'text': ' 야.'}, {'end': 297.7, 'start': 296.84, 'text': ' 지단.'}, {'end': 298.74, 'start': 297.7, 'text': ' 좋아.'}, {'end': 300.04, 'start': 298.74, 'text': ' 축구선수 아닙니다.'}, {'end': 304.48, 'start': 300.04, 'text': ' 지단도 그렇게 많이 필요하면 집에서 일일이 만들 필요 없이 이렇게 팔아.'}, {'end': 308.12, 'start': 304.48, 'text': ' 제가 예전에 GS 납품하는 도시락 공장에서 일했었거든요.'}, {'end': 308.96, 'start': 308.12, 'text': ' 요런 거 다 써.'}, {'end': 309.72, 'start': 308.96, 'text': ' 음.'}, {'end': 310.32, 'start': 309.72, 'text': ' GS.'}, {'end': 311.02, 'start': 310.32, 'text': ' GS.'}, {'end': 311.62, 'start': 311.02, 'text': ' 하하하하.'}, {'end': 312.12, 'start': 311.62, 'text': ' 어?'}, {'end': 313.22, 'start': 312.12, 'text': ' 하하하하.'}, {'end': 314.02, 'start': 313.22, 'text': ' 원숭이.'}, {'end': 316.46, 'start': 314.02, 'text': ' 하하하하.'}, {'end': 317.62, 'start': 316.46, 'text': ' 자 생일자도.'}, {'end': 318.36, 'start': 317.62, 'text': ' 자 생일자도.'}, {'end': 320.2, 'start': 318.36, 'text': ' 원주민밥 맛있네.'}, {'end': 321.9, 'start': 320.2, 'text': ' 형님 그거 혹시 꼽는 김에.'}, {'end': 323.1, 'start': 321.9, 'text': ' 요것도.'}, {'end': 323.86, 'start': 323.1, 'text': ' 하하하하.'}, {'end': 325.56, 'start': 323.86, 'text': ' 오 잘 꼽고 있네 거의.'}, {'end': 326.16, 'start': 325.56, 'text': ' 하하하하.'}, {'end': 326.7, 'start': 326.16, 'text': ' 하하하하.'}, {'end': 327.38, 'start': 326.84, 'text': ' 국물.'}, {'end': 328.8, 'start': 327.38, 'text': ' 국물.'}, {'end': 329.58, 'start': 328.8, 'text': ' 국물.'}, {'end': 330.28, 'start': 329.58, 'text': ' 국기 잘 했나?'}, {'end': 331.34, 'start': 330.28, 'text': ' 야 국기 잘 했다.'}, {'end': 331.78, 'start': 331.34, 'text': ' 예.'}, {'end': 332.68, 'start': 331.78, 'text': ' 고였다니까.'}, {'end': 335.62, 'start': 332.68, 'text': ' 국물 같이 넣어야 더 실한 있지 않을까.'}, {'end': 337.56, 'start': 335.62, 'text': ' 여기 살짝 마요.'}, {'end': 338.18, 'start': 337.56, 'text': ' 어?'}, {'end': 338.52, 'start': 338.18, 'text': ' 어?'}, {'end': 339.96, 'start': 338.52, 'text': ' 그러지 마요.'}, {'end': 341.58, 'start': 339.96, 'text': ' 하하하하.'}, {'end': 341.96, 'start': 341.58, 'text': ' 국물.'}, {'end': 342.56, 'start': 341.96, 'text': ' 국물.'}, {'end': 343.8, 'start': 342.56, 'text': ' 국물 넣는 거 그러지 마요.'}, {'end': 344.72, 'start': 343.8, 'text': ' 오오.'}, {'end': 346.46, 'start': 344.72, 'text': ' 야 오늘.'}, {'end': 347.36, 'start': 346.46, 'text': ' 가면 카메라.'}, {'end': 348.02, 'start': 347.36, 'text': ' 하하하하.'}, {'end': 349.5, 'start': 348.02, 'text': ' 하하하하.'}, {'end': 351.56, 'start': 349.5, 'text': ' 어어어.'}, {'end': 353.44, 'start': 351.56, 'text': ' 어둡다.'}, {'end': 354, 'start': 353.44, 'text': ' 밥.'}, {'end': 354.4, 'start': 354, 'text': ' 어?'}, {'end': 355.84, 'start': 354.4, 'text': ' 노랭이 많이 수다.'}, {'end': 356.16, 'start': 355.84, 'text': ' 예.'}, {'end': 357.76, 'start': 356.16, 'text': ' 간단하게 하나 말아보죠.'}, {'end': 362.98, 'start': 361.92, 'text': ' 짜잔!'}, {'end': 364.26, 'start': 363.4, 'text': ' 간단하게.'}, {'end': 366.48, 'start': 364.92, 'text': ' 자 오늘의 1호 김밥.'}, {'end': 367.44, 'start': 366.48, 'text': ' 첫 입입니다.'}, {'end': 368.98, 'start': 367.86, 'text': ' 역사적인 한 입.'}, {'end': 373.42, 'start': 372.4, 'text': ' 맛있어, 사나.'}, {'end': 377.2, 'start': 375.42, 'text': ' 2호 김밥 완성.'}, {'end': 378.1, 'start': 377.2, 'text': ' 어 2호 김밥.'}, {'end': 382.76, 'start': 381.22, 'text': ' 왜 이렇게 한쪽으로 쏠려?'}, {'end': 383.8, 'start': 382.76, 'text': ' 반반이 되네.'}, {'end': 385.48, 'start': 383.8, 'text': ' 야 너무 갈라치기 된 거 아니야?'}, {'end': 386.54, 'start': 385.48, 'text': ' 그래, 그래도 맛있잖아.'}, {'end': 389.08, 'start': 387.7, 'text': ' 밥이 좀 질 나?'}, {'end': 392.72, 'start': 390.3, 'text': ' 원래 밥에도 뭘 이렇게 하지 않습니까?'}, {'end': 398.62, 'start': 395.52, 'text': ' 와 근데 이 밥 자체로도 이미 맛있는 하나가 됐어.'}, {'end': 403.78, 'start': 402.62, 'text': ' 아 발로 먹었네.'}, {'end': 404.66, 'start': 403.78, 'text': ' 오늘 진짜..'}, {'end': 408.5, 'start': 407.66, 'text': ' 발로 먹어.'}, {'end': 410.2, 'start': 408.5, 'text': ' 어 발로..'}, {'end': 411.08, 'start': 410.2, 'text': ' 와 됐다.'}, {'end': 412.46, 'start': 411.6, 'text': ' 화목이!'}, {'end': 414.5, 'start': 412.7, 'text': ' 아직 여전히 좀 쏠려있어.'}, {'end': 417.48, 'start': 415.48, 'text': ' 화목이의 첫 입은 짱베에게.'}, {'end': 418.48, 'start': 417.48, 'text': ' 이럴 수가.'}, {'end': 424.16, 'start': 423.66, 'text': ' 어때?'}, {'end': 424.92, 'start': 424.16, 'text': ' 맛있다.'}, {'end': 426.16, 'start': 424.92, 'text': ' 오 맛있어요?'}, {'end': 429.12, 'start': 426.52, 'text': ' 감정이 더 묻어나니까 더 맛있는 것 같아.'}, {'end': 430.42, 'start': 429.12, 'text': ' 짱베 입장에서는.'}, {'end': 431.08, 'start': 430.42, 'text': ' 여러분들.'}, {'end': 433.2, 'start': 431.7, 'text': ' 감격사자 꺼졌습니다.'}, {'end': 434.96, 'start': 433.2, 'text': ' 아 좋아요.'}, {'end': 435.96, 'start': 434.96, 'text': ' 맑아 이제.'}, {'end': 439.8, 'start': 438.7, 'text': ' 오 이거 기대됩니다.'}, {'end': 440.44, 'start': 439.8, 'text': ' 참치.'}, {'end': 441.2, 'start': 440.44, 'text': ' 자 나오라.'}, {'end': 443.04, 'start': 442.2, 'text': ' 이거 봐요.'}, {'end': 444.48, 'start': 443.04, 'text': ' 짜잔.'}, {'end': 445.48, 'start': 444.48, 'text': ' 오.'}, {'end': 447.08, 'start': 446, 'text': ' 너도 쏠렸네.'}, {'end': 450.38, 'start': 448.08, 'text': ' 어 나도 한쪽으로 쏠렸지만.'}, {'end': 451.24, 'start': 450.38, 'text': ' 음 맛있어!'}, {'end': 451.96, 'start': 451.24, 'text': ' 참치김밥.'}, {'end': 456.08, 'start': 454.72, 'text': ' 혹시 참치김밥 별로 안 좋아?'}, {'end': 459.72, 'start': 458.6, 'text': ' 지퍼여쏘.'}, {'end': 462.34, 'start': 460.72, 'text': ' 요런 거 준비가 되었습니다.'}, {'end': 465.32, 'start': 463.72, 'text': ' 자 짱베 너 말해도 돼. 말해도 돼.'}, {'end': 465.82, 'start': 465.32, 'text': ' 그래?'}, {'end': 468.18, 'start': 465.82, 'text': ' 저는 저희 둘째 재료.'}, {'end': 469.6, 'start': 468.18, 'text': ' 분수 들어갑니다.'}, {'end': 470.88, 'start': 469.6, 'text': ' 어 분수?'}, {'end': 474.26, 'start': 471.48, 'text': ' 여기 제가 만든 참치김밥도 있습니다.'}, {'end': 475.36, 'start': 474.6, 'text': ' 어라.'}, {'end': 476.92, 'start': 475.48, 'text': ' 후차님 먹어보고 싶은 사람?'}, {'end': 477.58, 'start': 476.92, 'text': ' 야!'}, {'end': 478.34, 'start': 477.58, 'text': ' 안 좋아한다며?'}, {'end': 478.84, 'start': 478.34, 'text': ' 맞아.'}, {'end': 485.72, 'start': 482.44, 'text': ' 전 새우를 듬뿍 넣은 새우 기도를 만들어보겠습니다.'}, {'end': 486.98, 'start': 485.72, 'text': ' 야 나 처음 봐.'}, {'end': 488.38, 'start': 486.98, 'text': ' 나도 처음 본다.'}, {'end': 490.08, 'start': 488.38, 'text': ' 야 이거 어떻게 말하자나요?'}, {'end': 491.18, 'start': 490.08, 'text': ' 너의 느낌대로.'}, {'end': 493.14, 'start': 491.18, 'text': ' 우리 엄마는 발을 안 쓴다는데.'}, {'end': 494.12, 'start': 493.14, 'text': ' 옳지 옳지 옳지 옳지.'}, {'end': 495.42, 'start': 494.12, 'text': ' 오 니 잘한다!'}, {'end': 496.02, 'start': 495.42, 'text': ' 옳지!'}, {'end': 497.18, 'start': 496.02, 'text': ' 와 그냥 말아버린다.'}, {'end': 498.52, 'start': 497.18, 'text': ' 참야 니 재능 있다!'}, {'end': 499.98, 'start': 498.52, 'text': ' 어? 감각이 달라요?'}, {'end': 501.08, 'start': 499.98, 'text': ' 제가 먼저 먹어보겠습니다 그러면.'}, {'end': 501.58, 'start': 501.08, 'text': ' 그래.'}, {'end': 509.42, 'start': 505.48, 'text': ' 별맛이 안 났네 생각보다 생새우 살이.'}, {'end': 511.54, 'start': 509.42, 'text': ' 안에 뭐 양념거나 아니면 초장이나 이런 거.'}, {'end': 513.78, 'start': 511.54, 'text': ' 초장 느낌 있네.'}, {'end': 515.52, 'start': 513.78, 'text': ' 제가 말씀드리는 건 진멸이죠.'}, {'end': 518.64, 'start': 515.52, 'text': ' 오! 가장 얄쌍한 김밥이라고 하셨습니다.'}, {'end': 520.54, 'start': 518.64, 'text': ' 진멸이 김밥.'}, {'end': 524.58, 'start': 520.54, 'text': ' 이따가 내가 김밥 잔치를 더 화끈하게 만들어줄'}, {'end': 527.48, 'start': 524.58, 'text': ' 떡볶이를 준비해보겠습니다.'}, {'end': 528.48, 'start': 527.48, 'text': ' 찍어먹으면 맛있잖아요?'}, {'end': 530.38, 'start': 528.48, 'text': ' 내가 또 옛날에 상어...'}, {'end': 531.58, 'start': 530.38, 'text': ' 상어 먹었지 않습니까?'}, {'end': 533.22, 'start': 531.58, 'text': ' 고 경험을 바탕으로'}, {'end': 535.28, 'start': 533.22, 'text': ' 활용 점점을 찍어먹겠습니다.'}, {'end': 535.98, 'start': 535.28, 'text': ' 좋아요.'}, {'end': 537.68, 'start': 535.98, 'text': ' 이번에는 익은 거.'}, {'end': 539.58, 'start': 537.68, 'text': ' 살짝만.'}, {'end': 540.88, 'start': 539.58, 'text': ' 따란!'}, {'end': 542.32, 'start': 540.88, 'text': ' 오장대의 초장 김밥.'}, {'end': 546.18, 'start': 542.32, 'text': ' 어! 진이야.'}, {'end': 547.52, 'start': 546.18, 'text': ' 음!'}, {'end': 549.82, 'start': 547.52, 'text': ' 이건가 봐!'}, {'end': 552.42, 'start': 549.82, 'text': ' 조용히 하세요. 더 하니까 더.'}, {'end': 553.82, 'start': 552.42, 'text': ' 음!'}, {'end': 556.18, 'start': 553.82, 'text': ' 아! 소스였습니다 여러분들!'}, {'end': 558.22, 'start': 556.18, 'text': ' 소스였어요.'}, {'end': 560.52, 'start': 558.22, 'text': ' 하하하하하하'}, {'end': 563.84, 'start': 560.52, 'text': ' 너무 좋아서.'}, {'end': 565.02, 'start': 563.84, 'text': ' 오징어!'}, {'end': 565.52, 'start': 565.02, 'text': ' 김밥!'}, {'end': 567.02, 'start': 565.52, 'text': ' 오징어 김밥!'}, {'end': 569.12, 'start': 567.02, 'text': ' 김밥!'}, {'end': 572.92, 'start': 569.12, 'text': ' 하하하하하하'}, {'end': 573.52, 'start': 572.92, 'text': ' 와우!'}, {'end': 574.36, 'start': 573.52, 'text': ' 아직 안 끝났네.'}, {'end': 575.26, 'start': 574.36, 'text': ' 와우!'}, {'end': 578.02, 'start': 575.26, 'text': ' 하하하하하하'}, {'end': 579.98, 'start': 578.02, 'text': ' 야! 야! 외국인 아니야?'}, {'end': 583.68, 'start': 579.98, 'text': ' 자, 우리는 이제 공장처럼 김밥을 생산 중입니다.'}, {'end': 584.72, 'start': 583.68, 'text': ' 진짜 김밥 공장이야.'}, {'end': 586.86, 'start': 584.72, 'text': ' 자, 이제 내가 상감사를 활용한 방법을 알려줄게.'}, {'end': 587.72, 'start': 586.86, 'text': ' 네가 말해봐 봐.'}, {'end': 588.68, 'start': 587.72, 'text': ' 이거 잘 맞는 것 같아.'}, {'end': 589.38, 'start': 588.68, 'text': ' 응?'}, {'end': 590.22, 'start': 589.38, 'text': ' 꿀 팔라 그러네.'}, {'end': 591.56, 'start': 590.22, 'text': ' 궁금하다. 궁금한 사람 나와봐.'}, {'end': 593.92, 'start': 591.56, 'text': ' 갈고, 거기다가 이제 깻잎 한 두 장 정도.'}, {'end': 594.92, 'start': 593.92, 'text': ' 꼭다리 딴 뒤에.'}, {'end': 595.52, 'start': 595.02, 'text': ' 어.'}, {'end': 598.32, 'start': 595.52, 'text': ' 삼겹살 한 줄.'}, {'end': 599.68, 'start': 598.32, 'text': ' 단무지랑 우엉만.'}, {'end': 600.82, 'start': 599.68, 'text': ' 단무지랑 우엉만.'}, {'end': 601.62, 'start': 600.82, 'text': ' 야, 이걸 말 수 있나?'}, {'end': 603.18, 'start': 601.62, 'text': ' 자, 그리고 여기서 핵심인데.'}, {'end': 605.82, 'start': 603.18, 'text': ' 자, 저기 냉장고 안에 쌈장이 있어.'}, {'end': 606.72, 'start': 605.82, 'text': ' 아!'}, {'end': 607.72, 'start': 606.72, 'text': ' 쌈장 한 번 가져와.'}, {'end': 608.68, 'start': 607.72, 'text': ' 좋아, 좋아, 좋아.'}, {'end': 610.92, 'start': 608.68, 'text': ' 아주 좋아!'}, {'end': 612.48, 'start': 610.92, 'text': ' 아, 꿀팁인데?'}, {'end': 613.42, 'start': 612.48, 'text': ' 아, 잘해.'}, {'end': 615.02, 'start': 613.42, 'text': ' 아, 잘해.'}, {'end': 616.72, 'start': 615.02, 'text': ' 나 계속 일 시키려고 자도한 거 아니야?'}, {'end': 617.96, 'start': 616.72, 'text': ' 아니야, 진짜 잘해.'}, {'end': 620.08, 'start': 617.96, 'text': ' 이 역사적인 한 입은 생일자가 사야 된다.'}, {'end': 621.92, 'start': 620.08, 'text': ' 아, 생일!'}, {'end': 623.92, 'start': 621.92, 'text': ' 아, 생일!'}, {'end': 624.72, 'start': 623.92, 'text': ' 맛있어.'}, {'end': 626.22, 'start': 625.02, 'text': ' 오!'}, {'end': 627.02, 'start': 626.22, 'text': ' 그냥 깔끔하게 맛있어.'}, {'end': 628.02, 'start': 627.02, 'text': ' 지금 카메라만 틀어봐.'}, {'end': 629.72, 'start': 628.02, 'text': ' 이건 개발자가 먹어야 돼.'}, {'end': 630.72, 'start': 629.72, 'text': ' 그래, 개발자 한 입.'}, {'end': 637.52, 'start': 636.72, 'text': ' 너무 맛있어.'}, {'end': 638.32, 'start': 637.52, 'text': ' 고기 좋아.'}, {'end': 639.42, 'start': 638.32, 'text': ' 참치까지.'}, {'end': 640.92, 'start': 639.42, 'text': ' 아, 밥이 별로 없네요.'}, {'end': 642.42, 'start': 640.92, 'text': ' 그럼 맛살만 한 번 해볼까?'}, {'end': 644.12, 'start': 642.42, 'text': ' 밥 대신 맛살.'}, {'end': 645.02, 'start': 644.12, 'text': ' 저건 좀...'}, {'end': 645.72, 'start': 645.02, 'text': ' 아, 이럴 때 해보지.'}, {'end': 646.32, 'start': 645.72, 'text': ' 어찌하겠어.'}, {'end': 647.02, 'start': 646.32, 'text': ' 그래.'}, {'end': 648.72, 'start': 647.02, 'text': ' 난 밥 대신 부추 해볼래, 그럼.'}, {'end': 650.62, 'start': 648.72, 'text': ' 밥 좀 더 데워줘.'}, {'end': 652.62, 'start': 650.62, 'text': ' 두 개만 더 하면 딱 한 상자야.'}, {'end': 654.42, 'start': 652.62, 'text': ' 너 그거 돌려, 돌려, 돌려!'}, {'end': 656.22, 'start': 655.02, 'text': ' 일단 새우만 넣어보겠습니다.'}, {'end': 657.32, 'start': 656.22, 'text': ' 우와.'}, {'end': 660.02, 'start': 657.32, 'text': ' 짬배는 삼겹살이랑 새우.'}, {'end': 661.52, 'start': 660.02, 'text': ' 짬배는 부추.'}, {'end': 663.52, 'start': 661.52, 'text': ' 부추.'}, {'end': 664.72, 'start': 663.52, 'text': ' 벌칙인가요?'}, {'end': 666.02, 'start': 664.72, 'text': ' 아니, 일단 짬만 들어가면 괜찮아요.'}, {'end': 666.72, 'start': 666.02, 'text': ' 짬만 들어가면.'}, {'end': 669.02, 'start': 666.72, 'text': ' 달기에 또 의미가 있다.'}, {'end': 671.72, 'start': 669.02, 'text': ' 와, 기관총.'}, {'end': 673.22, 'start': 671.72, 'text': ' 야, 이게 뭐야.'}, {'end': 675.02, 'start': 673.22, 'text': ' 하하하하하하.'}, {'end': 676.82, 'start': 675.02, 'text': ' 슉창.'}, {'end': 677.42, 'start': 676.82, 'text': ' 슉창.'}, {'end': 679.02, 'start': 677.42, 'text': ' 오오오, 깊게 들어갔어요.'}, {'end': 683.02, 'start': 682.22, 'text': ' 아, 괜찮네?'}, {'end': 683.72, 'start': 683.02, 'text': ' 어, 괜찮네?'}, {'end': 684.02, 'start': 683.72, 'text': ' 어.'}, {'end': 685.52, 'start': 684.02, 'text': ' 노랭이 한번 줘, 노랭이 한번 줘.'}, {'end': 687.02, 'start': 685.52, 'text': ' 으음.'}, {'end': 688.02, 'start': 687.02, 'text': ' 저거 반탕인데.'}, {'end': 689.52, 'start': 688.02, 'text': ' 하하하하하하.'}, {'end': 690.52, 'start': 689.52, 'text': ' 어, 김이 없네?'}, {'end': 691.52, 'start': 690.52, 'text': ' 아, 맞네.'}, {'end': 692.52, 'start': 691.52, 'text': ' 더 안 걸려도 되겠다.'}, {'end': 694.02, 'start': 692.52, 'text': ' 야, 그럼 참치랑 비벼.'}, {'end': 695.02, 'start': 694.02, 'text': ' 좋은 밥에.'}, {'end': 696.52, 'start': 695.02, 'text': ' 오, 노랭이 비빔밥?'}, {'end': 698.52, 'start': 696.52, 'text': ' 와, 진짜 알차게.'}, {'end': 699.02, 'start': 698.52, 'text': ' 아, 여러분.'}, {'end': 700.52, 'start': 699.02, 'text': ' 떡볶이가 다 됐습니다.'}, {'end': 701.52, 'start': 700.52, 'text': ' 그랬어.'}, {'end': 702.52, 'start': 701.52, 'text': ' 할, 할.'}, {'end': 705.02, 'start': 702.52, 'text': ' 아까 강우이가 양감장도 직접 만들더라고.'}, {'end': 705.52, 'start': 705.02, 'text': ' 네.'}, {'end': 707.02, 'start': 705.52, 'text': ' 와, 이거 진짜 맛있겠다.'}, {'end': 707.52, 'start': 707.02, 'text': ' 진짜 맛있겠다.'}, {'end': 710.52, 'start': 707.52, 'text': ' 시중에 있는 레시피를 조금 참고해서'}, {'end': 712.02, 'start': 710.52, 'text': ' 제 식대로 한번 바꿔봤습니다.'}, {'end': 712.52, 'start': 712.02, 'text': ' 와.'}, {'end': 714.52, 'start': 714.02, 'text': ' 네.'}, {'end': 715.02, 'start': 714.52, 'text': ' 한...'}, {'end': 743.52, 'start': 715.02, 'text': ' 한...'}, {'end': 746.58, 'start': 744.02, 'text': ' 김밥이랑 같이 찍어 먹는 걸 좀 생각했습니다.'}, {'end': 749.36, 'start': 746.58, 'text': ' 오묘한 육향이 느껴지는데 어디서 낸 겁니까, 이거?'}, {'end': 750.78, 'start': 749.36, 'text': ' 아주 정확하구만.'}, {'end': 753.56, 'start': 750.78, 'text': ' 짱배가 구운 삼겹살에 기름을 좀 달아주신 것 같습니다.'}, {'end': 754.64, 'start': 753.56, 'text': ' 아, 그래.'}, {'end': 755.94, 'start': 754.64, 'text': ' 버섯 캐치한다고?'}, {'end': 756.9, 'start': 755.94, 'text': ' 야, 오늘 아들하고.'}, {'end': 758.1, 'start': 756.9, 'text': ' 가래다 물을 부쳐야 돼.'}, {'end': 758.9, 'start': 758.1, 'text': ' 이렇게.'}, {'end': 760.14, 'start': 758.9, 'text': ' 참치김밥.'}, {'end': 762.9, 'start': 760.14, 'text': ' 이거 고기 송수 찍어가지고.'}, {'end': 763.9, 'start': 762.9, 'text': ' 뭐야, 뭐야.'}, {'end': 770.44, 'start': 768.9, 'text': ' 야, 가훈이 너무 맛있다, 이거.'}, {'end': 771.88, 'start': 770.44, 'text': ' 찍어 먹으니까 좀 괜찮다.'}, {'end': 773.28, 'start': 771.88, 'text': ' 자, 잘라 놓은 것도 좀 드시고.'}, {'end': 774.28, 'start': 773.28, 'text': ' 감사해요.'}, {'end': 775.28, 'start': 774.28, 'text': ' 우와.'}, {'end': 777.28, 'start': 775.28, 'text': ' 평소에 강훈이가 이런 거 잘 안 해주는데.'}, {'end': 777.98, 'start': 777.28, 'text': ' 생일인 게 뭐야?'}, {'end': 779.28, 'start': 777.98, 'text': ' 생일이잖아, 친구가.'}, {'end': 781.28, 'start': 779.28, 'text': ' 그러니까, 감동받을 것 같다.'}, {'end': 782.78, 'start': 781.28, 'text': ' 친구 생일에만 있는 특별 서비스.'}, {'end': 784.28, 'start': 782.78, 'text': ' 내일부터 이런 거 없어.'}, {'end': 786.28, 'start': 785.28, 'text': ' 특별.'}, {'end': 789.78, 'start': 788.28, 'text': ' 먹고 얘기해, 먹고 얘기해.'}, {'end': 790.78, 'start': 789.78, 'text': ' 강훈이가 특별.'}, {'end': 792.28, 'start': 790.78, 'text': ' 땡땡땡이라고 했어.'}, {'end': 793.28, 'start': 792.28, 'text': ' 아, 특별.'}, {'end': 794.78, 'start': 793.28, 'text': ' 아, 이벤트라고 했다고.'}, {'end': 799.28, 'start': 797.78, 'text': ' 조직은 규칙이잖아.'}, {'end': 801.28, 'start': 800.28, 'text': ' 다 굳었어.'}, {'end': 805.28, 'start': 803.28, 'text': ' 오, 맛있다.'}, {'end': 807.28, 'start': 805.28, 'text': ' 여러분들, 잊지 말고 여기 주먹밥도 있어요.'}, {'end': 808.28, 'start': 807.28, 'text': ' 아, 맞아.'}, {'end': 809.28, 'start': 808.28, 'text': ' 근데 왜 그게 달라?'}, {'end': 810.28, 'start': 809.28, 'text': ' 점점 갈수록.'}, {'end': 813.28, 'start': 812.28, 'text': ' 너무 가늘어.'}, {'end': 814.28, 'start': 813.28, 'text': ' 싱거울 수도 있어.'}, {'end': 815.28, 'start': 814.28, 'text': ' 응.'}, {'end': 817.28, 'start': 815.28, 'text': ' 이럴 때 떡볶이를 같이 먹으면.'}, {'end': 818.28, 'start': 817.28, 'text': ' 완벽해.'}, {'end': 819.28, 'start': 818.28, 'text': ' 중간 크기.'}, {'end': 822.28, 'start': 820.28, 'text': ' 주먹밥은 주먹만 해야지.'}, {'end': 825.28, 'start': 824.28, 'text': ' 잘한다.'}, {'end': 826.28, 'start': 825.28, 'text': ' 음.'}, {'end': 827.28, 'start': 826.28, 'text': ' 거의 맨밥.'}, {'end': 832.28, 'start': 829.28, 'text': ' 먹으면서 우리 노랭이 미담 한 개씩만 하자.'}, {'end': 833.28, 'start': 832.28, 'text': ' 아.'}, {'end': 834.28, 'start': 833.28, 'text': ' 미담?'}, {'end': 835.28, 'start': 834.28, 'text': ' 응.'}, {'end': 836.28, 'start': 835.28, 'text': ' 그럼 선물로 가자.'}, {'end': 840.28, 'start': 839.28, 'text': ' 선물 하나씩 주면서.'}, {'end': 841.28, 'start': 840.28, 'text': ' 응.'}, {'end': 842.28, 'start': 841.28, 'text': ' 어, 미담 하나씩.'}, {'end': 843.28, 'start': 842.28, 'text': ' 난 그런 거 좋아.'}, {'end': 846.28, 'start': 844.28, 'text': ' 자, 누구부터 선물 증정할까요?'}, {'end': 848.28, 'start': 846.28, 'text': ' 어, 제가 먼저 하겠습니다.'}, {'end': 851.28, 'start': 848.28, 'text': ' 저는 3초 두바이 초콜릿 준비했습니다.'}, {'end': 853.28, 'start': 851.28, 'text': ' 그냥 두바이 초콜릿도 아니고.'}, {'end': 854.28, 'start': 853.28, 'text': ' 그게 뭔데?'}, {'end': 856.28, 'start': 854.28, 'text': ' 뭔가 3초 동안 두바이 초콜릿을 보입니다.'}, {'end': 857.28, 'start': 856.28, 'text': ' 짜잔.'}, {'end': 858.28, 'start': 857.28, 'text': ' 어? 우와.'}, {'end': 860.28, 'start': 858.28, 'text': ' 게임의 왓츠 젤다.'}, {'end': 861.28, 'start': 860.28, 'text': ' 원래 시계인데.'}, {'end': 862.28, 'start': 861.28, 'text': ' 어.'}, {'end': 864.28, 'start': 862.28, 'text': ' 이 안에 게임이 내장되어 있어.'}, {'end': 867.28, 'start': 864.28, 'text': ' 제가 이거 마리오 버전이 있는데 살까 말까 고민 많이 했던 건데.'}, {'end': 868.28, 'start': 867.28, 'text': ' 우와.'}, {'end': 869.28, 'start': 868.28, 'text': ' 대박.'}, {'end': 873.28, 'start': 869.28, 'text': ' 이 안에 젤다 1, 젤다 2, 젤다 어웨이킹까지 있습니다.'}, {'end': 874.28, 'start': 873.28, 'text': ' 아.'}, {'end': 875.28, 'start': 874.28, 'text': ' 바로 뜯어봐야겠다.'}, {'end': 876.28, 'start': 875.28, 'text': ' 어머.'}, {'end': 877.28, 'start': 876.28, 'text': ' 이 외래어 쓰는 건 인정하는 부분이죠?'}, {'end': 878.28, 'start': 877.28, 'text': ' 아.'}, {'end': 879.28, 'start': 878.28, 'text': ' 이제 끝났어.'}, {'end': 880.28, 'start': 879.28, 'text': ' 끝났어.'}, {'end': 881.28, 'start': 880.28, 'text': ' 아.'}, {'end': 882.28, 'start': 881.28, 'text': ' 끝났어.'}, {'end': 883.28, 'start': 882.28, 'text': ' 오케이.'}, {'end': 884.28, 'start': 883.28, 'text': ' 오케이.'}, {'end': 885.28, 'start': 884.28, 'text': ' 오케이.'}, {'end': 886.28, 'start': 885.28, 'text': ' 오케이.'}, {'end': 887.28, 'start': 886.28, 'text': ' 오케이.'}, {'end': 888.28, 'start': 887.28, 'text': ' 오케이.'}, {'end': 889.28, 'start': 888.28, 'text': ' 오케이.'}, {'end': 890.28, 'start': 889.28, 'text': ' 오케이.'}, {'end': 891.28, 'start': 890.28, 'text': ' 오케이.'}, {'end': 892.28, 'start': 891.28, 'text': ' 오케이.'}, {'end': 893.28, 'start': 892.28, 'text': ' 오케이.'}, {'end': 894.28, 'start': 893.28, 'text': ' 오케이.'}, {'end': 895.28, 'start': 894.28, 'text': ' 오케이.'}, {'end': 896.28, 'start': 895.28, 'text': ' 오케이.'}, {'end': 897.28, 'start': 896.28, 'text': ' 오케이.'}, {'end': 898.28, 'start': 897.28, 'text': ' 오케이.'}, {'end': 899.28, 'start': 898.28, 'text': ' 오케이.'}, {'end': 900.28, 'start': 899.28, 'text': ' 오케이.'}, {'end': 901.28, 'start': 900.28, 'text': ' 오케이.'}, {'end': 902.28, 'start': 901.28, 'text': ' 오케이.'}, {'end': 903.28, 'start': 902.28, 'text': ' 오케이.'}, {'end': 904.28, 'start': 903.28, 'text': ' 옢.'}, {'end': 905.28, 'start': 904.28, 'text': '东 경우 집에 मrafly 포장되어 있어요.'}, {'end': 906.28, 'start': 905.28, 'text': ' 오.'}, {'end': 907.28, 'start': 906.28, 'text': ' 오.'}, {'end': 908.28, 'start': 907.28, 'text': ' 오!'}, {'end': 909.28, 'start': 908.28, 'text': ' 오!'}, {'end': 910.28, 'start': 909.28, 'text': ' 오!'}, {'end': 911.28, 'start': 910.28, 'text': ' 오!'}, {'end': 912.28, 'start': 911.28, 'text': 'fed....'}, {'end': 913.28, 'start': 912.28, 'text': ' 드디어.'}, {'end': 914.28, 'start': 913.28, 'text': ' 오!'}, {'end': 915.28, 'start': 914.28, 'text': ' 오네� 04.'}, {'end': 916.28, 'start': 915.28, 'text': ' pussy naj.'}, {'end': 917.28, 'start': 916.28, 'text': ' 오!'}, {'end': 918.28, 'start': 917.28, 'text': ' 오!'}, {'end': 919.28, 'start': 918.28, 'text': ' 오.'}, {'end': 920.28, 'start': 919.28, 'text': ' 하!'}, {'end': 921.42, 'start': 920.28, 'text': ' 아, 감사합니다.'}, {'end': 924.1, 'start': 921.96, 'text': ' 저는 뭐 근데 귀담이라고..'}, {'end': 929.08, 'start': 924.1, 'text': ' 노랭이가 약간 좀 은근히 알게 모르게 꿀팁이나 이런 거 각종 정보들 알려주거든.'}, {'end': 931.04, 'start': 929.08, 'text': ' 원래 그런 거 넘어갈 수도 있잖아.'}, {'end': 932.74, 'start': 931.04, 'text': ' 제로 비락시켜 세일한다 이거.'}, {'end': 935.72, 'start': 932.74, 'text': ' 사이즈 사게 살 수 있다 링크 막 올려주고.'}, {'end': 939.72, 'start': 935.72, 'text': ' 그리고 특히나 저는 노랭이한테 약간 이런 각종 물건들 막 이렇게 막'}, {'end': 942.32, 'start': 939.72, 'text': ' 거의 흙값에 막 이렇게 구매한 적도 많습니다.'}, {'end': 943.68, 'start': 942.32, 'text': ' 아, 맞아. 막 저렴하게.'}, {'end': 944.88, 'start': 943.68, 'text': ' 노랭이가 쓰던 에어팟 프로.'}, {'end': 948.68, 'start': 944.88, 'text': ' 그리고 또 막 그 팝플 영상에도 좀 자주 막 나왔었던 온클레어 패딩.'}, {'end': 951.36, 'start': 948.68, 'text': ' 일상에서 그런 소소한 뭐랄까 도움들을 진짜 많이 줘요.'}, {'end': 954.44, 'start': 951.36, 'text': ' 맞아요. 저희 중에서도 가장 정보검색왕이거든요.'}, {'end': 957.06, 'start': 954.44, 'text': ' 단순히 그냥 정보가 많다는 게 아니야.'}, {'end': 960.98, 'start': 957.06, 'text': ' 학창시 시절에 정보검색능력대회에서 장교사가 나왔어.'}, {'end': 961.86, 'start': 960.98, 'text': ' 맞아요, 맞아요.'}, {'end': 963.92, 'start': 961.86, 'text': ' 아, 장교사는 뭔가 좀 고민되는..'}, {'end': 965.26, 'start': 963.92, 'text': ' 아니야, 상반되는 거야.'}, {'end': 966.22, 'start': 965.26, 'text': ' 고민되는..'}, {'end': 967.12, 'start': 966.22, 'text': ' 자, 그런 의미에서..'}, {'end': 970.76, 'start': 967.12, 'text': ' 지갑을 지켜줄 수 있지는 않지만 그래도'}, {'end': 973.36, 'start': 970.76, 'text': ' 양으로 승부하겠습니다.'}, {'end': 974.58, 'start': 973.36, 'text': ' 일단 이거..'}, {'end': 977.78, 'start': 976.68, 'text': ' 가면라이더!'}, {'end': 978.66, 'start': 977.78, 'text': ' 가면라이더!'}, {'end': 982.46, 'start': 978.66, 'text': ' 이건 최근에 팝플이 굉장히 재밌게 봤었던 뽑기로 뽑았는데'}, {'end': 983.74, 'start': 982.46, 'text': ' 이렇게 노랭이 생각이 나서'}, {'end': 984.58, 'start': 983.74, 'text': ' 축하해.'}, {'end': 986.14, 'start': 984.58, 'text': ' 아, 진짜?'}, {'end': 988.46, 'start': 986.14, 'text': ' A상이면 엄청 뽑기 힘든 거거든.'}, {'end': 990.02, 'start': 988.46, 'text': ' 한 번 뽑았는데 이렇게 나왔어.'}, {'end': 990.62, 'start': 990.02, 'text': ' 우와!'}, {'end': 993, 'start': 990.62, 'text': ' 근데 뽑기 한 번에 만 원, 이만 원 하잖아.'}, {'end': 996.26, 'start': 993, 'text': ' 그리고 중요한 의미는 저 물건이 뽑기가 아니면 구할 수가 없는 물건이니까.'}, {'end': 997.62, 'start': 996.26, 'text': ' 맞아, 맞아.'}, {'end': 1000.32, 'start': 997.62, 'text': ' 난 뽑기로 뽑은 걸 주기에는 또 그럴 것 같아서'}, {'end': 1001.42, 'start': 1000.32, 'text': ' 또 따로 추가로..'}, {'end': 1002.54, 'start': 1001.42, 'text': ' 아니, 와..'}, {'end': 1003.56, 'start': 1002.54, 'text': ' 네..'}, {'end': 1004.82, 'start': 1003.56, 'text': ' 이건 뭐야?'}, {'end': 1008.66, 'start': 1004.82, 'text': ' 이거는 이제 노랭이가 굉장히 좋아하는 게임 중에 하나죠.'}, {'end': 1010.42, 'start': 1008.66, 'text': ' 승리의 여신.'}, {'end': 1011.06, 'start': 1010.42, 'text': ' 오!'}, {'end': 1013.82, 'start': 1011.06, 'text': ' 제가 얼마 전에 또 팝업스토어까지 다녀왔는데'}, {'end': 1017.66, 'start': 1013.82, 'text': ' 이거는 평범한 팝업스토어의 굿즈가 있는데'}, {'end': 1020.04, 'start': 1017.66, 'text': ' 잠깐만, 방송에 나갈 수 있는 거야?'}, {'end': 1022.7, 'start': 1020.04, 'text': ' 야, 일단 봐봐, 이거 검열 될까, 이거?'}, {'end': 1024.7, 'start': 1022.7, 'text': ' 괜찮을까?'}, {'end': 1025.9, 'start': 1024.7, 'text': ' 나는 괜찮을 것 같은데?'}, {'end': 1026.82, 'start': 1025.9, 'text': ' 오, 잠깐만.'}, {'end': 1028.5, 'start': 1026.82, 'text': ' 자세히 보지 말고.'}, {'end': 1029.82, 'start': 1028.5, 'text': ' 아, 잠깐만.'}, {'end': 1031.9, 'start': 1029.82, 'text': ' 좀 더 검열을 많이 해야 될 것 같아요.'}, {'end': 1033.82, 'start': 1031.9, 'text': ' 이 캐릭터로 가려가지고.'}, {'end': 1036.66, 'start': 1033.82, 'text': ' 아, 이 캐릭터!'}, {'end': 1037.66, 'start': 1036.66, 'text': ' 아, 캐릭터!'}, {'end': 1038.5, 'start': 1037.66, 'text': ' 앨리스!'}, {'end': 1039.7, 'start': 1038.5, 'text': ' 이게 제가 좋아하는 캐릭터입니다.'}, {'end': 1041, 'start': 1039.7, 'text': ' 아, 맞아요. 아, 노랭이가 또 봤어.'}, {'end': 1044.2, 'start': 1041, 'text': ' 위키의 출시 1주년 기념 후쿠오카.'}, {'end': 1044.8, 'start': 1044.2, 'text': ' 가정입니까?'}, {'end': 1045.46, 'start': 1044.8, 'text': ' 네, 가정입니다.'}, {'end': 1046.8, 'start': 1045.46, 'text': ' 우와!'}, {'end': 1050.2, 'start': 1046.8, 'text': ' 아, 근데 안 뜯고 이렇게 보관하고 싶다.'}, {'end': 1051.2, 'start': 1050.2, 'text': ' 아, 이거 궁금하네요, 개인적으로.'}, {'end': 1052.7, 'start': 1051.2, 'text': ' 와, 이거 꽤 크더라고요.'}, {'end': 1053.7, 'start': 1052.7, 'text': ' 이게 모공품입니다.'}, {'end': 1055.9, 'start': 1053.7, 'text': ' 우와!'}, {'end': 1056.7, 'start': 1055.9, 'text': ' 대박!'}, {'end': 1057.5, 'start': 1056.7, 'text': ' 크네요?'}, {'end': 1060, 'start': 1057.5, 'text': ' 후독이들이 우리가 나이에 안 맞다고 생각하실 수도 있는데'}, {'end': 1061.5, 'start': 1060, 'text': ' 저희 특철물 되게 좋아합니다.'}, {'end': 1064.2, 'start': 1061.5, 'text': ' 새로운 변신 나올 때마다 다 같이'}, {'end': 1065, 'start': 1064.2, 'text': ' 우와!'}, {'end': 1065.8, 'start': 1065, 'text': ' 이러고 서거든요.'}, {'end': 1066.5, 'start': 1065.8, 'text': ' 새로운 슈트.'}, {'end': 1070.2, 'start': 1068.5, 'text': ' 예!'}, {'end': 1071.2, 'start': 1070.2, 'text': ' 퀄리티 뭐야?'}, {'end': 1072.2, 'start': 1071.2, 'text': ' 우와!'}, {'end': 1074, 'start': 1072.2, 'text': ' 오, 내가 한 가지 느낀 점.'}, {'end': 1076.3, 'start': 1074, 'text': ' 먼저 하길 잘했다.'}, {'end': 1078, 'start': 1076.3, 'text': ' 쿠오카의 그다음은 누굽니까?'}, {'end': 1079.3, 'start': 1078, 'text': ' 짜오, 그럼 하나.'}, {'end': 1080.3, 'start': 1079.3, 'text': ' 어, 일단 미담.'}, {'end': 1081.8, 'start': 1080.3, 'text': ' 어, 짜오의 미담, 짜오.'}, {'end': 1084.3, 'start': 1081.8, 'text': ' 일단 노랭이는 영화광입니다, 영화광.'}, {'end': 1086.2, 'start': 1084.3, 'text': ' 포스터도 모으고 하다 보니까'}, {'end': 1088.5, 'start': 1086.2, 'text': ' 포스터를 받기 위해 보는 영화도 있어.'}, {'end': 1091.7, 'start': 1088.5, 'text': ' 아, 근데 그런 와중에 또 작품성 있는 영화들도 되게 많이 보기 때문에'}, {'end': 1094.3, 'start': 1091.7, 'text': ' 그런 거 있으면 저한테 꼭 한번 물어봅니다, 거의.'}, {'end': 1095.5, 'start': 1094.3, 'text': ' 추천해 줍니다, 제가.'}, {'end': 1097.5, 'start': 1095.5, 'text': ' 최근에 위대한 쇼맨을 재고용을 했는데.'}, {'end': 1098.5, 'start': 1097.5, 'text': ' 최근에 위대한 쇼맨을 재고용을 했는데.'}, {'end': 1101.8, 'start': 1098.5, 'text': ' 돌비 시네마에서 한다고 이렇게 알려줘가지고'}, {'end': 1103.8, 'start': 1101.8, 'text': ' 여자친구가 진짜 재밌게 보고.'}, {'end': 1106.8, 'start': 1103.8, 'text': ' 여자친구가 진짜 노랭이한테 감사했어, 그거.'}, {'end': 1110.9, 'start': 1106.8, 'text': ' 음악 영화들이 돌비 시네마에서 보면 진짜 그 울림이 달라.'}, {'end': 1112.5, 'start': 1110.9, 'text': ' 전 꼭 추천해 주고 싶었거든.'}, {'end': 1114.1, 'start': 1112.5, 'text': ' 이런 거를 저도 좋아하는데'}, {'end': 1116.3, 'start': 1114.1, 'text': ' 항상 그런 정보 찾는 거에 대해서 미숙했는데'}, {'end': 1118, 'start': 1116.3, 'text': ' 노랭이가 있으면 알려주고'}, {'end': 1120.5, 'start': 1118, 'text': ' 가서 보면 또 좋아해서 난 또 감사해고.'}, {'end': 1125.8, 'start': 1120.5, 'text': ' 근데 다들 여기는 티켓팅, 여기는 물건 구매할 때, 여기 영화 볼 때.'}, {'end': 1128.3, 'start': 1125.8, 'text': ' 이거는 저희 노랭이 하는 거잖아.'}, {'end': 1131.1, 'start': 1128.3, 'text': ' 회의는 상품을 받아라.'}, {'end': 1134.4, 'start': 1131.1, 'text': ' 이런 것 좀 이제 같이 나누면 나도 좋겠네.'}, {'end': 1137.6, 'start': 1134.4, 'text': ' 선물은 뭐 그렇게 큰 의미가 있는 건 아닙니다만'}, {'end': 1139, 'start': 1137.6, 'text': ' 하지만 만드는 거 좋아하는데.'}, {'end': 1141.4, 'start': 1139, 'text': ' 오, 커.'}, {'end': 1144.5, 'start': 1141.4, 'text': ' 이런 거는 본 적이 없을 거예요.'}, {'end': 1146.6, 'start': 1144.5, 'text': ' 야, 이게 뭐야?'}, {'end': 1149.9, 'start': 1146.6, 'text': ' 퍼즐인데 그 종이로 돼 있는 거.'}, {'end': 1151.5, 'start': 1149.9, 'text': ' 근데 이게 중요한 거는 저'}, {'end': 1153.8, 'start': 1151.5, 'text': ' 이제 귀엽 파츠를 쓰면 자동으로 돌아가.'}, {'end': 1155.4, 'start': 1153.8, 'text': ' 아, 진짜?'}, {'end': 1156.9, 'start': 1155.4, 'text': ' 이런 거 좋아해가지고'}, {'end': 1158.3, 'start': 1156.9, 'text': ' 그 옛날에 초코파이였나?'}, {'end': 1159.6, 'start': 1158.3, 'text': ' 그 상품 들어있었어요.'}, {'end': 1161.2, 'start': 1159.6, 'text': ' 그렇지, 그렇지.'}, {'end': 1162, 'start': 1161.2, 'text': ' 코빌, 코빌.'}, {'end': 1163.2, 'start': 1162, 'text': ' 코빌인가?'}, {'end': 1166.2, 'start': 1163.2, 'text': ' 야, 근데 첫 프라이 치고는 난이도가 좀 높다.'}, {'end': 1167.4, 'start': 1166.2, 'text': ' 아, 얘 있어, 얘 있어.'}, {'end': 1169, 'start': 1167.4, 'text': ' 난이도가 있잖아.'}, {'end': 1170.3, 'start': 1169, 'text': ' 난이도 일단 별 4개.'}, {'end': 1171.5, 'start': 1170.3, 'text': ' 이거 주는 의미는'}, {'end': 1174.9, 'start': 1171.5, 'text': ' 이 밑에 보면 이게 세계적인 약간 명소들이 있어.'}, {'end': 1176.5, 'start': 1174.9, 'text': ' 아, 핸드마크 같은 거.'}, {'end': 1178.2, 'start': 1176.5, 'text': ' 그래서 혹시나 만약에 여기 들릴 일 있으면'}, {'end': 1180.3, 'start': 1178.2, 'text': ' 거기 물품 사와서 여기 보관하고 뭐 그런.'}, {'end': 1181.6, 'start': 1180.3, 'text': ' 아, 그런 거.'}, {'end': 1183.6, 'start': 1181.6, 'text': ' 진짜 저거 채우는 재미가 있겠다.'}, {'end': 1184.6, 'start': 1183.6, 'text': ' 아, 감사합니다.'}, {'end': 1187.1, 'start': 1184.6, 'text': ' 우와.'}, {'end': 1188.1, 'start': 1187.1, 'text': ' 야, 이거.'}, {'end': 1189.6, 'start': 1188.3, 'text': ' 마지막 선물 기쁘신 줄 알았는데.'}, {'end': 1190.5, 'start': 1189.6, 'text': ' 야, 제일.'}, {'end': 1192.8, 'start': 1190.5, 'text': ' 아니, 뭐 그런 건 아니지, 아니지.'}, {'end': 1193.8, 'start': 1192.8, 'text': ' 얼굴 빨개진다.'}, {'end': 1195.9, 'start': 1193.8, 'text': ' 내가 선물을 잘 할 줄은 몰라.'}, {'end': 1197, 'start': 1195.9, 'text': ' 고민을 좀 했죠.'}, {'end': 1198, 'start': 1197, 'text': ' 뭘 해야 되나.'}, {'end': 1199.2, 'start': 1198, 'text': ' 고민을 하다가'}, {'end': 1201.7, 'start': 1199.2, 'text': ' 더불어 약간 노랭이라는 인간에 대해서'}, {'end': 1203.4, 'start': 1201.7, 'text': ' 생각을 해봤습니다.'}, {'end': 1204.8, 'start': 1203.4, 'text': ' 이 친구가 없었더라면'}, {'end': 1207.3, 'start': 1204.8, 'text': ' 제 인생이 좀 많이 달라지지 않았을까.'}, {'end': 1208.2, 'start': 1207.3, 'text': ' 그런 생각을 해서'}, {'end': 1210.1, 'start': 1208.2, 'text': ' 사실 노랭이와의 미담'}, {'end': 1211.8, 'start': 1210.1, 'text': ' 없어요.'}, {'end': 1214.4, 'start': 1211.8, 'text': ' 그냥 노랭이와 함께 살아온 인생이'}, {'end': 1216.3, 'start': 1214.4, 'text': ' 네 생이었다.'}, {'end': 1217.7, 'start': 1216.3, 'text': ' 아름다운 인생.'}, {'end': 1219.5, 'start': 1217.7, 'text': ' 노랭이를 처음 만났던 그날'}, {'end': 1223, 'start': 1219.5, 'text': ' 방송부에서 웬 도실도실한 아이가 한 명 있어가지고'}, {'end': 1225.1, 'start': 1223, 'text': ' 나는 1학년 5반의 박강훈이다.'}, {'end': 1226.9, 'start': 1225.1, 'text': ' 시작.'}, {'end': 1228.7, 'start': 1226.9, 'text': ' 저 멘트가 특이했어.'}, {'end': 1231.1, 'start': 1228.7, 'text': ' 보통 고등학생 때 친구 만나면'}, {'end': 1233.1, 'start': 1231.1, 'text': ' 어, 반갑다 이렇게만 하지.'}, {'end': 1234, 'start': 1233.1, 'text': ' 갑자기 와가지고'}, {'end': 1236.4, 'start': 1234, 'text': ' 어, 나는 1학년 5반의 박강훈이다.'}, {'end': 1238.5, 'start': 1236.4, 'text': ' 그냥 외국식 인사인데.'}, {'end': 1240.7, 'start': 1238.5, 'text': ' 이 역사가 아주 힙합이다.'}, {'end': 1241.9, 'start': 1240.7, 'text': ' 전설의 시작이었네요.'}, {'end': 1243.5, 'start': 1241.9, 'text': ' 전설의 시작.'}, {'end': 1245.3, 'start': 1243.5, 'text': ' 지가 지 입으로.'}, {'end': 1247.5, 'start': 1245.3, 'text': ' 전설의 시작이었잖아.'}, {'end': 1250.7, 'start': 1247.7, 'text': ' 옛날에 책 인터뷰에서 쓴 거 있잖아.'}, {'end': 1253.4, 'start': 1250.7, 'text': ' 그 내용을 내가 한번 봤거든. 얘 파트를.'}, {'end': 1255.5, 'start': 1253.4, 'text': ' 얘가 팥뿌리를 너무 좋아합니다.'}, {'end': 1260.1, 'start': 1255.5, 'text': ' 우리를 만난 거를 자기 인생에서 복이라고 생각하는데'}, {'end': 1263.2, 'start': 1260.1, 'text': ' 얘가 원래 겉으로 그런 표현을 잘 안 한단 말이야.'}, {'end': 1266.5, 'start': 1263.2, 'text': ' 다 같이 하는 활동들이 좀 내가 좋아하는 스타일이었거든.'}, {'end': 1267.9, 'start': 1266.5, 'text': ' 게임하고 만들고'}, {'end': 1268.5, 'start': 1267.9, 'text': ' 맞아 맞아.'}, {'end': 1269.9, 'start': 1268.5, 'text': ' 그러는 게 좋더라고.'}, {'end': 1273.1, 'start': 1269.9, 'text': ' 결국에 그렇게 놀던 게 팥뿌리가 되었다.'}, {'end': 1276.9, 'start': 1273.1, 'text': ' 그래서 그냥 요즘에 노랭이가 집에 갈 때 좀 걸어가요.'}, {'end': 1281.7, 'start': 1277.7, 'text': ' 그러면서 평소보다 많이 쓰는 게 에어콘이에요.'}, {'end': 1284.9, 'start': 1281.7, 'text': ' 하루에 1시간 이상은 꼭 이걸 끼고 있더라고.'}, {'end': 1288.7, 'start': 1284.9, 'text': ' 그래서 설마 좋은 소리 들으라고?'}, {'end': 1291.7, 'start': 1288.7, 'text': ' 내가 어제 쓴 이 프로라는 거.'}, {'end': 1293.7, 'start': 1291.7, 'text': ' 와, 소리 너무 많이 나온 거 아이가.'}, {'end': 1294.7, 'start': 1293.7, 'text': ' 스노우보이터.'}, {'end': 1295.7, 'start': 1294.7, 'text': ' 스노우보이터.'}, {'end': 1296.7, 'start': 1295.7, 'text': ' 스노우보이터.'}, {'end': 1301.7, 'start': 1300.7, 'text': ' 고급스럽다.'}, {'end': 1306.7, 'start': 1305.7, 'text': ' 네 목소리만 들려.'}, {'end': 1312.7, 'start': 1307.7, 'text': ' 주변 소리 듣기 하니까 너희 목소리가 그냥 암기 것처럼 잘 들려.'}, {'end': 1313.7, 'start': 1312.7, 'text': ' 비교해봐.'}, {'end': 1315.7, 'start': 1313.7, 'text': ' 와 와 와 와 와.'}, {'end': 1317.7, 'start': 1315.7, 'text': ' 나만 켜.'}, {'end': 1318.7, 'start': 1317.7, 'text': ' 안 들려.'}, {'end': 1320.7, 'start': 1318.7, 'text': ' 우와.'}, {'end': 1322.7, 'start': 1320.7, 'text': ' 우와 하는 건 들려.'}, {'end': 1323.7, 'start': 1322.7, 'text': ' 우와 하는 건 들려.'}, {'end': 1324.7, 'start': 1323.7, 'text': ' 신기하다.'}, {'end': 1325.7, 'start': 1324.7, 'text': ' 신기하다.'}, {'end': 1327.7, 'start': 1325.7, 'text': ' 말소리가 아예 다 차단되는 건 아닌가 봐.'}, {'end': 1329.7, 'start': 1327.7, 'text': ' 적당히.'}, {'end': 1330.7, 'start': 1329.7, 'text': ' 와.'}, {'end': 1331.7, 'start': 1330.7, 'text': ' 와.'}, {'end': 1332.7, 'start': 1331.7, 'text': ' 와.'}, {'end': 1334.7, 'start': 1332.7, 'text': ' 이쪽에는 또 음악이 하는 단지에 나온다.'}, {'end': 1335.7, 'start': 1334.7, 'text': ' 말 들리나?'}, {'end': 1336.7, 'start': 1335.7, 'text': ' 말 들리나?'}, {'end': 1337.7, 'start': 1336.7, 'text': ' 우리말이 들리나?'}, {'end': 1339.7, 'start': 1337.7, 'text': ' 자기만의 세계로 맞춰서.'}, {'end': 1342.7, 'start': 1339.7, 'text': ' 와 이 에어팟이랑 베이스가 좀 다른데?'}, {'end': 1343.7, 'start': 1342.7, 'text': ' 아 그래요?'}, {'end': 1344.7, 'start': 1343.7, 'text': ' 진짜.'}, {'end': 1345.7, 'start': 1344.7, 'text': ' 아 들어봐봐.'}, {'end': 1352.7, 'start': 1351.7, 'text': ' 진짜 안 들리네?'}, {'end': 1353.7, 'start': 1352.7, 'text': ' 안 들리지?'}, {'end': 1359.7, 'start': 1358.7, 'text': ' 감사합니다.'}, {'end': 1360.7, 'start': 1359.7, 'text': ' 축하합니다.'}, {'end': 1363.7, 'start': 1362.7, 'text': ' 감사합니다.'}, {'end': 1365.7, 'start': 1363.7, 'text': ' 너네도 그냥 알차게 맞춤형 선물로.'}, {'end': 1366.7, 'start': 1365.7, 'text': ' 친구들이.'}, {'end': 1368.7, 'start': 1366.7, 'text': ' 오 진짜 제일 단다.'}, {'end': 1369.7, 'start': 1368.7, 'text': ' 제일 단다.'}, {'end': 1370.7, 'start': 1369.7, 'text': ' 제일 단다.'}, {'end': 1371.7, 'start': 1370.7, 'text': ' 자 충전하고 왔습니다.'}, {'end': 1372.7, 'start': 1371.7, 'text': ' 켜볼까나.'}, {'end': 1373.7, 'start': 1372.7, 'text': ' 오 궁금해.'}, {'end': 1374.7, 'start': 1373.7, 'text': ' 오 제일 단 소리.'}, {'end': 1375.7, 'start': 1374.7, 'text': ' 이게 이제 시계 화면인 거고.'}, {'end': 1376.7, 'start': 1375.7, 'text': ' 그리고 이제 게임 버튼을 누르면 게임을 할 수 있습니다.'}, {'end': 1377.7, 'start': 1376.7, 'text': ' 와우 와우.'}, {'end': 1378.7, 'start': 1377.7, 'text': ' 아우 게임에 빠져들었어.'}, {'end': 1379.7, 'start': 1378.7, 'text': ' 그만해.'}, {'end': 1380.7, 'start': 1379.7, 'text': ' 그만해.'}, {'end': 1381.7, 'start': 1380.7, 'text': ' 그만해.'}, {'end': 1382.7, 'start': 1381.7, 'text': ' 마무리할게.'}, {'end': 1383.7, 'start': 1382.7, 'text': ' 그만해.'}, {'end': 1384.7, 'start': 1383.7, 'text': ' 그만해.'}, {'end': 1385.7, 'start': 1384.7, 'text': ' 그만해.'}, {'end': 1386.7, 'start': 1385.7, 'text': ' 그만해.'}, {'end': 1387.7, 'start': 1386.7, 'text': ' 그만해.'}, {'end': 1388.7, 'start': 1387.7, 'text': ' 그만해.'}, {'end': 1389.7, 'start': 1388.7, 'text': ' 그만해.'}, {'end': 1390.7, 'start': 1389.7, 'text': ' 그만해.'}, {'end': 1391.7, 'start': 1390.7, 'text': ' 그만해.'}, {'end': 1392.7, 'start': 1391.7, 'text': ' 그만해.'}, {'end': 1393.7, 'start': 1392.7, 'text': ' 그만해.'}, {'end': 1394.7, 'start': 1393.7, 'text': ' 그만해.'}, {'end': 1395.7, 'start': 1394.7, 'text': ' 그만해.'}, {'end': 1396.7, 'start': 1395.7, 'text': ' 그냥 끝나고.'}, {'end': 1397.7, 'start': 1396.7, 'text': ' 끝나고 이거 네고 먹으면서.'}, {'end': 1398.7, 'start': 1397.7, 'text': ' 끝나네.'}, {'end': 1399.7, 'start': 1398.7, 'text': ' 아 너무 감사합니다.'}, {'end': 1400.7, 'start': 1399.7, 'text': ' 재밌었어.'}, {'end': 1403.62, 'start': 1400.7, 'text': ' 네 어쨌든 또 우리 노랭이의 생일.'}, {'end': 1405.92, 'start': 1403.62, 'text': ' 우리 뽀둥이분들 많이많이 축하해 주시고.'}, {'end': 1408.22, 'start': 1405.92, 'text': ' 다음 생일 진열이 생일에.'}, {'end': 1409.22, 'start': 1408.22, 'text': ' 와.'}, {'end': 1410.56, 'start': 1409.22, 'text': ' 좀 많이 남았어 아직.'}, {'end': 1411.56, 'start': 1410.56, 'text': ' 네.'}, {'end': 1414.14, 'start': 1411.56, 'text': ' 그때 한번 어떤 파티를 해볼지.'}, {'end': 1415.48, 'start': 1414.14, 'text': ' 진열이가 한번 생각해 보시죠.'}, {'end': 1416.16, 'start': 1415.48, 'text': ' 제가 이렇게 놀아보자.'}, {'end': 1455.68, 'start': 1425.7, 'text': ' 시청해주셔서 감사합니다.'}], 'summary_result': \"- 🎉 오늘은 노랭이의 생일입니다!\\n- 🍣 노랭이가 가장 좋아하는 음식은 김밥입니다.\\n- 👩\\u200d🍳 우리는 노랭이의 생일을 기념하여 김밥을 만들기로 했습니다.\\n- 🥳 김밥의 맛보다 함께하는 시간이 더 중요하다고 이야기했습니다.\\n- 📜 생일 룰로 욕과 외래어 사용 금지가 정해졌습니다.\\n- 🍤 노랭이의 어머니가 만든 김밥을 그리워하며 준비했습니다.\\n- 🎁 친구들이 선물과 미담을 나누며 생일을 축하했습니다.\\n- 🍲 떡볶이와 함께 김밥을 즐기며 다 같이 재미있게 놀았습니다.\\n\\n- 🎉 Today is Norangi's birthday!\\n- 🍣 Norangi's favorite food is gimbap.\\n- 👩\\u200d🍳 We decided to make gimbap to celebrate Norangi's birthday.\\n- 🥳 We talked about the importance of time together over the taste of gimbap.\\n- 📜 The birthday rules included a ban on swearing and foreign words.\\n- 🍤 We prepared gimbap while reminiscing about Norangi's mother's cooking.\\n- 🎁 Friends shared gifts and compliments to celebrate the birthday.\\n- 🍲 We enjoyed gimbap along with tteokbokki and had fun together.\"}, 'status': 'COMPLETED', 'workerId': 'jxd93hs8fbpe4z'}\n",
      "Error fetching results: 404\n",
      "Error message: 404 page not found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/run\"\n",
    "\n",
    "# FastAPI의 /hello 엔드포인트로 요청하기 위한 데이터\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/get_script_summary\",\n",
    "        \"method\": \"GET\",\n",
    "        \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"url\": you_url},\n",
    "    }\n",
    "}\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RUNSYNC 요청 보내기\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "# 응답 확인\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Initial Response:\", result)\n",
    "    \n",
    "    if result.get('status') in ['IN_PROGRESS',\"IN_QUEUE\"]:\n",
    "        job_id = result.get('id')\n",
    "        status_url = f\"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}\"\n",
    "        \n",
    "        while True:\n",
    "            status_response = requests.get(status_url, headers=headers)\n",
    "            if status_response.status_code == 200:\n",
    "                status_data = status_response.json()\n",
    "                print(f\"Current status: {status_data.get('status')}\")\n",
    "                \n",
    "                if status_data.get('status') == 'COMPLETED':\n",
    "                    print(f\"결과값:{status_data}\")\n",
    "                    result_url = f\"https://api.runpod.ai/v2/{endpoint_id}/result/{job_id}\"\n",
    "                    result_response = requests.get(result_url, headers=headers)\n",
    "                    \n",
    "                    if result_response.status_code == 200:\n",
    "                        final_result = result_response.json()\n",
    "                        print(\"Final Result:\", final_result)\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Error fetching results: {result_response.status_code}\")\n",
    "                        print(f\"Error message: {result_response.text}\")\n",
    "                        break\n",
    "                elif status_data.get('status') == 'FAILED':\n",
    "                    print(\"Job failed\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Error checking status: {status_response.status_code}\")\n",
    "                print(f\"Error message: {status_response.text}\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)  # 5초 대기 후 다시 상태 확인\n",
    "    else:\n",
    "        print(\"Job completed immediately\")\n",
    "        print(\"Final Result:\", result)\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"RUNPOD_API_KEY\")\n",
    "endpoint_id = os.getenv(\"RUNPOD_ENDPOINT_ID\")\n",
    "\n",
    "# RunPod RUNSYNC 엔드포인트 URL\n",
    "url = f\"https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}\"\n",
    "url2 = f\"https://api.runpod.ai/v2/{endpoint_id}/result/{job_id}\"\n",
    "\n",
    "# 요청 헤더에 API 키 추가\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "while True:\n",
    "    # RUNSYNC 요청 보내기\n",
    "    response = requests.get(url,headers=headers)\n",
    "\n",
    "    # 응답 확인\n",
    "    if response.status_code == 200:\n",
    "        if response.json().get(\"status\") == \"COMPLETED\":\n",
    "            response = requests.get(url2,headers=headers)\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Job status: {response.json().get('status')}\")\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'404 page not found'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "RUNPOD_API_URL = f\"https://api.runpod.ai/v2/{endpoint_id}/runsync\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "payload = {\n",
    "    \"input\": {\n",
    "        \"endpoint\": \"/rag_stream_chat\",\n",
    "        \"method\": \"POST\",\n",
    "        \"headers\": {\"x-session-id\": \"1234asdf\"},\n",
    "        \"params\": {\"prompt\": \"영상의 주제가 뭔가요?\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    RUNPOD_API_URL, headers=headers, json=payload, stream=True\n",
    ")\n",
    "\n",
    "# for chunk in response.iter_content(chunk_size=None):\n",
    "#     if chunk:\n",
    "#         chunk_data = chunk.decode(\"utf-8\").strip()\n",
    "#         if chunk_data.startswith(\"data: \"):\n",
    "#             chunk_content = chunk_data[6:]\n",
    "#             if chunk_content == \"[DONE]\":\n",
    "#                 break\n",
    "#             try:\n",
    "#                 content = json.loads(chunk_content)\n",
    "#                 print(\"Stream content:\", content)\n",
    "#             except json.JSONDecodeError:\n",
    "#                 print(\"Invalid JSON:\", chunk_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "영상\n",
      "영상의\n",
      "영상의 주\n",
      "영상의 주제\n",
      "영상의 주제는\n",
      "영상의 주제는 까\n",
      "영상의 주제는 까르\n",
      "영상의 주제는 까르보\n",
      "영상의 주제는 까르보나라\n",
      "영상의 주제는 까르보나라 요\n",
      "영상의 주제는 까르보나라 요리\n",
      "영상의 주제는 까르보나라 요리 과정\n",
      "영상의 주제는 까르보나라 요리 과정에\n",
      "영상의 주제는 까르보나라 요리 과정에 대한\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다.\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨,\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란,\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루고\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루고 있습니다\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루고 있습니다.\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루고 있습니다.\n",
      "영상의 주제는 까르보나라 요리 과정에 대한 내용입니다. 영상에서는 베이컨, 계란, 면수 등을 사용하여 맛있는 까르보나라를 만드는 방법을 다루고 있습니다.[DONE]\n"
     ]
    }
   ],
   "source": [
    "answer = \"\"\n",
    "for chunk in response.json().get(\"output\"):\n",
    "    answer += chunk.get(\"content\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"script.txt\",\"r\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the last chapter, you and I started to step through the internal workings of a transformer.\n",
      "This is one of the key pieces of technology inside large language models, and a lot of\n",
      "other tools in the modern wave of AI.\n",
      "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and\n",
      "in this chapter, you and I will dig into what this attention mechanism is, visualizing how\n",
      "it processes data.\n",
      "As a quick recap, here's the important context I want you to have in mind.\n",
      "The goal of the model that you and I are studying is to take in a piece of text and predict\n",
      "what word comes next.\n",
      "The input text is broken up into little pieces that we call tokens, and these are very often\n",
      "words or pieces of words, but just to make the examples in this video easier for you\n",
      "and me to think about, let's simplify by pretending that tokens are always just words.\n",
      "The first step in a transformer is to associate each token with a high-dimensional vector,\n",
      "what we call its embedding.\n",
      "Now the most important idea I want you to have in mind is how directions in this high-dimensional\n",
      "space of all possible embeddings can correspond with semantic meaning.\n",
      "In the last chapter we saw an example for how direction can correspond to gender, in\n",
      "the sense that adding a certain step in this space can take you from the embedding of a\n",
      "masculine noun to the embedding of the corresponding feminine noun.\n",
      "just one example, you could imagine how many other directions in this high-dimensional space\n",
      "could correspond to numerous other aspects of a word's meaning. The aim of a transformer is to\n",
      "progressively adjust these embeddings so that they don't merely encode an individual word,\n",
      "but instead they bake in some much, much richer contextual meaning. I should say up front that a\n",
      "lot of people find the attention mechanism, this key piece in a transformer, very confusing, so\n",
      "don't worry if it takes some time for things to sink in. I think that before we dive into the\n",
      "computational details and all the matrix multiplications, it's worth thinking about a\n",
      "couple examples for the kind of behavior that we want attention to enable. Consider the phrases\n",
      "American true mole, one mole of carbon dioxide, and take a biopsy of the mole. You and I know\n",
      "that the word mole has different meanings in each one of these, based on the context.\n",
      "But after the first step of a transformer, the one that breaks up the text and associates each\n",
      "token with a vector, the vector that's associated with mole would be the same in all three of these\n",
      "cases, because this initial token embedding is effectively a lookup table with no reference to\n",
      "the context. It's only in the next step of the transformer that the surrounding embeddings have\n",
      "the chance to pass information into this one. The picture you might have in mind is that there\n",
      "are multiple distinct directions in this embedding space encoding the multiple distinct meanings of\n",
      "the word mole, and that a well-trained attention block calculates what you need to add to the\n",
      "generic embedding to move it to one of these more specific directions, as a function of the context.\n",
      "To take another example, consider the embedding of the word tower. This is presumably some very\n",
      "generic, non-specific direction in the space, associated with lots of other large, tall nouns.\n",
      "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update\n",
      "this vector so that it points in a direction that more specifically encodes the Eiffel Tower,\n",
      "maybe correlated with vectors associated with Paris and France and things made of steel.\n",
      "If it was also preceded by the word miniature, then the vector should be updated even further\n",
      "so that it no longer correlates with large tall things. More generally than just refining the\n",
      "meaning of a word, the attention block allows the model to move information encoded in one\n",
      "embedding to that of another, potentially ones that are quite far away, and potentially\n",
      "with information that's much richer than just a single word.\n",
      "What we saw in the last chapter was how after all of the vectors flow through the network,\n",
      "including many different attention blocks, the computation that you perform to produce\n",
      "a prediction of the next token is entirely a function of the last vector in the sequence.\n",
      "So imagine, for example, that the text you input is most of an entire mystery novel,\n",
      "way up to a point near the end which reads, therefore the murderer was, if the model is\n",
      "going to accurately predict the next word, that final vector in the sequence which began its life\n",
      "simply embedding the word was will have to have been updated by all of the attention blocks\n",
      "to represent much much more than any individual word, somehow encoding all of the information\n",
      "from the full context window that's relevant to predicting the next word. To step through the\n",
      "the computations though let's take a much simpler example. Imagine that the input includes the\n",
      "phrase a fluffy blue creature roamed the verdant forest and for the moment suppose that the only\n",
      "type of update that we care about is having the adjectives adjust the meanings of their\n",
      "corresponding nouns. What I'm about to describe is what we would call a single head of attention\n",
      "and later we will see how the attention block consists of many different heads run in parallel.\n",
      "Again, the initial embedding for each word is some high-dimensional vector\n",
      "that only encodes the meaning of that particular word with no context.\n",
      "Actually, that's not quite true. They also encode the position of the word.\n",
      "There's a lot more to say about the specific way that positions are encoded,\n",
      "but right now all you need to know is that the entries of this vector are enough to tell you\n",
      "both what the word is and where it exists in the context. Let's go ahead and denote these\n",
      "embeddings with the letter E, the goal is to have a series of computations produce a\n",
      "new refined set of embeddings where, for example, those corresponding to the nouns have ingested\n",
      "the meaning from their corresponding adjectives.\n",
      "And playing the deep learning game, we want most of the computations involved to look\n",
      "like matrix-vector products where the matrices are full of tunable weights, things that the\n",
      "model will learn based on data.\n",
      "To be clear, I'm making up this example of adjectives updating nouns just to illustrate\n",
      "the type of behavior that you could imagine an intention had doing.\n",
      "As with so much deep learning, the true behavior is much harder to parse, because it's based\n",
      "on tweaking and tuning a huge number of parameters to minimize some cost function.\n",
      "It's just that as we step through all of the different matrices filled with parameters\n",
      "that are involved in this process, I think it's really helpful to have an imagined example\n",
      "of something that it could be doing to help keep it all more concrete.\n",
      "For the first step of this process, you might imagine each noun, like creature, asking the\n",
      "question, hey, are there any adjectives sitting in front of me, and for the words fluffy and\n",
      "blue to each be able to answer, yeah, I'm an adjective and I'm in that position.\n",
      "That question is somehow encoded as yet another vector, another list of numbers, which we\n",
      "call the query for this word.\n",
      "This query vector, though, has a much smaller dimension than the embedding vector, say 128.\n",
      "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying\n",
      "it by the embedding.\n",
      "Compressing things a bit, let's write that query vector as q, and then anytime you see\n",
      "me put a matrix next to an arrow like this one, it's meant to represent that multiplying\n",
      "this matrix by the vector at the arrow's start gives you the vector at the arrow's end.\n",
      "In this case, you multiply this matrix by all of the embeddings in the context, producing\n",
      "one query vector for each token.\n",
      "The entries of this matrix are parameters of the model, which means the true behavior\n",
      "is learned from data, and in practice what this matrix does in a particular attention\n",
      "head is challenging to parse.\n",
      "But for our sake, imagining an example that we might hope it would learn, we'll suppose\n",
      "that this query matrix maps the embeddings of nouns to certain directions in this smaller\n",
      "query space that somehow encodes the notion of looking for adjectives in preceding positions.\n",
      "As to what it does to other embeddings, who knows, maybe it simultaneously tries to accomplish\n",
      "some other goal with those, right now we're laser focused on the nouns.\n",
      "At the same time, associated with this is a second matrix called the key matrix, which\n",
      "you also multiply by every one of the embeddings.\n",
      "This produces a second sequence of vectors that we call the keys.\n",
      "Conceptually you want to think of the keys as potentially answering the queries.\n",
      "This key matrix is also full of tunable parameters, and just like the query matrix it maps the\n",
      "embedding vectors to that same smaller dimensional space.\n",
      "You think of the keys as matching the queries whenever they closely align with each other.\n",
      "In our example, you would imagine that the key matrix maps the adjectives, like fluffy\n",
      "and blue, to vectors that are closely aligned with the query produced by the word creature.\n",
      "To measure how well each key matches each query, you compute a dot product between each\n",
      "possible key-query pair.\n",
      "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond\n",
      "the larger dot products, the places where the keys and queries align. For our adjective-noun example,\n",
      "that would look a little more like this, where if the keys produced by fluffy and blue really do\n",
      "align closely with the query produced by creature, then the dot products in these two spots would be\n",
      "some large positive numbers. In the lingo, machine learning people would say that this means the\n",
      "embeddings of fluffy and blue attend to the embedding of creature. By contrast to the dot\n",
      "product between the key for some other word like the and the query for creature would be some small\n",
      "or negative value that reflects that these are unrelated to each other. So we have this grid of\n",
      "values that can be any real number from negative infinity to infinity giving us a score for how\n",
      "relevant each word is to updating the meaning of every other word. The way we're about to use these\n",
      "scores is to take a certain weighted sum along each column weighted by the relevance. So instead\n",
      "Instead of having values range from negative infinity to infinity, what we want is for\n",
      "the numbers in these columns to be between 0 and 1, and for each column to add up to\n",
      "1, as if they were a probability distribution.\n",
      "If you're coming in from the last chapter, you know what we need to do then.\n",
      "We compute a softmax along each one of these columns to normalize the values.\n",
      "In our picture, after you apply softmax to all of the columns, we'll fill in the grid\n",
      "with these normalized values.\n",
      "At this point, you're safe to think about each column as giving weights\n",
      "according to how relevant the word on the left is to the corresponding value at the top.\n",
      "We call this grid an attention pattern.\n",
      "Now, if you look at the original Transformer paper,\n",
      "there's a really compact way that they write this all down.\n",
      "Here, the variables q and k represent the full arrays of query and key vectors respectively,\n",
      "those little vectors you get by multiplying the embeddings by the query and the key matrices.\n",
      "This expression up in the numerator is a really compact way to represent the grid of all possible\n",
      "dot products between pairs of keys and queries. A small technical detail that I didn't mention\n",
      "is that for numerical stability it happens to be helpful to divide all of these values by the\n",
      "square root of the dimension in that key query space. Then this softmax that's wrapped around\n",
      "the full expression, is meant to be understood to apply column by column.\n",
      "As to that V term, we'll talk about it in just a second.\n",
      "Before that, there's one other technical detail that so far I've skipped.\n",
      "During the training process, when you run this model on a given text example, and all\n",
      "of the weights are slightly adjusted and tuned to either reward or punish it based on how\n",
      "high a probability it assigns to the true next word in the passage, it turns out to\n",
      "make the whole training process a lot more efficient if you simultaneously have it predict\n",
      "every possible next token following each initial sub-sequence of tokens in this passage.\n",
      "For example, with the phrase that we've been focusing on, it might also be predicting what\n",
      "words follow creature, and what words follow the.\n",
      "This is really nice, because it means what would otherwise be a single training example\n",
      "effectively acts as many.\n",
      "For the purposes of our attention pattern, it means that you never want to allow later\n",
      "words to influence earlier words, since otherwise they could kind of give away the answer for\n",
      "what comes next. What this means is that we want all of these spots here, the ones representing\n",
      "later tokens influencing earlier ones, to somehow be forced to be zero. The simplest thing you might\n",
      "think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one\n",
      "anymore, they wouldn't be normalized. So instead a common way to do this is that before applying\n",
      "softmax you set all of those entries to be negative infinity. If you do that then after\n",
      "After applying softmax, all of those get turned into zero, but the columns stay normalized.\n",
      "This process is called masking.\n",
      "There are versions of attention where you don't apply it, but in our GPT example, even\n",
      "though this is more relevant during the training phase than it would be, say, running it as\n",
      "a chatbot or something like that, you do always apply this masking to prevent later tokens\n",
      "from influencing earlier ones.\n",
      "Another fact that's worth reflecting on about this attention pattern is how its size is\n",
      "equal to the square of the context size.\n",
      "So this is why context size can be a really huge bottleneck for large language models,\n",
      "and scaling it up is non-trivial.\n",
      "As you might imagine, motivated by a desire for bigger and bigger context windows, recent\n",
      "years have seen some variations to the attention mechanism aimed at making context more scalable.\n",
      "But right here, you and I are staying focused on the basics.\n",
      "Okay, great, computing this pattern lets the model deduce which words are relevant to which\n",
      "other words.\n",
      "Now you need to actually update the embeddings, allowing words to pass information to whichever\n",
      "other words they're relevant to.\n",
      "For example, you want the embedding of fluffy to somehow cause a change to creature that\n",
      "moves it to a different part of this 12,000 dimensional embedding space that more specifically\n",
      "encodes a fluffy creature.\n",
      "What I'm going to do here is first show you the most straightforward way that you could\n",
      "do this, though there's a slight way that this gets modified in the context of multi-headed\n",
      "attention.\n",
      "This most straightforward way would be to use a third matrix, what we call the value\n",
      "matrix, which you multiply by the embedding of that first word, for example fluffy.\n",
      "The result of this is what you would call a value vector, and this is something that\n",
      "you add to the embedding of the second word, in this case something you add to the embedding\n",
      "of creature.\n",
      "So, this value vector lives in the same very high dimensional space as the embeddings.\n",
      "When you multiply this value matrix by the embedding of a word, you might think of it\n",
      "as saying if this word is relevant to adjusting the meaning of something else, what exactly should\n",
      "be added to the embedding of that something else in order to reflect this? Looking back in our\n",
      "diagram, let's set aside all of the keys and the queries, since after you compute the attention\n",
      "pattern you're done with those, then you're going to take this value matrix and multiply it by every\n",
      "one of those embeddings to produce a sequence of value vectors. You might think of these value\n",
      "vectors as being kind of associated with the corresponding keys.\n",
      "For each column in this diagram, you multiply each of the value vectors by the corresponding\n",
      "weight in that column.\n",
      "For example, here, under the embedding of creature, you would be adding large proportions\n",
      "of the value vectors for fluffy and blue, while all of the other value vectors get zeroed\n",
      "out, or at least nearly zeroed out.\n",
      "And then finally, the way to actually update the embedding associated with this column,\n",
      "previously encoding some context-free meaning of creature, you add together all of these\n",
      "rescaled values in the column, producing a change that you want to add that I'll label\n",
      "delta E, and then you add that to the original embedding.\n",
      "Hopefully what results is a more refined vector encoding the more contextually rich meaning,\n",
      "like that of a fluffy blue creature.\n",
      "And of course you don't just do this to one embedding, you apply the same weighted sum\n",
      "across all of the columns in this picture, producing a sequence of changes.\n",
      "Adding all of those changes to the corresponding embeddings produces a full sequence of more\n",
      "refined embeddings popping out of the attention block.\n",
      "Zooming out, this whole process is what you would describe as a single head of attention.\n",
      "As I've described things so far, this process is parameterized by three distinct matrices,\n",
      "all filled with tunable parameters, the key, the query, and the value.\n",
      "I want to take a moment to continue what we started in the last chapter with the scorekeeping\n",
      "where we count up the total number of model parameters using the numbers from GPT-3.\n",
      "These key and query matrices each have 12,288 columns, matching the embedding dimension,\n",
      "and 128 rows, matching the dimension of that smaller key query space.\n",
      "This gives us an additional 1.5 million or so parameters for each one.\n",
      "If you look at that value matrix by contrast, the way I've described things so far would\n",
      "suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both\n",
      "its inputs and its outputs live in this very large embedding space.\n",
      "If true, that would mean about 150 million added parameters.\n",
      "And to be clear, you could do that, you could devote orders of magnitude more parameters\n",
      "to the value map than to the key and query.\n",
      "But in practice, it is much more efficient if instead you make it so that the number\n",
      "of parameters devoted to this value map is the same as the number devoted to the key\n",
      "in the query.\n",
      "This is especially relevant in the setting of running multiple attention heads in parallel.\n",
      "The way this looks is that the value map is factored as a product of two smaller matrices.\n",
      "Conceptually, I would still encourage you to think about the overall linear map, one\n",
      "with inputs and outputs both in this larger embedding space, for example taking the embedding\n",
      "of blue to this blueness direction that you would add to nouns.\n",
      "It's just that it's broken up into two separate steps.\n",
      "The first matrix on the right here has a smaller number of rows, typically the same size as\n",
      "the key query space.\n",
      "What this means is you can think of it as mapping the large embedding vectors down to\n",
      "a much smaller space.\n",
      "This is not the conventional naming, but I'm going to call this the value down matrix.\n",
      "The second matrix maps from this smaller space back up to the embedding space, producing\n",
      "the vectors that you use to make the actual updates.\n",
      "I'm going to call this one the value-up matrix, which, again, is not conventional.\n",
      "The way that you would see this written in most papers looks a little different.\n",
      "I'll talk about it in a minute.\n",
      "In my opinion, it tends to make things a little more conceptually confusing.\n",
      "To throw in linear algebra jargon here, what we're basically doing is constraining the\n",
      "overall value map to be a low-rank transformation.\n",
      "Turning back to the parameter count, all four of these matrices have the same size, and\n",
      "Then adding them all up, we get about 6.3 million parameters for one attention head.\n",
      "As a quick side note, to be a little more accurate, everything described so far is what\n",
      "people would call a self-attention head, to distinguish it from a variation that comes\n",
      "up in other models that's called cross-attention.\n",
      "This isn't relevant to our GPT example, but if you're curious, cross-attention involves\n",
      "models that process two distinct types of data, like text in one language and text in\n",
      "another language that's part of an ongoing generation of a translation.\n",
      "Or maybe audio input of speech, and an ongoing transcription.\n",
      "A cross-attention head looks almost identical.\n",
      "The only difference is that the key and query maps act on different datasets.\n",
      "In a model doing translation, for example, the keys might come from one language, while\n",
      "the queries come from another, and the attention pattern could describe which words from one\n",
      "language correspond to which words in another.\n",
      "And in this setting there would typically be no masking, since there's not really any\n",
      "notion of later tokens affecting earlier ones.\n",
      "Staying focused on self-attention though, if you understood everything so far, and if\n",
      "you were to stop here, you would come away with the essence of what attention really\n",
      "is.\n",
      "All that's really left to us is to lay out the sense in which you do this many, many\n",
      "different times.\n",
      "In our central example we focused on adjectives updating nouns, but of course there are lots\n",
      "of different ways that context can influence the meaning of a word.\n",
      "If the words they crashed the preceded the word car, it has implications for the shape\n",
      "and the structure of that car, and a lot of associations might be less grammatical.\n",
      "If the word wizard is anywhere in the same passage as Harry, it suggests that this might\n",
      "be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were\n",
      "in that passage, then perhaps the embedding of Harry should instead be updated to refer\n",
      "to the prince.\n",
      "For every different type of contextual updating that you might imagine, the parameters of\n",
      "these key and query matrices would be different to capture the different attention patterns,\n",
      "and the parameters of our value map would be different based on what should be added to the\n",
      "embeddings. And again, in practice the true behavior of these maps is much more difficult\n",
      "to interpret, where the weights are set to do whatever the model needs them to do to best\n",
      "accomplish its goal of predicting the next token. As I said before, everything we described is a\n",
      "single head of attention, and a full attention block inside a transformer consists of what's\n",
      "called multi-headed attention where you run a lot of these operations in parallel each with its own\n",
      "distinct key query and value maps. GPT-3 for example uses 96 attention heads inside each block.\n",
      "Considering that each one is already a bit confusing it's certainly a lot to hold in your\n",
      "head. Just to spell it all out very explicitly this means you have 96 distinct key and query\n",
      "matrices producing 96 distinct attention patterns. Then each head has its own distinct value matrices\n",
      "used to produce 96 sequences of value vectors. These are all added together using the\n",
      "corresponding attention patterns as weights. What this means is that for each position in the\n",
      "context, each token, every one of these heads produces a proposed change to be added to the\n",
      "embedding in that position. So what you do is you sum together all of those proposed changes,\n",
      "one for each head, and you add the result to the original embedding of that position.\n",
      "This entire sum here would be one slice of what's outputted from this multi-headed attention block,\n",
      "a single one of those refined embeddings that pops out the other end of it.\n",
      "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.\n",
      "The overall idea is that by running many distinct heads in parallel,\n",
      "you're giving the model the capacity to learn many distinct ways that context changes meaning.\n",
      "Pulling up our running tally for parameter count with 96 heads, each including its own variation\n",
      "of these four matrices, each block of multi-headed attention ends up with around 600 million\n",
      "parameters. There's one added slightly annoying thing that I should really mention for any of you\n",
      "who go on to read more about transformers. You remember how I said that the value map is factored\n",
      "out into these two distinct matrices, which I labeled as the value down and the value up\n",
      "matrices. The way that I framed things would suggest that you see this pair of matrices\n",
      "inside each attention head, and you could absolutely implement it this way. That would\n",
      "be a valid design. But the way that you see this written in papers and the way that it's\n",
      "implemented in practice looks a little different. All of these value up matrices for each head\n",
      "appear stapled together in one giant matrix that we call the output matrix, associated with\n",
      "the entire multi-headed attention block. And when you see people refer to the value matrix for a\n",
      "given attention head, they're typically only referring to this first step, the one that I\n",
      "was labeling as the value down projection into the smaller space. For the curious among you,\n",
      "I've left an on-screen note about it. It's one of those details that runs the risk of distracting\n",
      "from the main conceptual points, but I do want to call it out just so that you know if you read\n",
      "about this in other sources. Setting aside all the technical nuances, in the preview from the\n",
      "last chapter, we saw how data flowing through a transformer doesn't just flow through a single\n",
      "attention block. For one thing, it also goes through these other operations called multi-layer\n",
      "perceptrons. We'll talk more about those in the next chapter. And then it repeatedly goes through\n",
      "many, many copies of both of these operations. What this means is that after a given word imbibes\n",
      "some of its context, there are many more chances for this more nuanced embedding to be influenced\n",
      "by its more nuanced surroundings. The further down the network you go, with each embedding\n",
      "taking in more and more meaning from all the other embeddings, which themselves are getting\n",
      "more and more nuanced, the hope is that there's the capacity to encode higher level and more\n",
      "abstract ideas about a given input beyond just descriptors and grammatical structure.\n",
      "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are\n",
      "are relevant to the piece, and things like that.\n",
      "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the\n",
      "total number of key, query, and value parameters is multiplied by another 96, which brings\n",
      "the total sum to just under 58 billion distinct parameters devoted to all of the attention\n",
      "heads.\n",
      "That is a lot, to be sure, but it's only about a third of the 175 billion that are\n",
      "in the network in total.\n",
      "So even though attention gets all of the attention, the majority of parameters come from the blocks\n",
      "sitting in between these steps.\n",
      "In the next chapter, you and I will talk more about those other blocks and also a lot more\n",
      "about the training process.\n",
      "A big part of the story for the success of the attention mechanism is not so much any\n",
      "specific kind of behavior that it enables, but the fact that it's extremely parallelizable,\n",
      "meaning that you can run a huge number of computations in a short time using GPUs.\n",
      "that one of the big lessons about deep learning in the last decade or two has been that scale\n",
      "alone seems to give huge qualitative improvements in model performance. There's a huge advantage to\n",
      "parallelizable architectures that let you do this. If you want to learn more about this stuff, I've\n",
      "left lots of links in the description. In particular, anything produced by Andre Karpathy or Chris Ola\n",
      "tend to be pure gold. In this video, I wanted to just jump into attention in its current form,\n",
      "but if you're curious about more of the history for how we got here and how you might reinvent\n",
      "this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of\n",
      "that motivation. Also, Britt Cruz from the channel The Art of the Problem\n",
      "has a really nice video about the history of large language models.\n",
      "you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "url = \"https://www.youtube.com/watch?v=yF_YIxxjWU4\"\n",
    "yt = YouTube(url, use_po_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the tool: https://github.com/YunzheZJU/youtube-po-token-generator, to get the token\n"
     ]
    }
   ],
   "source": [
    "yt.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
