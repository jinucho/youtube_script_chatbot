[
    {
        "text": "In the last chapter, you and I started to step through the internal workings of a transformer.",
        "start": 0.12,
        "end": 4.66
    },
    {
        "text": "This is one of the key pieces of technology inside large language models, and a lot of",
        "start": 4.66,
        "end": 8.52
    },
    {
        "text": "other tools in the modern wave of AI.",
        "start": 8.52,
        "end": 11.1
    },
    {
        "text": "It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and",
        "start": 11.1,
        "end": 15.88
    },
    {
        "text": "in this chapter, you and I will dig into what this attention mechanism is, visualizing how",
        "start": 15.88,
        "end": 20.64
    },
    {
        "text": "it processes data.",
        "start": 20.64,
        "end": 26.56
    },
    {
        "text": "As a quick recap, here's the important context I want you to have in mind.",
        "start": 26.56,
        "end": 30.2
    },
    {
        "text": "The goal of the model that you and I are studying is to take in a piece of text and predict",
        "start": 30.2,
        "end": 34.82
    },
    {
        "text": "what word comes next.",
        "start": 34.82,
        "end": 36.88
    },
    {
        "text": "The input text is broken up into little pieces that we call tokens, and these are very often",
        "start": 36.88,
        "end": 41.64
    },
    {
        "text": "words or pieces of words, but just to make the examples in this video easier for you",
        "start": 41.64,
        "end": 45.96
    },
    {
        "text": "and me to think about, let's simplify by pretending that tokens are always just words.",
        "start": 45.96,
        "end": 51.52
    },
    {
        "text": "The first step in a transformer is to associate each token with a high-dimensional vector,",
        "start": 51.52,
        "end": 56.6
    },
    {
        "text": "what we call its embedding.",
        "start": 56.6,
        "end": 58.28
    },
    {
        "text": "Now the most important idea I want you to have in mind is how directions in this high-dimensional",
        "start": 58.28,
        "end": 62.74
    },
    {
        "text": "space of all possible embeddings can correspond with semantic meaning.",
        "start": 62.74,
        "end": 67.72
    },
    {
        "text": "In the last chapter we saw an example for how direction can correspond to gender, in",
        "start": 67.72,
        "end": 71.8
    },
    {
        "text": "the sense that adding a certain step in this space can take you from the embedding of a",
        "start": 71.8,
        "end": 76.1
    },
    {
        "text": "masculine noun to the embedding of the corresponding feminine noun.",
        "start": 76.1,
        "end": 80.28
    },
    {
        "text": "just one example, you could imagine how many other directions in this high-dimensional space",
        "start": 80.28,
        "end": 84.3
    },
    {
        "text": "could correspond to numerous other aspects of a word's meaning. The aim of a transformer is to",
        "start": 84.3,
        "end": 90.54
    },
    {
        "text": "progressively adjust these embeddings so that they don't merely encode an individual word,",
        "start": 90.54,
        "end": 95.38
    },
    {
        "text": "but instead they bake in some much, much richer contextual meaning. I should say up front that a",
        "start": 95.76,
        "end": 101.18
    },
    {
        "text": "lot of people find the attention mechanism, this key piece in a transformer, very confusing, so",
        "start": 101.18,
        "end": 106.44
    },
    {
        "text": "don't worry if it takes some time for things to sink in. I think that before we dive into the",
        "start": 106.44,
        "end": 111.04
    },
    {
        "text": "computational details and all the matrix multiplications, it's worth thinking about a",
        "start": 111.04,
        "end": 115.04
    },
    {
        "text": "couple examples for the kind of behavior that we want attention to enable. Consider the phrases",
        "start": 115.04,
        "end": 120.9
    },
    {
        "text": "American true mole, one mole of carbon dioxide, and take a biopsy of the mole. You and I know",
        "start": 120.9,
        "end": 127.32
    },
    {
        "text": "that the word mole has different meanings in each one of these, based on the context.",
        "start": 127.32,
        "end": 130.86
    },
    {
        "text": "But after the first step of a transformer, the one that breaks up the text and associates each",
        "start": 131.4,
        "end": 135.4
    },
    {
        "text": "token with a vector, the vector that's associated with mole would be the same in all three of these",
        "start": 135.4,
        "end": 140.6
    },
    {
        "text": "cases, because this initial token embedding is effectively a lookup table with no reference to",
        "start": 140.6,
        "end": 145.58
    },
    {
        "text": "the context. It's only in the next step of the transformer that the surrounding embeddings have",
        "start": 145.58,
        "end": 150.9
    },
    {
        "text": "the chance to pass information into this one. The picture you might have in mind is that there",
        "start": 150.9,
        "end": 155.52
    },
    {
        "text": "are multiple distinct directions in this embedding space encoding the multiple distinct meanings of",
        "start": 155.52,
        "end": 160.52
    },
    {
        "text": "the word mole, and that a well-trained attention block calculates what you need to add to the",
        "start": 160.52,
        "end": 165.82
    },
    {
        "text": "generic embedding to move it to one of these more specific directions, as a function of the context.",
        "start": 165.82,
        "end": 171.74
    },
    {
        "text": "To take another example, consider the embedding of the word tower. This is presumably some very",
        "start": 172.9,
        "end": 178.16
    },
    {
        "text": "generic, non-specific direction in the space, associated with lots of other large, tall nouns.",
        "start": 178.16,
        "end": 183.42
    },
    {
        "text": "If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update",
        "start": 183.98,
        "end": 189.54
    },
    {
        "text": "this vector so that it points in a direction that more specifically encodes the Eiffel Tower,",
        "start": 189.54,
        "end": 194.66
    },
    {
        "text": "maybe correlated with vectors associated with Paris and France and things made of steel.",
        "start": 194.66,
        "end": 198.98
    },
    {
        "text": "If it was also preceded by the word miniature, then the vector should be updated even further",
        "start": 199.78,
        "end": 204.74
    },
    {
        "text": "so that it no longer correlates with large tall things. More generally than just refining the",
        "start": 204.74,
        "end": 211.06
    },
    {
        "text": "meaning of a word, the attention block allows the model to move information encoded in one",
        "start": 211.06,
        "end": 215.94
    },
    {
        "text": "embedding to that of another, potentially ones that are quite far away, and potentially",
        "start": 215.94,
        "end": 220.62
    },
    {
        "text": "with information that's much richer than just a single word.",
        "start": 220.62,
        "end": 224.1
    },
    {
        "text": "What we saw in the last chapter was how after all of the vectors flow through the network,",
        "start": 224.1,
        "end": 228.3
    },
    {
        "text": "including many different attention blocks, the computation that you perform to produce",
        "start": 228.3,
        "end": 232.26
    },
    {
        "text": "a prediction of the next token is entirely a function of the last vector in the sequence.",
        "start": 232.26,
        "end": 239.14
    },
    {
        "text": "So imagine, for example, that the text you input is most of an entire mystery novel,",
        "start": 239.14,
        "end": 243.82
    },
    {
        "text": "way up to a point near the end which reads, therefore the murderer was, if the model is",
        "start": 243.82,
        "end": 249.02
    },
    {
        "text": "going to accurately predict the next word, that final vector in the sequence which began its life",
        "start": 249.02,
        "end": 254.3
    },
    {
        "text": "simply embedding the word was will have to have been updated by all of the attention blocks",
        "start": 254.3,
        "end": 259.18
    },
    {
        "text": "to represent much much more than any individual word, somehow encoding all of the information",
        "start": 259.18,
        "end": 264.54
    },
    {
        "text": "from the full context window that's relevant to predicting the next word. To step through the",
        "start": 264.54,
        "end": 269.98
    },
    {
        "text": "the computations though let's take a much simpler example. Imagine that the input includes the",
        "start": 269.98,
        "end": 274.64
    },
    {
        "text": "phrase a fluffy blue creature roamed the verdant forest and for the moment suppose that the only",
        "start": 274.64,
        "end": 280.48
    },
    {
        "text": "type of update that we care about is having the adjectives adjust the meanings of their",
        "start": 280.48,
        "end": 285.24
    },
    {
        "text": "corresponding nouns. What I'm about to describe is what we would call a single head of attention",
        "start": 285.24,
        "end": 290.2
    },
    {
        "text": "and later we will see how the attention block consists of many different heads run in parallel.",
        "start": 290.2,
        "end": 295.38
    },
    {
        "text": "Again, the initial embedding for each word is some high-dimensional vector",
        "start": 296.02,
        "end": 299.62
    },
    {
        "text": "that only encodes the meaning of that particular word with no context.",
        "start": 299.62,
        "end": 303.86
    },
    {
        "text": "Actually, that's not quite true. They also encode the position of the word.",
        "start": 303.86,
        "end": 307.94
    },
    {
        "text": "There's a lot more to say about the specific way that positions are encoded,",
        "start": 307.94,
        "end": 311.46
    },
    {
        "text": "but right now all you need to know is that the entries of this vector are enough to tell you",
        "start": 311.46,
        "end": 315.46
    },
    {
        "text": "both what the word is and where it exists in the context. Let's go ahead and denote these",
        "start": 315.46,
        "end": 320.42
    },
    {
        "text": "embeddings with the letter E, the goal is to have a series of computations produce a",
        "start": 320.42,
        "end": 325.12
    },
    {
        "text": "new refined set of embeddings where, for example, those corresponding to the nouns have ingested",
        "start": 325.12,
        "end": 330.84
    },
    {
        "text": "the meaning from their corresponding adjectives.",
        "start": 330.84,
        "end": 333.94
    },
    {
        "text": "And playing the deep learning game, we want most of the computations involved to look",
        "start": 333.94,
        "end": 337.8
    },
    {
        "text": "like matrix-vector products where the matrices are full of tunable weights, things that the",
        "start": 337.8,
        "end": 342.5
    },
    {
        "text": "model will learn based on data.",
        "start": 342.5,
        "end": 344.8
    },
    {
        "text": "To be clear, I'm making up this example of adjectives updating nouns just to illustrate",
        "start": 344.8,
        "end": 349.0
    },
    {
        "text": "the type of behavior that you could imagine an intention had doing.",
        "start": 349.0,
        "end": 353.06
    },
    {
        "text": "As with so much deep learning, the true behavior is much harder to parse, because it's based",
        "start": 353.06,
        "end": 356.9
    },
    {
        "text": "on tweaking and tuning a huge number of parameters to minimize some cost function.",
        "start": 356.9,
        "end": 362.06
    },
    {
        "text": "It's just that as we step through all of the different matrices filled with parameters",
        "start": 362.06,
        "end": 365.58
    },
    {
        "text": "that are involved in this process, I think it's really helpful to have an imagined example",
        "start": 365.58,
        "end": 370.24
    },
    {
        "text": "of something that it could be doing to help keep it all more concrete.",
        "start": 370.24,
        "end": 374.26
    },
    {
        "text": "For the first step of this process, you might imagine each noun, like creature, asking the",
        "start": 374.26,
        "end": 378.58
    },
    {
        "text": "question, hey, are there any adjectives sitting in front of me, and for the words fluffy and",
        "start": 378.58,
        "end": 383.58
    },
    {
        "text": "blue to each be able to answer, yeah, I'm an adjective and I'm in that position.",
        "start": 383.58,
        "end": 389.52
    },
    {
        "text": "That question is somehow encoded as yet another vector, another list of numbers, which we",
        "start": 389.52,
        "end": 394.4
    },
    {
        "text": "call the query for this word.",
        "start": 394.4,
        "end": 397.02
    },
    {
        "text": "This query vector, though, has a much smaller dimension than the embedding vector, say 128.",
        "start": 397.02,
        "end": 403.24
    },
    {
        "text": "Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying",
        "start": 403.24,
        "end": 408.1
    },
    {
        "text": "it by the embedding.",
        "start": 408.1,
        "end": 411.22
    },
    {
        "text": "Compressing things a bit, let's write that query vector as q, and then anytime you see",
        "start": 411.22,
        "end": 415.44
    },
    {
        "text": "me put a matrix next to an arrow like this one, it's meant to represent that multiplying",
        "start": 415.44,
        "end": 420.38
    },
    {
        "text": "this matrix by the vector at the arrow's start gives you the vector at the arrow's end.",
        "start": 420.38,
        "end": 426.3
    },
    {
        "text": "In this case, you multiply this matrix by all of the embeddings in the context, producing",
        "start": 426.3,
        "end": 430.56
    },
    {
        "text": "one query vector for each token.",
        "start": 430.56,
        "end": 433.78
    },
    {
        "text": "The entries of this matrix are parameters of the model, which means the true behavior",
        "start": 433.78,
        "end": 437.82
    },
    {
        "text": "is learned from data, and in practice what this matrix does in a particular attention",
        "start": 437.82,
        "end": 441.8
    },
    {
        "text": "head is challenging to parse.",
        "start": 441.8,
        "end": 444.04
    },
    {
        "text": "But for our sake, imagining an example that we might hope it would learn, we'll suppose",
        "start": 444.04,
        "end": 447.98
    },
    {
        "text": "that this query matrix maps the embeddings of nouns to certain directions in this smaller",
        "start": 447.98,
        "end": 452.7
    },
    {
        "text": "query space that somehow encodes the notion of looking for adjectives in preceding positions.",
        "start": 452.7,
        "end": 459.0
    },
    {
        "text": "As to what it does to other embeddings, who knows, maybe it simultaneously tries to accomplish",
        "start": 459.0,
        "end": 463.42
    },
    {
        "text": "some other goal with those, right now we're laser focused on the nouns.",
        "start": 463.42,
        "end": 467.38
    },
    {
        "text": "At the same time, associated with this is a second matrix called the key matrix, which",
        "start": 467.38,
        "end": 472.28
    },
    {
        "text": "you also multiply by every one of the embeddings.",
        "start": 472.28,
        "end": 475.38
    },
    {
        "text": "This produces a second sequence of vectors that we call the keys.",
        "start": 475.38,
        "end": 479.44
    },
    {
        "text": "Conceptually you want to think of the keys as potentially answering the queries.",
        "start": 479.44,
        "end": 484.04
    },
    {
        "text": "This key matrix is also full of tunable parameters, and just like the query matrix it maps the",
        "start": 484.04,
        "end": 488.46
    },
    {
        "text": "embedding vectors to that same smaller dimensional space.",
        "start": 488.46,
        "end": 492.3
    },
    {
        "text": "You think of the keys as matching the queries whenever they closely align with each other.",
        "start": 492.3,
        "end": 497.64
    },
    {
        "text": "In our example, you would imagine that the key matrix maps the adjectives, like fluffy",
        "start": 497.64,
        "end": 501.58
    },
    {
        "text": "and blue, to vectors that are closely aligned with the query produced by the word creature.",
        "start": 501.58,
        "end": 507.7
    },
    {
        "text": "To measure how well each key matches each query, you compute a dot product between each",
        "start": 507.7,
        "end": 512.52
    },
    {
        "text": "possible key-query pair.",
        "start": 512.52,
        "end": 514.94
    },
    {
        "text": "I like to visualize a grid full of a bunch of dots, where the bigger dots correspond",
        "start": 514.94,
        "end": 519.2
    },
    {
        "text": "the larger dot products, the places where the keys and queries align. For our adjective-noun example,",
        "start": 519.2,
        "end": 525.04
    },
    {
        "text": "that would look a little more like this, where if the keys produced by fluffy and blue really do",
        "start": 525.04,
        "end": 530.96
    },
    {
        "text": "align closely with the query produced by creature, then the dot products in these two spots would be",
        "start": 530.96,
        "end": 536.64
    },
    {
        "text": "some large positive numbers. In the lingo, machine learning people would say that this means the",
        "start": 536.64,
        "end": 541.6
    },
    {
        "text": "embeddings of fluffy and blue attend to the embedding of creature. By contrast to the dot",
        "start": 541.6,
        "end": 546.96
    },
    {
        "text": "product between the key for some other word like the and the query for creature would be some small",
        "start": 546.96,
        "end": 552.96
    },
    {
        "text": "or negative value that reflects that these are unrelated to each other. So we have this grid of",
        "start": 552.96,
        "end": 558.88
    },
    {
        "text": "values that can be any real number from negative infinity to infinity giving us a score for how",
        "start": 558.88,
        "end": 564.88
    },
    {
        "text": "relevant each word is to updating the meaning of every other word. The way we're about to use these",
        "start": 564.88,
        "end": 570.32
    },
    {
        "text": "scores is to take a certain weighted sum along each column weighted by the relevance. So instead",
        "start": 570.32,
        "end": 576.8
    },
    {
        "text": "Instead of having values range from negative infinity to infinity, what we want is for",
        "start": 576.8,
        "end": 581.02
    },
    {
        "text": "the numbers in these columns to be between 0 and 1, and for each column to add up to",
        "start": 581.02,
        "end": 585.96
    },
    {
        "text": "1, as if they were a probability distribution.",
        "start": 585.96,
        "end": 589.34
    },
    {
        "text": "If you're coming in from the last chapter, you know what we need to do then.",
        "start": 589.34,
        "end": 592.68
    },
    {
        "text": "We compute a softmax along each one of these columns to normalize the values.",
        "start": 592.68,
        "end": 600.42
    },
    {
        "text": "In our picture, after you apply softmax to all of the columns, we'll fill in the grid",
        "start": 600.42,
        "end": 604.4
    },
    {
        "text": "with these normalized values.",
        "start": 604.4,
        "end": 606.9
    },
    {
        "text": "At this point, you're safe to think about each column as giving weights",
        "start": 606.9,
        "end": 609.8
    },
    {
        "text": "according to how relevant the word on the left is to the corresponding value at the top.",
        "start": 609.8,
        "end": 615.0
    },
    {
        "text": "We call this grid an attention pattern.",
        "start": 615.0,
        "end": 617.9
    },
    {
        "text": "Now, if you look at the original Transformer paper,",
        "start": 617.9,
        "end": 620.1
    },
    {
        "text": "there's a really compact way that they write this all down.",
        "start": 620.1,
        "end": 623.7
    },
    {
        "text": "Here, the variables q and k represent the full arrays of query and key vectors respectively,",
        "start": 623.7,
        "end": 630.7
    },
    {
        "text": "those little vectors you get by multiplying the embeddings by the query and the key matrices.",
        "start": 630.7,
        "end": 635.1
    },
    {
        "text": "This expression up in the numerator is a really compact way to represent the grid of all possible",
        "start": 635.1,
        "end": 640.3
    },
    {
        "text": "dot products between pairs of keys and queries. A small technical detail that I didn't mention",
        "start": 640.3,
        "end": 646.06
    },
    {
        "text": "is that for numerical stability it happens to be helpful to divide all of these values by the",
        "start": 646.06,
        "end": 650.62
    },
    {
        "text": "square root of the dimension in that key query space. Then this softmax that's wrapped around",
        "start": 650.62,
        "end": 656.78
    },
    {
        "text": "the full expression, is meant to be understood to apply column by column.",
        "start": 656.78,
        "end": 661.84
    },
    {
        "text": "As to that V term, we'll talk about it in just a second.",
        "start": 661.84,
        "end": 665.24
    },
    {
        "text": "Before that, there's one other technical detail that so far I've skipped.",
        "start": 665.24,
        "end": 669.14
    },
    {
        "text": "During the training process, when you run this model on a given text example, and all",
        "start": 669.14,
        "end": 673.22
    },
    {
        "text": "of the weights are slightly adjusted and tuned to either reward or punish it based on how",
        "start": 673.22,
        "end": 677.54
    },
    {
        "text": "high a probability it assigns to the true next word in the passage, it turns out to",
        "start": 677.54,
        "end": 681.54
    },
    {
        "text": "make the whole training process a lot more efficient if you simultaneously have it predict",
        "start": 681.54,
        "end": 685.9
    },
    {
        "text": "every possible next token following each initial sub-sequence of tokens in this passage.",
        "start": 685.9,
        "end": 692.08
    },
    {
        "text": "For example, with the phrase that we've been focusing on, it might also be predicting what",
        "start": 692.08,
        "end": 696.0
    },
    {
        "text": "words follow creature, and what words follow the.",
        "start": 696.0,
        "end": 700.4
    },
    {
        "text": "This is really nice, because it means what would otherwise be a single training example",
        "start": 700.4,
        "end": 704.04
    },
    {
        "text": "effectively acts as many.",
        "start": 704.04,
        "end": 706.22
    },
    {
        "text": "For the purposes of our attention pattern, it means that you never want to allow later",
        "start": 706.22,
        "end": 710.28
    },
    {
        "text": "words to influence earlier words, since otherwise they could kind of give away the answer for",
        "start": 710.28,
        "end": 715.12
    },
    {
        "text": "what comes next. What this means is that we want all of these spots here, the ones representing",
        "start": 715.12,
        "end": 719.92
    },
    {
        "text": "later tokens influencing earlier ones, to somehow be forced to be zero. The simplest thing you might",
        "start": 719.92,
        "end": 726.72
    },
    {
        "text": "think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one",
        "start": 726.72,
        "end": 730.96
    },
    {
        "text": "anymore, they wouldn't be normalized. So instead a common way to do this is that before applying",
        "start": 730.96,
        "end": 735.84
    },
    {
        "text": "softmax you set all of those entries to be negative infinity. If you do that then after",
        "start": 735.84,
        "end": 740.88
    },
    {
        "text": "After applying softmax, all of those get turned into zero, but the columns stay normalized.",
        "start": 740.88,
        "end": 746.18
    },
    {
        "text": "This process is called masking.",
        "start": 746.18,
        "end": 747.96
    },
    {
        "text": "There are versions of attention where you don't apply it, but in our GPT example, even",
        "start": 747.96,
        "end": 752.02
    },
    {
        "text": "though this is more relevant during the training phase than it would be, say, running it as",
        "start": 752.02,
        "end": 755.62
    },
    {
        "text": "a chatbot or something like that, you do always apply this masking to prevent later tokens",
        "start": 755.62,
        "end": 759.86
    },
    {
        "text": "from influencing earlier ones.",
        "start": 759.86,
        "end": 762.68
    },
    {
        "text": "Another fact that's worth reflecting on about this attention pattern is how its size is",
        "start": 762.68,
        "end": 767.12
    },
    {
        "text": "equal to the square of the context size.",
        "start": 767.12,
        "end": 769.94
    },
    {
        "text": "So this is why context size can be a really huge bottleneck for large language models,",
        "start": 769.94,
        "end": 774.16
    },
    {
        "text": "and scaling it up is non-trivial.",
        "start": 774.16,
        "end": 776.5
    },
    {
        "text": "As you might imagine, motivated by a desire for bigger and bigger context windows, recent",
        "start": 776.5,
        "end": 780.82
    },
    {
        "text": "years have seen some variations to the attention mechanism aimed at making context more scalable.",
        "start": 780.82,
        "end": 786.0
    },
    {
        "text": "But right here, you and I are staying focused on the basics.",
        "start": 786.0,
        "end": 789.44
    },
    {
        "text": "Okay, great, computing this pattern lets the model deduce which words are relevant to which",
        "start": 789.44,
        "end": 794.84
    },
    {
        "text": "other words.",
        "start": 794.84,
        "end": 796.16
    },
    {
        "text": "Now you need to actually update the embeddings, allowing words to pass information to whichever",
        "start": 796.16,
        "end": 801.12
    },
    {
        "text": "other words they're relevant to.",
        "start": 801.12,
        "end": 803.18
    },
    {
        "text": "For example, you want the embedding of fluffy to somehow cause a change to creature that",
        "start": 803.18,
        "end": 808.24
    },
    {
        "text": "moves it to a different part of this 12,000 dimensional embedding space that more specifically",
        "start": 808.24,
        "end": 813.2
    },
    {
        "text": "encodes a fluffy creature.",
        "start": 813.2,
        "end": 815.52
    },
    {
        "text": "What I'm going to do here is first show you the most straightforward way that you could",
        "start": 815.52,
        "end": 818.8
    },
    {
        "text": "do this, though there's a slight way that this gets modified in the context of multi-headed",
        "start": 818.8,
        "end": 823.04
    },
    {
        "text": "attention.",
        "start": 823.04,
        "end": 824.26
    },
    {
        "text": "This most straightforward way would be to use a third matrix, what we call the value",
        "start": 824.26,
        "end": 828.08
    },
    {
        "text": "matrix, which you multiply by the embedding of that first word, for example fluffy.",
        "start": 828.08,
        "end": 833.5
    },
    {
        "text": "The result of this is what you would call a value vector, and this is something that",
        "start": 833.5,
        "end": 837.34
    },
    {
        "text": "you add to the embedding of the second word, in this case something you add to the embedding",
        "start": 837.34,
        "end": 841.44
    },
    {
        "text": "of creature.",
        "start": 841.44,
        "end": 842.44
    },
    {
        "text": "So, this value vector lives in the same very high dimensional space as the embeddings.",
        "start": 842.44,
        "end": 847.58
    },
    {
        "text": "When you multiply this value matrix by the embedding of a word, you might think of it",
        "start": 847.58,
        "end": 851.2
    },
    {
        "text": "as saying if this word is relevant to adjusting the meaning of something else, what exactly should",
        "start": 851.2,
        "end": 856.96
    },
    {
        "text": "be added to the embedding of that something else in order to reflect this? Looking back in our",
        "start": 856.96,
        "end": 862.8
    },
    {
        "text": "diagram, let's set aside all of the keys and the queries, since after you compute the attention",
        "start": 862.8,
        "end": 867.92
    },
    {
        "text": "pattern you're done with those, then you're going to take this value matrix and multiply it by every",
        "start": 867.92,
        "end": 872.56
    },
    {
        "text": "one of those embeddings to produce a sequence of value vectors. You might think of these value",
        "start": 872.56,
        "end": 878.0
    },
    {
        "text": "vectors as being kind of associated with the corresponding keys.",
        "start": 878.0,
        "end": 882.48
    },
    {
        "text": "For each column in this diagram, you multiply each of the value vectors by the corresponding",
        "start": 882.48,
        "end": 887.88
    },
    {
        "text": "weight in that column.",
        "start": 887.88,
        "end": 890.14
    },
    {
        "text": "For example, here, under the embedding of creature, you would be adding large proportions",
        "start": 890.14,
        "end": 894.26
    },
    {
        "text": "of the value vectors for fluffy and blue, while all of the other value vectors get zeroed",
        "start": 894.26,
        "end": 899.64
    },
    {
        "text": "out, or at least nearly zeroed out.",
        "start": 899.64,
        "end": 902.24
    },
    {
        "text": "And then finally, the way to actually update the embedding associated with this column,",
        "start": 902.24,
        "end": 906.52
    },
    {
        "text": "previously encoding some context-free meaning of creature, you add together all of these",
        "start": 906.52,
        "end": 910.7
    },
    {
        "text": "rescaled values in the column, producing a change that you want to add that I'll label",
        "start": 910.7,
        "end": 915.8
    },
    {
        "text": "delta E, and then you add that to the original embedding.",
        "start": 915.8,
        "end": 919.64
    },
    {
        "text": "Hopefully what results is a more refined vector encoding the more contextually rich meaning,",
        "start": 919.64,
        "end": 925.12
    },
    {
        "text": "like that of a fluffy blue creature.",
        "start": 925.12,
        "end": 927.54
    },
    {
        "text": "And of course you don't just do this to one embedding, you apply the same weighted sum",
        "start": 927.54,
        "end": 931.62
    },
    {
        "text": "across all of the columns in this picture, producing a sequence of changes.",
        "start": 931.62,
        "end": 936.46
    },
    {
        "text": "Adding all of those changes to the corresponding embeddings produces a full sequence of more",
        "start": 936.46,
        "end": 941.1
    },
    {
        "text": "refined embeddings popping out of the attention block.",
        "start": 941.1,
        "end": 944.76
    },
    {
        "text": "Zooming out, this whole process is what you would describe as a single head of attention.",
        "start": 944.76,
        "end": 949.78
    },
    {
        "text": "As I've described things so far, this process is parameterized by three distinct matrices,",
        "start": 949.78,
        "end": 954.9
    },
    {
        "text": "all filled with tunable parameters, the key, the query, and the value.",
        "start": 954.9,
        "end": 959.56
    },
    {
        "text": "I want to take a moment to continue what we started in the last chapter with the scorekeeping",
        "start": 959.56,
        "end": 963.3
    },
    {
        "text": "where we count up the total number of model parameters using the numbers from GPT-3.",
        "start": 963.3,
        "end": 969.42
    },
    {
        "text": "These key and query matrices each have 12,288 columns, matching the embedding dimension,",
        "start": 969.42,
        "end": 975.5
    },
    {
        "text": "and 128 rows, matching the dimension of that smaller key query space.",
        "start": 975.5,
        "end": 980.34
    },
    {
        "text": "This gives us an additional 1.5 million or so parameters for each one.",
        "start": 980.34,
        "end": 984.92
    },
    {
        "text": "If you look at that value matrix by contrast, the way I've described things so far would",
        "start": 984.92,
        "end": 989.38
    },
    {
        "text": "suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both",
        "start": 989.38,
        "end": 996.76
    },
    {
        "text": "its inputs and its outputs live in this very large embedding space.",
        "start": 996.76,
        "end": 1001.6
    },
    {
        "text": "If true, that would mean about 150 million added parameters.",
        "start": 1001.6,
        "end": 1005.72
    },
    {
        "text": "And to be clear, you could do that, you could devote orders of magnitude more parameters",
        "start": 1005.72,
        "end": 1009.76
    },
    {
        "text": "to the value map than to the key and query.",
        "start": 1009.76,
        "end": 1012.22
    },
    {
        "text": "But in practice, it is much more efficient if instead you make it so that the number",
        "start": 1012.22,
        "end": 1015.96
    },
    {
        "text": "of parameters devoted to this value map is the same as the number devoted to the key",
        "start": 1015.96,
        "end": 1020.14
    },
    {
        "text": "in the query.",
        "start": 1020.14,
        "end": 1021.5
    },
    {
        "text": "This is especially relevant in the setting of running multiple attention heads in parallel.",
        "start": 1021.5,
        "end": 1026.26
    },
    {
        "text": "The way this looks is that the value map is factored as a product of two smaller matrices.",
        "start": 1026.26,
        "end": 1030.72
    },
    {
        "text": "Conceptually, I would still encourage you to think about the overall linear map, one",
        "start": 1030.72,
        "end": 1034.9
    },
    {
        "text": "with inputs and outputs both in this larger embedding space, for example taking the embedding",
        "start": 1034.9,
        "end": 1039.72
    },
    {
        "text": "of blue to this blueness direction that you would add to nouns.",
        "start": 1039.72,
        "end": 1044.06
    },
    {
        "text": "It's just that it's broken up into two separate steps.",
        "start": 1044.06,
        "end": 1047.22
    },
    {
        "text": "The first matrix on the right here has a smaller number of rows, typically the same size as",
        "start": 1047.22,
        "end": 1051.78
    },
    {
        "text": "the key query space.",
        "start": 1051.78,
        "end": 1053.16
    },
    {
        "text": "What this means is you can think of it as mapping the large embedding vectors down to",
        "start": 1053.16,
        "end": 1057.2
    },
    {
        "text": "a much smaller space.",
        "start": 1057.2,
        "end": 1059.34
    },
    {
        "text": "This is not the conventional naming, but I'm going to call this the value down matrix.",
        "start": 1059.34,
        "end": 1063.5
    },
    {
        "text": "The second matrix maps from this smaller space back up to the embedding space, producing",
        "start": 1063.5,
        "end": 1068.32
    },
    {
        "text": "the vectors that you use to make the actual updates.",
        "start": 1068.32,
        "end": 1070.94
    },
    {
        "text": "I'm going to call this one the value-up matrix, which, again, is not conventional.",
        "start": 1070.94,
        "end": 1075.28
    },
    {
        "text": "The way that you would see this written in most papers looks a little different.",
        "start": 1075.28,
        "end": 1078.46
    },
    {
        "text": "I'll talk about it in a minute.",
        "start": 1078.46,
        "end": 1079.46
    },
    {
        "text": "In my opinion, it tends to make things a little more conceptually confusing.",
        "start": 1079.46,
        "end": 1083.3
    },
    {
        "text": "To throw in linear algebra jargon here, what we're basically doing is constraining the",
        "start": 1083.3,
        "end": 1087.22
    },
    {
        "text": "overall value map to be a low-rank transformation.",
        "start": 1087.22,
        "end": 1091.54
    },
    {
        "text": "Turning back to the parameter count, all four of these matrices have the same size, and",
        "start": 1091.54,
        "end": 1095.92
    },
    {
        "text": "Then adding them all up, we get about 6.3 million parameters for one attention head.",
        "start": 1095.92,
        "end": 1102.2
    },
    {
        "text": "As a quick side note, to be a little more accurate, everything described so far is what",
        "start": 1102.2,
        "end": 1105.54
    },
    {
        "text": "people would call a self-attention head, to distinguish it from a variation that comes",
        "start": 1105.54,
        "end": 1109.48
    },
    {
        "text": "up in other models that's called cross-attention.",
        "start": 1109.48,
        "end": 1112.54
    },
    {
        "text": "This isn't relevant to our GPT example, but if you're curious, cross-attention involves",
        "start": 1112.54,
        "end": 1116.84
    },
    {
        "text": "models that process two distinct types of data, like text in one language and text in",
        "start": 1116.84,
        "end": 1122.02
    },
    {
        "text": "another language that's part of an ongoing generation of a translation.",
        "start": 1122.02,
        "end": 1125.7
    },
    {
        "text": "Or maybe audio input of speech, and an ongoing transcription.",
        "start": 1125.7,
        "end": 1130.42
    },
    {
        "text": "A cross-attention head looks almost identical.",
        "start": 1130.42,
        "end": 1133.12
    },
    {
        "text": "The only difference is that the key and query maps act on different datasets.",
        "start": 1133.12,
        "end": 1137.94
    },
    {
        "text": "In a model doing translation, for example, the keys might come from one language, while",
        "start": 1137.94,
        "end": 1142.32
    },
    {
        "text": "the queries come from another, and the attention pattern could describe which words from one",
        "start": 1142.32,
        "end": 1146.88
    },
    {
        "text": "language correspond to which words in another.",
        "start": 1146.88,
        "end": 1150.5
    },
    {
        "text": "And in this setting there would typically be no masking, since there's not really any",
        "start": 1150.5,
        "end": 1153.66
    },
    {
        "text": "notion of later tokens affecting earlier ones.",
        "start": 1153.66,
        "end": 1157.34
    },
    {
        "text": "Staying focused on self-attention though, if you understood everything so far, and if",
        "start": 1157.34,
        "end": 1161.2
    },
    {
        "text": "you were to stop here, you would come away with the essence of what attention really",
        "start": 1161.2,
        "end": 1164.7
    },
    {
        "text": "is.",
        "start": 1164.7,
        "end": 1165.92
    },
    {
        "text": "All that's really left to us is to lay out the sense in which you do this many, many",
        "start": 1165.92,
        "end": 1170.48
    },
    {
        "text": "different times.",
        "start": 1170.48,
        "end": 1172.26
    },
    {
        "text": "In our central example we focused on adjectives updating nouns, but of course there are lots",
        "start": 1172.26,
        "end": 1176.68
    },
    {
        "text": "of different ways that context can influence the meaning of a word.",
        "start": 1176.68,
        "end": 1180.48
    },
    {
        "text": "If the words they crashed the preceded the word car, it has implications for the shape",
        "start": 1180.48,
        "end": 1185.24
    },
    {
        "text": "and the structure of that car, and a lot of associations might be less grammatical.",
        "start": 1185.24,
        "end": 1189.88
    },
    {
        "text": "If the word wizard is anywhere in the same passage as Harry, it suggests that this might",
        "start": 1189.88,
        "end": 1194.58
    },
    {
        "text": "be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were",
        "start": 1194.58,
        "end": 1199.36
    },
    {
        "text": "in that passage, then perhaps the embedding of Harry should instead be updated to refer",
        "start": 1199.36,
        "end": 1203.6
    },
    {
        "text": "to the prince.",
        "start": 1203.6,
        "end": 1205.16
    },
    {
        "text": "For every different type of contextual updating that you might imagine, the parameters of",
        "start": 1205.16,
        "end": 1209.52
    },
    {
        "text": "these key and query matrices would be different to capture the different attention patterns,",
        "start": 1209.52,
        "end": 1213.84
    },
    {
        "text": "and the parameters of our value map would be different based on what should be added to the",
        "start": 1213.84,
        "end": 1218.56
    },
    {
        "text": "embeddings. And again, in practice the true behavior of these maps is much more difficult",
        "start": 1218.56,
        "end": 1223.52
    },
    {
        "text": "to interpret, where the weights are set to do whatever the model needs them to do to best",
        "start": 1223.52,
        "end": 1227.76
    },
    {
        "text": "accomplish its goal of predicting the next token. As I said before, everything we described is a",
        "start": 1227.76,
        "end": 1233.36
    },
    {
        "text": "single head of attention, and a full attention block inside a transformer consists of what's",
        "start": 1233.36,
        "end": 1238.4
    },
    {
        "text": "called multi-headed attention where you run a lot of these operations in parallel each with its own",
        "start": 1238.4,
        "end": 1243.68
    },
    {
        "text": "distinct key query and value maps. GPT-3 for example uses 96 attention heads inside each block.",
        "start": 1243.68,
        "end": 1251.92
    },
    {
        "text": "Considering that each one is already a bit confusing it's certainly a lot to hold in your",
        "start": 1251.92,
        "end": 1256.08
    },
    {
        "text": "head. Just to spell it all out very explicitly this means you have 96 distinct key and query",
        "start": 1256.08,
        "end": 1261.68
    },
    {
        "text": "matrices producing 96 distinct attention patterns. Then each head has its own distinct value matrices",
        "start": 1261.68,
        "end": 1268.64
    },
    {
        "text": "used to produce 96 sequences of value vectors. These are all added together using the",
        "start": 1268.64,
        "end": 1274.56
    },
    {
        "text": "corresponding attention patterns as weights. What this means is that for each position in the",
        "start": 1274.56,
        "end": 1279.84
    },
    {
        "text": "context, each token, every one of these heads produces a proposed change to be added to the",
        "start": 1279.84,
        "end": 1285.76
    },
    {
        "text": "embedding in that position. So what you do is you sum together all of those proposed changes,",
        "start": 1285.76,
        "end": 1291.28
    },
    {
        "text": "one for each head, and you add the result to the original embedding of that position.",
        "start": 1291.28,
        "end": 1295.36
    },
    {
        "text": "This entire sum here would be one slice of what's outputted from this multi-headed attention block,",
        "start": 1296.56,
        "end": 1303.36
    },
    {
        "text": "a single one of those refined embeddings that pops out the other end of it.",
        "start": 1303.36,
        "end": 1307.28
    },
    {
        "text": "Again, this is a lot to think about, so don't worry at all if it takes some time to sink in.",
        "start": 1308.16,
        "end": 1312.32
    },
    {
        "text": "The overall idea is that by running many distinct heads in parallel,",
        "start": 1312.32,
        "end": 1316.24
    },
    {
        "text": "you're giving the model the capacity to learn many distinct ways that context changes meaning.",
        "start": 1316.24,
        "end": 1321.68
    },
    {
        "text": "Pulling up our running tally for parameter count with 96 heads, each including its own variation",
        "start": 1323.52,
        "end": 1328.72
    },
    {
        "text": "of these four matrices, each block of multi-headed attention ends up with around 600 million",
        "start": 1328.72,
        "end": 1334.4
    },
    {
        "text": "parameters. There's one added slightly annoying thing that I should really mention for any of you",
        "start": 1334.4,
        "end": 1339.76
    },
    {
        "text": "who go on to read more about transformers. You remember how I said that the value map is factored",
        "start": 1339.76,
        "end": 1344.48
    },
    {
        "text": "out into these two distinct matrices, which I labeled as the value down and the value up",
        "start": 1344.48,
        "end": 1348.88
    },
    {
        "text": "matrices. The way that I framed things would suggest that you see this pair of matrices",
        "start": 1348.88,
        "end": 1353.86
    },
    {
        "text": "inside each attention head, and you could absolutely implement it this way. That would",
        "start": 1353.86,
        "end": 1359.02
    },
    {
        "text": "be a valid design. But the way that you see this written in papers and the way that it's",
        "start": 1359.02,
        "end": 1362.74
    },
    {
        "text": "implemented in practice looks a little different. All of these value up matrices for each head",
        "start": 1362.74,
        "end": 1368.14
    },
    {
        "text": "appear stapled together in one giant matrix that we call the output matrix, associated with",
        "start": 1368.14,
        "end": 1374.38
    },
    {
        "text": "the entire multi-headed attention block. And when you see people refer to the value matrix for a",
        "start": 1374.38,
        "end": 1379.26
    },
    {
        "text": "given attention head, they're typically only referring to this first step, the one that I",
        "start": 1379.26,
        "end": 1383.78
    },
    {
        "text": "was labeling as the value down projection into the smaller space. For the curious among you,",
        "start": 1383.78,
        "end": 1389.32
    },
    {
        "text": "I've left an on-screen note about it. It's one of those details that runs the risk of distracting",
        "start": 1389.46,
        "end": 1393.42
    },
    {
        "text": "from the main conceptual points, but I do want to call it out just so that you know if you read",
        "start": 1393.42,
        "end": 1397.46
    },
    {
        "text": "about this in other sources. Setting aside all the technical nuances, in the preview from the",
        "start": 1397.46,
        "end": 1402.42
    },
    {
        "text": "last chapter, we saw how data flowing through a transformer doesn't just flow through a single",
        "start": 1402.42,
        "end": 1407.28
    },
    {
        "text": "attention block. For one thing, it also goes through these other operations called multi-layer",
        "start": 1407.28,
        "end": 1412.04
    },
    {
        "text": "perceptrons. We'll talk more about those in the next chapter. And then it repeatedly goes through",
        "start": 1412.04,
        "end": 1416.7
    },
    {
        "text": "many, many copies of both of these operations. What this means is that after a given word imbibes",
        "start": 1416.7,
        "end": 1423.16
    },
    {
        "text": "some of its context, there are many more chances for this more nuanced embedding to be influenced",
        "start": 1423.16,
        "end": 1428.32
    },
    {
        "text": "by its more nuanced surroundings. The further down the network you go, with each embedding",
        "start": 1428.32,
        "end": 1433.6
    },
    {
        "text": "taking in more and more meaning from all the other embeddings, which themselves are getting",
        "start": 1433.6,
        "end": 1437.76
    },
    {
        "text": "more and more nuanced, the hope is that there's the capacity to encode higher level and more",
        "start": 1437.76,
        "end": 1442.32
    },
    {
        "text": "abstract ideas about a given input beyond just descriptors and grammatical structure.",
        "start": 1442.32,
        "end": 1447.68
    },
    {
        "text": "Things like sentiment and tone and whether it's a poem and what underlying scientific truths are",
        "start": 1447.68,
        "end": 1453.12
    },
    {
        "text": "are relevant to the piece, and things like that.",
        "start": 1453.12,
        "end": 1456.78
    },
    {
        "text": "Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the",
        "start": 1456.78,
        "end": 1462.98
    },
    {
        "text": "total number of key, query, and value parameters is multiplied by another 96, which brings",
        "start": 1462.98,
        "end": 1468.52
    },
    {
        "text": "the total sum to just under 58 billion distinct parameters devoted to all of the attention",
        "start": 1468.52,
        "end": 1474.36
    },
    {
        "text": "heads.",
        "start": 1474.36,
        "end": 1475.36
    },
    {
        "text": "That is a lot, to be sure, but it's only about a third of the 175 billion that are",
        "start": 1475.36,
        "end": 1479.9
    },
    {
        "text": "in the network in total.",
        "start": 1479.9,
        "end": 1481.66
    },
    {
        "text": "So even though attention gets all of the attention, the majority of parameters come from the blocks",
        "start": 1481.66,
        "end": 1486.62
    },
    {
        "text": "sitting in between these steps.",
        "start": 1486.62,
        "end": 1488.62
    },
    {
        "text": "In the next chapter, you and I will talk more about those other blocks and also a lot more",
        "start": 1488.62,
        "end": 1492.32
    },
    {
        "text": "about the training process.",
        "start": 1492.32,
        "end": 1494.18
    },
    {
        "text": "A big part of the story for the success of the attention mechanism is not so much any",
        "start": 1494.18,
        "end": 1498.48
    },
    {
        "text": "specific kind of behavior that it enables, but the fact that it's extremely parallelizable,",
        "start": 1498.48,
        "end": 1504.34
    },
    {
        "text": "meaning that you can run a huge number of computations in a short time using GPUs.",
        "start": 1504.34,
        "end": 1509.64
    },
    {
        "text": "that one of the big lessons about deep learning in the last decade or two has been that scale",
        "start": 1509.64,
        "end": 1513.62
    },
    {
        "text": "alone seems to give huge qualitative improvements in model performance. There's a huge advantage to",
        "start": 1513.62,
        "end": 1518.66
    },
    {
        "text": "parallelizable architectures that let you do this. If you want to learn more about this stuff, I've",
        "start": 1518.66,
        "end": 1523.88
    },
    {
        "text": "left lots of links in the description. In particular, anything produced by Andre Karpathy or Chris Ola",
        "start": 1523.88,
        "end": 1528.84
    },
    {
        "text": "tend to be pure gold. In this video, I wanted to just jump into attention in its current form,",
        "start": 1528.84,
        "end": 1533.82
    },
    {
        "text": "but if you're curious about more of the history for how we got here and how you might reinvent",
        "start": 1534.04,
        "end": 1537.56
    },
    {
        "text": "this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of",
        "start": 1537.56,
        "end": 1541.72
    },
    {
        "text": "that motivation. Also, Britt Cruz from the channel The Art of the Problem",
        "start": 1541.72,
        "end": 1545.48
    },
    {
        "text": "has a really nice video about the history of large language models.",
        "start": 1545.48,
        "end": 1548.84
    },
    {
        "text": "you",
        "start": 1567.56,
        "end": 1569.62
    }
]